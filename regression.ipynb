{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, Sequential, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from generator import Generator\n",
    "from keras import metrics\n",
    "import importlib\n",
    "import keras.backend as K"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T04:18:23.550248Z",
     "start_time": "2024-10-12T04:18:16.004499Z"
    }
   },
   "id": "f70dabf5c8e7e463"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datasetLoader import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T04:18:23.566249Z",
     "start_time": "2024-10-12T04:18:23.554251Z"
    }
   },
   "id": "8d981596359c20a5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss', marker=\".\")\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T04:18:23.581250Z",
     "start_time": "2024-10-12T04:18:23.568250Z"
    }
   },
   "id": "d8275c36c94a1d86"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "    data_sorb_tensor = K.constant((data_sorb.T[40:]).T)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    #print(\"data_sorb_tensor.shape = \", data_sorb_tensor.shape, y_pred[:,:128].shape)\n",
    "    isotherm_from_distribution = K.dot(y_pred[:,:128], data_sorb_tensor)\n",
    "    #print(\"isotherm_from_distribution = \", isotherm_from_distribution.shape, y_pred[:,128:].shape)\n",
    "    return K.mean(K.square(y_pred[:,:128] - y_true[:,:128]) + \n",
    "                   0.2*K.mean(K.square(y_pred[:,128:] - isotherm_from_distribution)))\n",
    "\n",
    "#np.ediff1d(pore_widths, to_begin=pore_widths[0])    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:52.488451Z",
     "start_time": "2024-10-12T05:43:52.472456Z"
    }
   },
   "id": "619b039f2d0a573a"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    layer = []\n",
    "    layer.append(Input(shape=371))\n",
    "    layer.append(layers.Dense(371, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(180, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Concatenate()([layer[-1], layer[0]]))\n",
    "    #model = tf.keras.Model(inputs=layer[0], outputs=layer[-1])\n",
    "    model.compile(loss=custom_loss, optimizer='Adam')\n",
    "    model.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:52.976451Z",
     "start_time": "2024-10-12T05:43:52.957449Z"
    }
   },
   "id": "8016a917c590c98a"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "with open(\"data/datasets/reports_best_tyhanov.npz\", 'rb') as f:\n",
    "    dataset = np.load(f)\n",
    "    isotherm_data = dataset[\"isotherm_data\"]\n",
    "    pore_distribution_data = dataset[\"pore_distribution_data\"]\n",
    "    pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:53.652853Z",
     "start_time": "2024-10-12T05:43:53.622851Z"
    }
   },
   "id": "58dd8a7b2ff1a173"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "x, y = load_dataset('data/datasets/reports_best_tyhanov.npz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:54.134059Z",
     "start_time": "2024-10-12T05:43:54.098083Z"
    }
   },
   "id": "a0d9a25ca1cb5c4b"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:54.463060Z",
     "start_time": "2024-10-12T05:43:54.456062Z"
    }
   },
   "id": "fffa14b27b9fcdc7"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "model = create_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:54.841060Z",
     "start_time": "2024-10-12T05:43:54.772059Z"
    }
   },
   "id": "a098bec7cbb731de"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3077.2695\n",
      "Epoch 1: val_loss improved from inf to 3063.85498, saving model to data/models\\reports_losss.keras\n",
      "28/28 [==============================] - 1s 15ms/step - loss: 3063.8982 - val_loss: 3063.8550 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3352.3472\n",
      "Epoch 2: val_loss improved from 3063.85498 to 3061.70898, saving model to data/models\\reports_losss.keras\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3063.0112 - val_loss: 3061.7090 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3155.2773\n",
      "Epoch 3: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3063.9661 - val_loss: 3063.2363 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3349.0845\n",
      "Epoch 4: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.1360 - val_loss: 3062.2515 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3222.0708\n",
      "Epoch 5: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.2864 - val_loss: 3062.6223 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2917.1318\n",
      "Epoch 6: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.6685 - val_loss: 3068.4941 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3026.5378\n",
      "Epoch 7: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.3660 - val_loss: 3066.3408 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3311.4265\n",
      "Epoch 8: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.8257 - val_loss: 3063.5732 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3283.0037\n",
      "Epoch 9: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3066.4053 - val_loss: 3090.8940 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3062.8040\n",
      "Epoch 10: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.5120 - val_loss: 3062.8662 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2908.4224\n",
      "Epoch 11: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.4429 - val_loss: 3065.0540 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3123.5408\n",
      "Epoch 12: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3070.2710 - val_loss: 3067.4634 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3152.1199\n",
      "Epoch 13: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3086.8425 - val_loss: 3105.9155 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3086.5398\n",
      "Epoch 14: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3086.5398 - val_loss: 3070.8364 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2965.1802\n",
      "Epoch 15: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3096.9937 - val_loss: 3080.4863 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3048.2041\n",
      "Epoch 16: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 3090.3760 - val_loss: 3067.5042 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3069.3201\n",
      "Epoch 17: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 3069.3201 - val_loss: 3066.9521 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3045.9739\n",
      "Epoch 18: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.5452 - val_loss: 3070.3989 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3203.1560\n",
      "Epoch 19: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.8323 - val_loss: 3094.4187 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3098.1377\n",
      "Epoch 20: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.7705 - val_loss: 3068.1353 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3136.7407\n",
      "Epoch 21: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3069.4102 - val_loss: 3062.5581 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3209.6638\n",
      "Epoch 22: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3070.0981 - val_loss: 3069.6638 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3160.5923\n",
      "Epoch 23: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3066.2991 - val_loss: 3067.6880 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3052.8884\n",
      "Epoch 24: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3068.8242 - val_loss: 3065.3462 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3044.1824\n",
      "Epoch 25: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3067.8413 - val_loss: 3063.4268 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3228.8276\n",
      "Epoch 26: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.3496 - val_loss: 3077.3037 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2797.9424\n",
      "Epoch 27: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3068.4097 - val_loss: 3065.3352 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3036.2439\n",
      "Epoch 28: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3064.4988 - val_loss: 3064.6819 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3110.0515\n",
      "Epoch 29: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.4397 - val_loss: 3065.7446 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 2970.5618\n",
      "Epoch 30: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3066.1702 - val_loss: 3068.3564 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3077.5532\n",
      "Epoch 31: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3077.5532 - val_loss: 3069.1541 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3305.7341\n",
      "Epoch 32: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.7764 - val_loss: 3086.2754 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3195.4492\n",
      "Epoch 33: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3075.2827 - val_loss: 3068.5066 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3066.4663\n",
      "Epoch 34: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3066.4663 - val_loss: 3064.3125 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3067.3008\n",
      "Epoch 35: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3067.3008 - val_loss: 3066.5281 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 2891.5652\n",
      "Epoch 36: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.1689 - val_loss: 3065.0186 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3139.0962\n",
      "Epoch 37: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3153.7275 - val_loss: 3092.3225 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3055.0322\n",
      "Epoch 38: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3131.1064 - val_loss: 3122.0044 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3120.9834\n",
      "Epoch 39: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3109.7920 - val_loss: 3080.9045 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3028.6333\n",
      "Epoch 40: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3088.6931 - val_loss: 3073.2793 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2994.4443\n",
      "Epoch 41: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3082.7869 - val_loss: 3081.3804 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3111.9150\n",
      "Epoch 42: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3076.6516 - val_loss: 3066.0034 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3133.3086\n",
      "Epoch 43: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 3068.4434 - val_loss: 3077.0557 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 3007.1362\n",
      "Epoch 44: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3073.7483 - val_loss: 3075.2610 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3188.3594\n",
      "Epoch 45: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 3072.6243 - val_loss: 3085.9678 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2921.8730\n",
      "Epoch 46: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3083.7346 - val_loss: 3167.0869 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 3116.1018\n",
      "Epoch 47: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3163.0684 - val_loss: 3146.2751 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3181.4875\n",
      "Epoch 48: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3137.0024 - val_loss: 3117.2234 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 3346.1633\n",
      "Epoch 49: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3102.0896 - val_loss: 3106.9607 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3258.2051\n",
      "Epoch 50: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3101.1689 - val_loss: 3093.1548 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2872.4280\n",
      "Epoch 51: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3086.8499 - val_loss: 3085.6707 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2986.2358\n",
      "Epoch 52: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3081.4409 - val_loss: 3070.6533 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2960.4521\n",
      "Epoch 53: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3072.9575 - val_loss: 3071.6023 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2912.4810\n",
      "Epoch 54: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.7498 - val_loss: 3065.0129 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3154.6521\n",
      "Epoch 55: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3064.5869 - val_loss: 3062.4587 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3123.9517\n",
      "Epoch 56: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3064.4355 - val_loss: 3064.1492 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3135.2192\n",
      "Epoch 57: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3073.1917 - val_loss: 3085.5154 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2757.7397\n",
      "Epoch 58: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.0142 - val_loss: 3067.0911 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2996.9983\n",
      "Epoch 59: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.3904 - val_loss: 3062.7085 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3211.2439\n",
      "Epoch 60: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.4878 - val_loss: 3069.6831 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3110.0537\n",
      "Epoch 61: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.6243 - val_loss: 3065.7092 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3044.6277\n",
      "Epoch 62: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3068.8491 - val_loss: 3073.9812 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3171.9819\n",
      "Epoch 63: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3074.5183 - val_loss: 3112.9929 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3026.5166\n",
      "Epoch 64: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3082.7451 - val_loss: 3066.4351 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3428.2986\n",
      "Epoch 65: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.2080 - val_loss: 3064.1509 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3145.8337\n",
      "Epoch 66: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3071.0984 - val_loss: 3075.9963 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3102.3198\n",
      "Epoch 67: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3070.5095 - val_loss: 3063.9832 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3083.0159\n",
      "Epoch 68: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3068.4097 - val_loss: 3067.4216 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 2865.9399\n",
      "Epoch 69: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3068.2734 - val_loss: 3065.2385 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3112.9927\n",
      "Epoch 70: val_loss did not improve from 3061.70898\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3066.7361 - val_loss: 3067.8770 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3037.9336\n",
      "Epoch 71: val_loss improved from 3061.70898 to 3061.48657, saving model to data/models\\reports_losss.keras\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 3068.3496 - val_loss: 3061.4866 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3350.1770\n",
      "Epoch 72: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.2087 - val_loss: 3068.2761 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3065.0142\n",
      "Epoch 73: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 3065.0142 - val_loss: 3064.2563 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2604.4346\n",
      "Epoch 74: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.3704 - val_loss: 3065.2385 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3206.4434\n",
      "Epoch 75: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.2429 - val_loss: 3062.8521 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3001.6382\n",
      "Epoch 76: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3065.1543 - val_loss: 3062.9238 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2838.9075\n",
      "Epoch 77: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3086.8694 - val_loss: 3090.7288 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3398.1763\n",
      "Epoch 78: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3113.1624 - val_loss: 3156.9407 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3287.3733\n",
      "Epoch 79: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3102.1870 - val_loss: 3083.3926 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3043.7725\n",
      "Epoch 80: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3081.2512 - val_loss: 3070.9463 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3050.7092\n",
      "Epoch 81: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.1680 - val_loss: 3073.2273 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3148.7341\n",
      "Epoch 82: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3068.2146 - val_loss: 3086.6492 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3102.6306\n",
      "Epoch 83: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3089.4575 - val_loss: 3095.7466 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2942.7026\n",
      "Epoch 84: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.6111 - val_loss: 3069.3108 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3471.0110\n",
      "Epoch 85: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.3943 - val_loss: 3079.4827 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3234.7332\n",
      "Epoch 86: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.5947 - val_loss: 3062.5359 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3067.6165\n",
      "Epoch 87: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3067.6165 - val_loss: 3062.2991 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3086.4922\n",
      "Epoch 88: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 3071.9900 - val_loss: 3069.8562 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2517.5308\n",
      "Epoch 89: val_loss did not improve from 3061.48657\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.2461 - val_loss: 3063.5195 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3052.9641\n",
      "Epoch 90: val_loss improved from 3061.48657 to 3061.00854, saving model to data/models\\reports_losss.keras\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3063.8008 - val_loss: 3061.0085 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3300.8938\n",
      "Epoch 91: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.3569 - val_loss: 3067.1541 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2863.7529\n",
      "Epoch 92: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.0413 - val_loss: 3065.7366 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3434.8784\n",
      "Epoch 93: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.2253 - val_loss: 3078.4651 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3064.5586\n",
      "Epoch 94: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3073.7371 - val_loss: 3090.4365 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3208.3872\n",
      "Epoch 95: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3208.3872 - val_loss: 3185.8547 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3050.9026\n",
      "Epoch 96: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3137.2673 - val_loss: 3089.5679 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3129.0681\n",
      "Epoch 97: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3081.6196 - val_loss: 3102.9204 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3170.2153\n",
      "Epoch 98: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3100.0938 - val_loss: 3083.2151 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3075.1609\n",
      "Epoch 99: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3075.1609 - val_loss: 3068.1272 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3324.8335\n",
      "Epoch 100: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3101.7283 - val_loss: 3092.2271 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2966.4966\n",
      "Epoch 101: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3085.6265 - val_loss: 3067.2527 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3039.9517\n",
      "Epoch 102: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.4758 - val_loss: 3073.0540 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3156.4070\n",
      "Epoch 103: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.1155 - val_loss: 3064.3259 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3040.4678\n",
      "Epoch 104: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.7668 - val_loss: 3066.1040 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2900.2747\n",
      "Epoch 105: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.6396 - val_loss: 3064.8416 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2770.1926\n",
      "Epoch 106: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.0806 - val_loss: 3066.8684 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3079.1260\n",
      "Epoch 107: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.4856 - val_loss: 3072.6741 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3336.3696\n",
      "Epoch 108: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3104.5894 - val_loss: 3080.6958 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3062.8481\n",
      "Epoch 109: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 3079.6611 - val_loss: 3071.9036 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2922.5471\n",
      "Epoch 110: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3068.3628 - val_loss: 3064.5337 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3334.4802\n",
      "Epoch 111: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.1321 - val_loss: 3062.0586 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2670.2764\n",
      "Epoch 112: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.7444 - val_loss: 3065.8440 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3079.1550\n",
      "Epoch 113: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.6780 - val_loss: 3061.4744 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2981.7046\n",
      "Epoch 114: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.3892 - val_loss: 3063.2854 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3102.1267\n",
      "Epoch 115: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.4573 - val_loss: 3066.1506 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3165.6616\n",
      "Epoch 116: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.8855 - val_loss: 3063.2329 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3173.3962\n",
      "Epoch 117: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.8325 - val_loss: 3062.5596 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3042.3213\n",
      "Epoch 118: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3084.2195 - val_loss: 3108.4133 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2975.1306\n",
      "Epoch 119: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3080.7188 - val_loss: 3067.5303 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2948.2830\n",
      "Epoch 120: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.8142 - val_loss: 3073.5730 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3085.7739\n",
      "Epoch 121: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3085.7739 - val_loss: 3074.2175 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3272.2871\n",
      "Epoch 122: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3070.6003 - val_loss: 3093.0369 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3340.5669\n",
      "Epoch 123: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3083.3486 - val_loss: 3072.7786 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2952.0818\n",
      "Epoch 124: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.4912 - val_loss: 3075.6873 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3112.5833\n",
      "Epoch 125: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.8623 - val_loss: 3064.8220 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2987.0994\n",
      "Epoch 126: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3071.0012 - val_loss: 3084.5376 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3016.2617\n",
      "Epoch 127: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3100.7444 - val_loss: 3093.3838 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3192.8811\n",
      "Epoch 128: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3105.2549 - val_loss: 3094.1931 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3131.9497\n",
      "Epoch 129: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3081.7471 - val_loss: 3077.2671 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2999.8245\n",
      "Epoch 130: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.0449 - val_loss: 3064.8193 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2970.8215\n",
      "Epoch 131: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.8015 - val_loss: 3085.4666 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3001.0654\n",
      "Epoch 132: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.0559 - val_loss: 3078.8816 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3211.4993\n",
      "Epoch 133: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.8206 - val_loss: 3066.6062 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3102.9868\n",
      "Epoch 134: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.4482 - val_loss: 3067.5029 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3094.7852\n",
      "Epoch 135: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3088.8191 - val_loss: 3132.3577 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2924.6057\n",
      "Epoch 136: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3100.1489 - val_loss: 3083.4385 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2783.4517\n",
      "Epoch 137: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3086.6687 - val_loss: 3087.9221 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3138.0784\n",
      "Epoch 138: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3084.8560 - val_loss: 3078.5088 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2804.7244\n",
      "Epoch 139: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.0071 - val_loss: 3072.4910 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3002.1387\n",
      "Epoch 140: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.1289 - val_loss: 3065.2927 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3172.6326\n",
      "Epoch 141: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3068.6609 - val_loss: 3068.8184 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3126.6052\n",
      "Epoch 142: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 3066.8276 - val_loss: 3069.3694 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2484.3506\n",
      "Epoch 143: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3067.2419 - val_loss: 3066.6829 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3289.6453\n",
      "Epoch 144: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.9482 - val_loss: 3063.6943 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3112.5000\n",
      "Epoch 145: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.2026 - val_loss: 3061.4795 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2898.4319\n",
      "Epoch 146: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3062.8152 - val_loss: 3063.2761 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3124.5405\n",
      "Epoch 147: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.8582 - val_loss: 3092.7595 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3156.7415\n",
      "Epoch 148: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 3078.1489 - val_loss: 3066.5159 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3100.4260\n",
      "Epoch 149: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 3065.7444 - val_loss: 3065.3982 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3244.2026\n",
      "Epoch 150: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 3065.5923 - val_loss: 3069.4697 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3482.0737\n",
      "Epoch 151: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3075.2244 - val_loss: 3070.7515 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2970.9341\n",
      "Epoch 152: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.3831 - val_loss: 3069.3147 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3137.8076\n",
      "Epoch 153: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3086.8235 - val_loss: 3095.9805 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3240.8547\n",
      "Epoch 154: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3079.8115 - val_loss: 3071.5476 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2942.7791\n",
      "Epoch 155: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3075.1880 - val_loss: 3079.3613 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 3118.8350\n",
      "Epoch 156: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3076.4285 - val_loss: 3071.3267 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3047.0081\n",
      "Epoch 157: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.9597 - val_loss: 3079.2473 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2693.9551\n",
      "Epoch 158: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.5327 - val_loss: 3064.4285 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3285.0610\n",
      "Epoch 159: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.4619 - val_loss: 3064.5400 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2926.0977\n",
      "Epoch 160: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3064.1648 - val_loss: 3061.2783 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3255.3325\n",
      "Epoch 161: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.8242 - val_loss: 3063.2466 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3151.3997\n",
      "Epoch 162: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.2888 - val_loss: 3064.7961 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2831.7686\n",
      "Epoch 163: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.0383 - val_loss: 3061.5505 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3066.2175\n",
      "Epoch 164: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.9382 - val_loss: 3065.0530 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3078.1772\n",
      "Epoch 165: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.8809 - val_loss: 3069.6021 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3333.5420\n",
      "Epoch 166: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3066.3364 - val_loss: 3065.2002 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3038.9719\n",
      "Epoch 167: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3122.1189 - val_loss: 3147.8735 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3301.0974\n",
      "Epoch 168: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3131.9287 - val_loss: 3155.3877 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2973.4905\n",
      "Epoch 169: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3144.9912 - val_loss: 3341.9717 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3245.5515\n",
      "Epoch 170: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3169.6611 - val_loss: 3135.7734 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2653.6016\n",
      "Epoch 171: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3116.1853 - val_loss: 3097.1279 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2784.1460\n",
      "Epoch 172: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3184.6313 - val_loss: 3149.4773 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2942.3418\n",
      "Epoch 173: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3164.8552 - val_loss: 3142.3289 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3102.1641\n",
      "Epoch 174: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3144.6072 - val_loss: 3100.6069 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3177.9355\n",
      "Epoch 175: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3095.7283 - val_loss: 3097.6230 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3163.6355\n",
      "Epoch 176: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3083.5354 - val_loss: 3074.0352 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2948.3560\n",
      "Epoch 177: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3077.9336 - val_loss: 3071.1292 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3123.8569\n",
      "Epoch 178: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.8801 - val_loss: 3071.8972 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3237.2266\n",
      "Epoch 179: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.2183 - val_loss: 3065.8596 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3070.0134\n",
      "Epoch 180: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3070.0134 - val_loss: 3072.1199 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3110.8960\n",
      "Epoch 181: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3074.9851 - val_loss: 3088.0286 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 2981.4268\n",
      "Epoch 182: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3070.3152 - val_loss: 3066.3928 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3216.3628\n",
      "Epoch 183: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3068.4124 - val_loss: 3065.8906 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3072.7708\n",
      "Epoch 184: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3066.9675 - val_loss: 3061.9116 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2791.6299\n",
      "Epoch 185: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.9102 - val_loss: 3077.8511 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2751.5571\n",
      "Epoch 186: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3087.4077 - val_loss: 3089.6809 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 3001.5901\n",
      "Epoch 187: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3081.2959 - val_loss: 3071.0022 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3065.3862\n",
      "Epoch 188: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3080.2625 - val_loss: 3091.9341 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3092.8877\n",
      "Epoch 189: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3092.8877 - val_loss: 3069.5933 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2876.3691\n",
      "Epoch 190: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.9309 - val_loss: 3065.9253 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3429.9265\n",
      "Epoch 191: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.2068 - val_loss: 3067.9575 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2790.8062\n",
      "Epoch 192: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3074.4751 - val_loss: 3080.6516 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3074.4761\n",
      "Epoch 193: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3081.2915 - val_loss: 3080.2722 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2995.9468\n",
      "Epoch 194: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3082.1670 - val_loss: 3071.0830 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2936.0137\n",
      "Epoch 195: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.5620 - val_loss: 3070.1716 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3180.5276\n",
      "Epoch 196: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.7559 - val_loss: 3069.2151 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3159.4443\n",
      "Epoch 197: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.0203 - val_loss: 3072.1062 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3209.9973\n",
      "Epoch 198: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3067.1016 - val_loss: 3062.7932 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3026.8362\n",
      "Epoch 199: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.6504 - val_loss: 3063.4463 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3136.8945\n",
      "Epoch 200: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.0276 - val_loss: 3067.4126 - lr: 0.0010\n",
      "Epoch 201/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3036.3821\n",
      "Epoch 201: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.9177 - val_loss: 3062.3650 - lr: 0.0010\n",
      "Epoch 202/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3134.7947\n",
      "Epoch 202: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.0547 - val_loss: 3063.3003 - lr: 0.0010\n",
      "Epoch 203/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3226.0007\n",
      "Epoch 203: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3076.7925 - val_loss: 3068.6663 - lr: 0.0010\n",
      "Epoch 204/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3118.0088\n",
      "Epoch 204: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3070.6489 - val_loss: 3063.0371 - lr: 0.0010\n",
      "Epoch 205/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3043.3147\n",
      "Epoch 205: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.2449 - val_loss: 3064.8857 - lr: 0.0010\n",
      "Epoch 206/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3071.3657\n",
      "Epoch 206: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.0803 - val_loss: 3074.0493 - lr: 0.0010\n",
      "Epoch 207/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3015.7524\n",
      "Epoch 207: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.9641 - val_loss: 3072.5422 - lr: 0.0010\n",
      "Epoch 208/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3287.5464\n",
      "Epoch 208: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3073.5095 - val_loss: 3075.0132 - lr: 0.0010\n",
      "Epoch 209/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3071.6387\n",
      "Epoch 209: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3092.9226 - val_loss: 3098.6992 - lr: 0.0010\n",
      "Epoch 210/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3190.9595\n",
      "Epoch 210: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3104.0173 - val_loss: 3267.8115 - lr: 0.0010\n",
      "Epoch 211/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3427.5872\n",
      "Epoch 211: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3184.0237 - val_loss: 3151.0437 - lr: 0.0010\n",
      "Epoch 212/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3107.9199\n",
      "Epoch 212: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3149.4141 - val_loss: 3125.2515 - lr: 0.0010\n",
      "Epoch 213/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3345.4932\n",
      "Epoch 213: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3109.1179 - val_loss: 3126.1047 - lr: 0.0010\n",
      "Epoch 214/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3190.7573\n",
      "Epoch 214: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3086.0015 - val_loss: 3072.9871 - lr: 0.0010\n",
      "Epoch 215/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3126.4338\n",
      "Epoch 215: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.4263 - val_loss: 3077.5637 - lr: 0.0010\n",
      "Epoch 216/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3275.2815\n",
      "Epoch 216: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3082.8745 - val_loss: 3070.1711 - lr: 0.0010\n",
      "Epoch 217/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3113.3879\n",
      "Epoch 217: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.9871 - val_loss: 3065.7451 - lr: 0.0010\n",
      "Epoch 218/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3127.9990\n",
      "Epoch 218: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.9976 - val_loss: 3075.4478 - lr: 0.0010\n",
      "Epoch 219/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3098.9656\n",
      "Epoch 219: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.1516 - val_loss: 3065.2988 - lr: 0.0010\n",
      "Epoch 220/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3100.4150\n",
      "Epoch 220: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.3433 - val_loss: 3062.2534 - lr: 0.0010\n",
      "Epoch 221/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3002.0527\n",
      "Epoch 221: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.8867 - val_loss: 3067.0632 - lr: 0.0010\n",
      "Epoch 222/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3079.2539\n",
      "Epoch 222: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.6541 - val_loss: 3061.3171 - lr: 0.0010\n",
      "Epoch 223/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3056.8513\n",
      "Epoch 223: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.5684 - val_loss: 3065.8486 - lr: 0.0010\n",
      "Epoch 224/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3004.4207\n",
      "Epoch 224: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.6089 - val_loss: 3070.2183 - lr: 0.0010\n",
      "Epoch 225/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2986.5657\n",
      "Epoch 225: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.6538 - val_loss: 3066.2048 - lr: 0.0010\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3063.7080\n",
      "Epoch 226: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3063.7080 - val_loss: 3064.3958 - lr: 0.0010\n",
      "Epoch 227/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2715.8174\n",
      "Epoch 227: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.4402 - val_loss: 3067.0588 - lr: 0.0010\n",
      "Epoch 228/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3066.7866\n",
      "Epoch 228: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.9370 - val_loss: 3062.9988 - lr: 0.0010\n",
      "Epoch 229/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3079.4177\n",
      "Epoch 229: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3065.7478 - val_loss: 3063.0698 - lr: 0.0010\n",
      "Epoch 230/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3027.9692\n",
      "Epoch 230: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.6931 - val_loss: 3070.0215 - lr: 0.0010\n",
      "Epoch 231/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3415.9768\n",
      "Epoch 231: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3082.3391 - val_loss: 3123.5640 - lr: 0.0010\n",
      "Epoch 232/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3210.3870\n",
      "Epoch 232: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3164.0662 - val_loss: 3108.8506 - lr: 0.0010\n",
      "Epoch 233/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3274.6692\n",
      "Epoch 233: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3110.5557 - val_loss: 3079.7939 - lr: 0.0010\n",
      "Epoch 234/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2817.2400\n",
      "Epoch 234: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3087.8154 - val_loss: 3091.2041 - lr: 0.0010\n",
      "Epoch 235/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2913.4822\n",
      "Epoch 235: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.1460 - val_loss: 3066.4778 - lr: 0.0010\n",
      "Epoch 236/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3038.1287\n",
      "Epoch 236: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.3997 - val_loss: 3062.0330 - lr: 0.0010\n",
      "Epoch 237/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3143.1921\n",
      "Epoch 237: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3063.0400 - val_loss: 3061.3516 - lr: 0.0010\n",
      "Epoch 238/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3027.2683\n",
      "Epoch 238: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3063.4797 - val_loss: 3063.3708 - lr: 0.0010\n",
      "Epoch 239/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3056.3022\n",
      "Epoch 239: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.2549 - val_loss: 3069.7200 - lr: 0.0010\n",
      "Epoch 240/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3063.4424\n",
      "Epoch 240: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.9487 - val_loss: 3062.9524 - lr: 0.0010\n",
      "Epoch 241/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3305.5117\n",
      "Epoch 241: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.6064 - val_loss: 3063.9424 - lr: 0.0010\n",
      "Epoch 242/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3106.9470\n",
      "Epoch 242: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.1631 - val_loss: 3062.5952 - lr: 0.0010\n",
      "Epoch 243/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2679.6892\n",
      "Epoch 243: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.9993 - val_loss: 3063.9324 - lr: 0.0010\n",
      "Epoch 244/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2800.5911\n",
      "Epoch 244: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.6296 - val_loss: 3063.2756 - lr: 0.0010\n",
      "Epoch 245/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3073.0415\n",
      "Epoch 245: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.5044 - val_loss: 3066.0864 - lr: 0.0010\n",
      "Epoch 246/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3183.3760\n",
      "Epoch 246: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3064.1677 - val_loss: 3065.3730 - lr: 0.0010\n",
      "Epoch 247/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3281.2869\n",
      "Epoch 247: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3064.2480 - val_loss: 3065.2551 - lr: 0.0010\n",
      "Epoch 248/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3213.9397\n",
      "Epoch 248: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3062.0396 - val_loss: 3062.7427 - lr: 0.0010\n",
      "Epoch 249/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3034.9150\n",
      "Epoch 249: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3065.3035 - val_loss: 3070.7905 - lr: 0.0010\n",
      "Epoch 250/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3057.5591\n",
      "Epoch 250: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3076.9753 - val_loss: 3071.8806 - lr: 0.0010\n",
      "Epoch 251/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3395.8752\n",
      "Epoch 251: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3075.1414 - val_loss: 3068.6687 - lr: 0.0010\n",
      "Epoch 252/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3337.2041\n",
      "Epoch 252: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.7063 - val_loss: 3067.4834 - lr: 0.0010\n",
      "Epoch 253/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3262.8940\n",
      "Epoch 253: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3063.7854 - val_loss: 3064.1304 - lr: 0.0010\n",
      "Epoch 254/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2880.4399\n",
      "Epoch 254: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3063.7878 - val_loss: 3061.1680 - lr: 0.0010\n",
      "Epoch 255/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3003.2432\n",
      "Epoch 255: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3066.2083 - val_loss: 3064.3442 - lr: 0.0010\n",
      "Epoch 256/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2639.6487\n",
      "Epoch 256: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.0483 - val_loss: 3069.8284 - lr: 0.0010\n",
      "Epoch 257/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3332.5991\n",
      "Epoch 257: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.7488 - val_loss: 3062.6401 - lr: 0.0010\n",
      "Epoch 258/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3065.0017\n",
      "Epoch 258: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.2896 - val_loss: 3064.4746 - lr: 0.0010\n",
      "Epoch 259/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3158.2266\n",
      "Epoch 259: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.2793 - val_loss: 3068.2439 - lr: 0.0010\n",
      "Epoch 260/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3155.2085\n",
      "Epoch 260: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.8730 - val_loss: 3066.7141 - lr: 0.0010\n",
      "Epoch 261/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3077.9229\n",
      "Epoch 261: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.8528 - val_loss: 3062.4800 - lr: 0.0010\n",
      "Epoch 262/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3121.2737\n",
      "Epoch 262: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3063.5349 - val_loss: 3061.7488 - lr: 0.0010\n",
      "Epoch 263/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3131.1670\n",
      "Epoch 263: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3062.2258 - val_loss: 3062.6992 - lr: 0.0010\n",
      "Epoch 264/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3168.4382\n",
      "Epoch 264: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.9265 - val_loss: 3067.2805 - lr: 0.0010\n",
      "Epoch 265/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3148.8040\n",
      "Epoch 265: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.8521 - val_loss: 3065.2930 - lr: 0.0010\n",
      "Epoch 266/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3219.0989\n",
      "Epoch 266: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.3037 - val_loss: 3061.9995 - lr: 0.0010\n",
      "Epoch 267/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3078.9456\n",
      "Epoch 267: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3071.2241 - val_loss: 3066.5776 - lr: 0.0010\n",
      "Epoch 268/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3227.0371\n",
      "Epoch 268: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.7744 - val_loss: 3087.4224 - lr: 0.0010\n",
      "Epoch 269/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2872.6355\n",
      "Epoch 269: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3082.0627 - val_loss: 3381.9624 - lr: 0.0010\n",
      "Epoch 270/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3219.3757\n",
      "Epoch 270: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3192.7700 - val_loss: 3181.8765 - lr: 0.0010\n",
      "Epoch 271/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3248.5007\n",
      "Epoch 271: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3146.9814 - val_loss: 3184.8945 - lr: 0.0010\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3132.1895\n",
      "Epoch 272: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3132.1895 - val_loss: 3121.2124 - lr: 0.0010\n",
      "Epoch 273/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2875.8474\n",
      "Epoch 273: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3093.8306 - val_loss: 3090.9331 - lr: 0.0010\n",
      "Epoch 274/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 3506.3872\n",
      "Epoch 274: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3093.0215 - val_loss: 3100.4648 - lr: 0.0010\n",
      "Epoch 275/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3133.5464\n",
      "Epoch 275: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3091.2537 - val_loss: 3080.4048 - lr: 0.0010\n",
      "Epoch 276/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3174.7922\n",
      "Epoch 276: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3116.1096 - val_loss: 3096.1431 - lr: 0.0010\n",
      "Epoch 277/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3174.0994\n",
      "Epoch 277: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3082.9917 - val_loss: 3067.2930 - lr: 0.0010\n",
      "Epoch 278/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3187.5806\n",
      "Epoch 278: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.3000 - val_loss: 3069.6653 - lr: 0.0010\n",
      "Epoch 279/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3071.1277\n",
      "Epoch 279: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.4255 - val_loss: 3062.8394 - lr: 0.0010\n",
      "Epoch 280/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3172.0208\n",
      "Epoch 280: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.9719 - val_loss: 3062.1638 - lr: 0.0010\n",
      "Epoch 281/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2820.1287\n",
      "Epoch 281: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.9014 - val_loss: 3062.8420 - lr: 0.0010\n",
      "Epoch 282/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3059.0630\n",
      "Epoch 282: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.2478 - val_loss: 3062.1870 - lr: 0.0010\n",
      "Epoch 283/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3006.6321\n",
      "Epoch 283: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3061.8892 - val_loss: 3061.7722 - lr: 0.0010\n",
      "Epoch 284/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2976.2258\n",
      "Epoch 284: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.0959 - val_loss: 3062.7317 - lr: 0.0010\n",
      "Epoch 285/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2962.1626\n",
      "Epoch 285: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3084.4077 - val_loss: 3090.5750 - lr: 0.0010\n",
      "Epoch 286/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3052.0227\n",
      "Epoch 286: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3097.4065 - val_loss: 3071.0220 - lr: 0.0010\n",
      "Epoch 287/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3361.3745\n",
      "Epoch 287: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.5874 - val_loss: 3091.6785 - lr: 0.0010\n",
      "Epoch 288/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3074.5249\n",
      "Epoch 288: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3089.7915 - val_loss: 3084.2188 - lr: 0.0010\n",
      "Epoch 289/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2674.1917\n",
      "Epoch 289: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.2461 - val_loss: 3067.8611 - lr: 0.0010\n",
      "Epoch 290/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3069.3274\n",
      "Epoch 290: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3092.8301 - val_loss: 3078.0576 - lr: 0.0010\n",
      "Epoch 291/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2936.9624\n",
      "Epoch 291: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.1636 - val_loss: 3065.5327 - lr: 0.0010\n",
      "Epoch 292/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3106.9160\n",
      "Epoch 292: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3076.7686 - val_loss: 3087.1714 - lr: 0.0010\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3077.2327\n",
      "Epoch 293: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3077.2327 - val_loss: 3065.3247 - lr: 0.0010\n",
      "Epoch 294/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2989.0500\n",
      "Epoch 294: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3082.2449 - val_loss: 3075.9812 - lr: 0.0010\n",
      "Epoch 295/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3044.8892\n",
      "Epoch 295: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3080.9104 - val_loss: 3070.9805 - lr: 0.0010\n",
      "Epoch 296/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3102.3572\n",
      "Epoch 296: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3071.6018 - val_loss: 3077.2852 - lr: 0.0010\n",
      "Epoch 297/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2902.2344\n",
      "Epoch 297: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.8606 - val_loss: 3070.4531 - lr: 0.0010\n",
      "Epoch 298/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2987.2898\n",
      "Epoch 298: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3069.8823 - val_loss: 3089.7397 - lr: 0.0010\n",
      "Epoch 299/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2740.6768\n",
      "Epoch 299: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.9722 - val_loss: 3072.0496 - lr: 0.0010\n",
      "Epoch 300/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2992.5747\n",
      "Epoch 300: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3076.6328 - val_loss: 3067.3667 - lr: 0.0010\n",
      "Epoch 301/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3371.7866\n",
      "Epoch 301: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.8364 - val_loss: 3070.8738 - lr: 0.0010\n",
      "Epoch 302/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3052.7388\n",
      "Epoch 302: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3069.9089 - val_loss: 3090.4119 - lr: 0.0010\n",
      "Epoch 303/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2771.2310\n",
      "Epoch 303: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3086.4277 - val_loss: 3075.7966 - lr: 0.0010\n",
      "Epoch 304/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3026.7405\n",
      "Epoch 304: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.9976 - val_loss: 3065.9761 - lr: 0.0010\n",
      "Epoch 305/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3150.5759\n",
      "Epoch 305: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.5300 - val_loss: 3071.0723 - lr: 0.0010\n",
      "Epoch 306/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3311.6392\n",
      "Epoch 306: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.0176 - val_loss: 3071.4131 - lr: 0.0010\n",
      "Epoch 307/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3344.1162\n",
      "Epoch 307: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3103.2231 - val_loss: 3111.5830 - lr: 0.0010\n",
      "Epoch 308/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3034.2764\n",
      "Epoch 308: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3106.7744 - val_loss: 3133.8694 - lr: 0.0010\n",
      "Epoch 309/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3154.2085\n",
      "Epoch 309: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3093.6670 - val_loss: 3075.3669 - lr: 0.0010\n",
      "Epoch 310/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3165.2571\n",
      "Epoch 310: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.4299 - val_loss: 3065.6450 - lr: 0.0010\n",
      "Epoch 311/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3034.8872\n",
      "Epoch 311: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.7241 - val_loss: 3061.8740 - lr: 0.0010\n",
      "Epoch 312/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2698.7817\n",
      "Epoch 312: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3062.6611 - val_loss: 3064.9932 - lr: 0.0010\n",
      "Epoch 313/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2791.0278\n",
      "Epoch 313: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3068.1228 - val_loss: 3065.1565 - lr: 0.0010\n",
      "Epoch 314/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3216.1692\n",
      "Epoch 314: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3069.8074 - val_loss: 3063.5623 - lr: 0.0010\n",
      "Epoch 315/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3499.5227\n",
      "Epoch 315: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.3860 - val_loss: 3067.0251 - lr: 0.0010\n",
      "Epoch 316/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3130.1436\n",
      "Epoch 316: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.8584 - val_loss: 3065.5452 - lr: 0.0010\n",
      "Epoch 317/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2605.0596\n",
      "Epoch 317: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.1912 - val_loss: 3064.2598 - lr: 0.0010\n",
      "Epoch 318/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3202.3669\n",
      "Epoch 318: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3073.0249 - val_loss: 3085.5928 - lr: 0.0010\n",
      "Epoch 319/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3168.7900\n",
      "Epoch 319: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 3073.8362 - val_loss: 3063.8538 - lr: 0.0010\n",
      "Epoch 320/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3272.6929\n",
      "Epoch 320: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.8435 - val_loss: 3094.3552 - lr: 0.0010\n",
      "Epoch 321/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3292.2722\n",
      "Epoch 321: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3218.7969 - val_loss: 3118.2993 - lr: 0.0010\n",
      "Epoch 322/500\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 3192.3181\n",
      "Epoch 322: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3130.5300 - val_loss: 3081.4929 - lr: 0.0010\n",
      "Epoch 323/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3200.2212\n",
      "Epoch 323: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3083.0024 - val_loss: 3076.5369 - lr: 0.0010\n",
      "Epoch 324/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3045.5105\n",
      "Epoch 324: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.0386 - val_loss: 3071.9683 - lr: 0.0010\n",
      "Epoch 325/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3063.0244\n",
      "Epoch 325: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3075.0322 - val_loss: 3068.9490 - lr: 0.0010\n",
      "Epoch 326/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2909.6152\n",
      "Epoch 326: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3076.6245 - val_loss: 3073.7336 - lr: 0.0010\n",
      "Epoch 327/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3239.3083\n",
      "Epoch 327: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.0430 - val_loss: 3062.2976 - lr: 0.0010\n",
      "Epoch 328/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3058.6379\n",
      "Epoch 328: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.1094 - val_loss: 3064.2463 - lr: 0.0010\n",
      "Epoch 329/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3200.8401\n",
      "Epoch 329: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.1667 - val_loss: 3063.6772 - lr: 0.0010\n",
      "Epoch 330/500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 3100.4670\n",
      "Epoch 330: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3065.3350 - val_loss: 3061.0667 - lr: 0.0010\n",
      "Epoch 331/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3073.1667\n",
      "Epoch 331: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.8621 - val_loss: 3064.9736 - lr: 0.0010\n",
      "Epoch 332/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3144.2822\n",
      "Epoch 332: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.9436 - val_loss: 3070.6775 - lr: 0.0010\n",
      "Epoch 333/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3131.3999\n",
      "Epoch 333: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.4875 - val_loss: 3071.9131 - lr: 0.0010\n",
      "Epoch 334/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3010.0518\n",
      "Epoch 334: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.7197 - val_loss: 3061.9197 - lr: 0.0010\n",
      "Epoch 335/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3256.9106\n",
      "Epoch 335: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.9573 - val_loss: 3065.4490 - lr: 0.0010\n",
      "Epoch 336/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3136.8777\n",
      "Epoch 336: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3072.7053 - val_loss: 3064.9866 - lr: 0.0010\n",
      "Epoch 337/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3071.5967\n",
      "Epoch 337: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.0288 - val_loss: 3070.6289 - lr: 0.0010\n",
      "Epoch 338/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3298.2817\n",
      "Epoch 338: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.5256 - val_loss: 3094.2571 - lr: 0.0010\n",
      "Epoch 339/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3065.1833\n",
      "Epoch 339: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.2932 - val_loss: 3069.3870 - lr: 0.0010\n",
      "Epoch 340/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2877.5977\n",
      "Epoch 340: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.8369 - val_loss: 3073.5798 - lr: 0.0010\n",
      "Epoch 341/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3016.8293\n",
      "Epoch 341: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.2695 - val_loss: 3072.2759 - lr: 0.0010\n",
      "Epoch 342/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3158.8403\n",
      "Epoch 342: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.0996 - val_loss: 3061.1936 - lr: 0.0010\n",
      "Epoch 343/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2960.2839\n",
      "Epoch 343: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.6992 - val_loss: 3065.1594 - lr: 0.0010\n",
      "Epoch 344/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3123.5845\n",
      "Epoch 344: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.0781 - val_loss: 3079.5620 - lr: 0.0010\n",
      "Epoch 345/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3128.2029\n",
      "Epoch 345: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.6267 - val_loss: 3066.7043 - lr: 0.0010\n",
      "Epoch 346/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3177.5090\n",
      "Epoch 346: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.0957 - val_loss: 3071.1707 - lr: 0.0010\n",
      "Epoch 347/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3081.6421\n",
      "Epoch 347: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3067.7527 - val_loss: 3064.0381 - lr: 0.0010\n",
      "Epoch 348/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2972.8921\n",
      "Epoch 348: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.0825 - val_loss: 3077.3987 - lr: 0.0010\n",
      "Epoch 349/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3117.1248\n",
      "Epoch 349: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3080.8601 - val_loss: 3079.2246 - lr: 0.0010\n",
      "Epoch 350/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3162.7778\n",
      "Epoch 350: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3081.6660 - val_loss: 3072.0020 - lr: 0.0010\n",
      "Epoch 351/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3035.1128\n",
      "Epoch 351: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3135.5217 - val_loss: 3103.9175 - lr: 0.0010\n",
      "Epoch 352/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3087.7598\n",
      "Epoch 352: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3099.1772 - val_loss: 3071.5081 - lr: 0.0010\n",
      "Epoch 353/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3018.4326\n",
      "Epoch 353: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3089.4287 - val_loss: 3082.3809 - lr: 0.0010\n",
      "Epoch 354/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3208.8679\n",
      "Epoch 354: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3084.7122 - val_loss: 3075.1333 - lr: 0.0010\n",
      "Epoch 355/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3253.0249\n",
      "Epoch 355: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.6584 - val_loss: 3080.6221 - lr: 0.0010\n",
      "Epoch 356/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2708.1841\n",
      "Epoch 356: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.8818 - val_loss: 3102.2429 - lr: 0.0010\n",
      "Epoch 357/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3320.6743\n",
      "Epoch 357: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3106.4978 - val_loss: 3187.5623 - lr: 0.0010\n",
      "Epoch 358/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3072.2124\n",
      "Epoch 358: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3115.4805 - val_loss: 3301.1558 - lr: 0.0010\n",
      "Epoch 359/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3304.9470\n",
      "Epoch 359: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3227.8174 - val_loss: 3125.3918 - lr: 0.0010\n",
      "Epoch 360/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3261.2944\n",
      "Epoch 360: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3232.1343 - val_loss: 3162.9441 - lr: 0.0010\n",
      "Epoch 361/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3124.3909\n",
      "Epoch 361: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3162.2424 - val_loss: 3159.3657 - lr: 0.0010\n",
      "Epoch 362/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3267.5203\n",
      "Epoch 362: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3159.5872 - val_loss: 3103.1755 - lr: 0.0010\n",
      "Epoch 363/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3058.7854\n",
      "Epoch 363: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3093.4097 - val_loss: 3093.1218 - lr: 0.0010\n",
      "Epoch 364/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3003.4622\n",
      "Epoch 364: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3095.8308 - val_loss: 3080.8784 - lr: 0.0010\n",
      "Epoch 365/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3154.4028\n",
      "Epoch 365: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3080.3225 - val_loss: 3071.0664 - lr: 0.0010\n",
      "Epoch 366/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3455.3350\n",
      "Epoch 366: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3070.3186 - val_loss: 3067.6716 - lr: 0.0010\n",
      "Epoch 367/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2885.3743\n",
      "Epoch 367: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3075.6565 - val_loss: 3076.3965 - lr: 0.0010\n",
      "Epoch 368/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3107.4868\n",
      "Epoch 368: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3082.8792 - val_loss: 3067.9895 - lr: 0.0010\n",
      "Epoch 369/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2864.2051\n",
      "Epoch 369: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.4539 - val_loss: 3065.0813 - lr: 0.0010\n",
      "Epoch 370/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3130.0996\n",
      "Epoch 370: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3067.6577 - val_loss: 3066.3401 - lr: 0.0010\n",
      "Epoch 371/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3082.2329\n",
      "Epoch 371: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.6328 - val_loss: 3063.7380 - lr: 0.0010\n",
      "Epoch 372/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3178.6978\n",
      "Epoch 372: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.2881 - val_loss: 3062.0115 - lr: 0.0010\n",
      "Epoch 373/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3102.8574\n",
      "Epoch 373: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.0061 - val_loss: 3065.6174 - lr: 0.0010\n",
      "Epoch 374/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3195.9473\n",
      "Epoch 374: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.2998 - val_loss: 3073.3687 - lr: 0.0010\n",
      "Epoch 375/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3213.3652\n",
      "Epoch 375: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.0271 - val_loss: 3067.7688 - lr: 0.0010\n",
      "Epoch 376/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3202.7378\n",
      "Epoch 376: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.2874 - val_loss: 3079.6667 - lr: 0.0010\n",
      "Epoch 377/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3107.1943\n",
      "Epoch 377: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.9280 - val_loss: 3075.7209 - lr: 0.0010\n",
      "Epoch 378/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3288.0059\n",
      "Epoch 378: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.8809 - val_loss: 3064.7549 - lr: 0.0010\n",
      "Epoch 379/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2822.7085\n",
      "Epoch 379: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.4097 - val_loss: 3061.8521 - lr: 0.0010\n",
      "Epoch 380/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2974.8108\n",
      "Epoch 380: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.0046 - val_loss: 3061.0776 - lr: 0.0010\n",
      "Epoch 381/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3152.2458\n",
      "Epoch 381: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.8625 - val_loss: 3062.2368 - lr: 0.0010\n",
      "Epoch 382/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3126.4543\n",
      "Epoch 382: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.3008 - val_loss: 3063.2136 - lr: 0.0010\n",
      "Epoch 383/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3186.3965\n",
      "Epoch 383: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.6011 - val_loss: 3061.9629 - lr: 0.0010\n",
      "Epoch 384/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3204.4851\n",
      "Epoch 384: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.5479 - val_loss: 3074.3315 - lr: 0.0010\n",
      "Epoch 385/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3239.5945\n",
      "Epoch 385: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3080.3413 - val_loss: 3093.4966 - lr: 0.0010\n",
      "Epoch 386/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2987.8018\n",
      "Epoch 386: val_loss did not improve from 3061.00854\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.8557 - val_loss: 3074.2427 - lr: 0.0010\n",
      "Epoch 387/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3184.9463\n",
      "Epoch 387: val_loss improved from 3061.00854 to 3060.64795, saving model to data/models\\reports_losss.keras\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3065.5466 - val_loss: 3060.6479 - lr: 0.0010\n",
      "Epoch 388/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3025.9592\n",
      "Epoch 388: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.9797 - val_loss: 3061.7852 - lr: 0.0010\n",
      "Epoch 389/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3056.4521\n",
      "Epoch 389: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.4932 - val_loss: 3063.1321 - lr: 0.0010\n",
      "Epoch 390/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2965.1091\n",
      "Epoch 390: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.0618 - val_loss: 3062.6541 - lr: 0.0010\n",
      "Epoch 391/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2915.0283\n",
      "Epoch 391: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3065.1060 - val_loss: 3065.4619 - lr: 0.0010\n",
      "Epoch 392/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3062.4167\n",
      "Epoch 392: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.1025 - val_loss: 3061.6460 - lr: 0.0010\n",
      "Epoch 393/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3036.5774\n",
      "Epoch 393: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.4065 - val_loss: 3065.1687 - lr: 0.0010\n",
      "Epoch 394/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3059.7234\n",
      "Epoch 394: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.5791 - val_loss: 3063.0789 - lr: 0.0010\n",
      "Epoch 395/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3153.7900\n",
      "Epoch 395: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3063.4995 - val_loss: 3060.6902 - lr: 0.0010\n",
      "Epoch 396/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3312.0989\n",
      "Epoch 396: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3062.8677 - val_loss: 3067.1763 - lr: 0.0010\n",
      "Epoch 397/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3038.5994\n",
      "Epoch 397: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.6250 - val_loss: 3065.7043 - lr: 0.0010\n",
      "Epoch 398/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3118.0889\n",
      "Epoch 398: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.5100 - val_loss: 3077.4219 - lr: 0.0010\n",
      "Epoch 399/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3519.5974\n",
      "Epoch 399: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3421.9924 - val_loss: 3487.6575 - lr: 0.0010\n",
      "Epoch 400/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3384.6909\n",
      "Epoch 400: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3293.2097 - val_loss: 3174.5352 - lr: 0.0010\n",
      "Epoch 401/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3456.9980\n",
      "Epoch 401: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3329.6619 - val_loss: 3621.5769 - lr: 0.0010\n",
      "Epoch 402/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3277.7117\n",
      "Epoch 402: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3386.6414 - val_loss: 3148.9629 - lr: 0.0010\n",
      "Epoch 403/500\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 3104.3508\n",
      "Epoch 403: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3209.6599 - val_loss: 3201.8215 - lr: 0.0010\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3159.4983\n",
      "Epoch 404: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3159.4983 - val_loss: 3123.3005 - lr: 0.0010\n",
      "Epoch 405/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3095.7556\n",
      "Epoch 405: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3111.3972 - val_loss: 3096.9570 - lr: 0.0010\n",
      "Epoch 406/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3334.5081\n",
      "Epoch 406: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3093.6677 - val_loss: 3085.6252 - lr: 0.0010\n",
      "Epoch 407/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3331.1707\n",
      "Epoch 407: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3083.8972 - val_loss: 3076.4478 - lr: 0.0010\n",
      "Epoch 408/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2844.1511\n",
      "Epoch 408: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.4976 - val_loss: 3078.2244 - lr: 0.0010\n",
      "Epoch 409/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 2827.0127\n",
      "Epoch 409: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3075.0142 - val_loss: 3072.5442 - lr: 0.0010\n",
      "Epoch 410/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3064.6880\n",
      "Epoch 410: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3074.0369 - val_loss: 3073.3811 - lr: 0.0010\n",
      "Epoch 411/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3256.6951\n",
      "Epoch 411: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.6755 - val_loss: 3083.9246 - lr: 0.0010\n",
      "Epoch 412/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3257.6001\n",
      "Epoch 412: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.6333 - val_loss: 3071.2988 - lr: 0.0010\n",
      "Epoch 413/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3074.5583\n",
      "Epoch 413: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3076.1382 - val_loss: 3072.3872 - lr: 0.0010\n",
      "Epoch 414/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3217.6384\n",
      "Epoch 414: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3074.6958 - val_loss: 3073.4680 - lr: 0.0010\n",
      "Epoch 415/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2871.2769\n",
      "Epoch 415: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.8936 - val_loss: 3069.4204 - lr: 0.0010\n",
      "Epoch 416/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3358.6494\n",
      "Epoch 416: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3069.6104 - val_loss: 3071.0364 - lr: 0.0010\n",
      "Epoch 417/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3063.7302\n",
      "Epoch 417: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 3070.9202 - val_loss: 3068.9055 - lr: 0.0010\n",
      "Epoch 418/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3023.4790\n",
      "Epoch 418: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3068.9365 - val_loss: 3068.6108 - lr: 0.0010\n",
      "Epoch 419/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3273.6018\n",
      "Epoch 419: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3069.6970 - val_loss: 3069.0723 - lr: 0.0010\n",
      "Epoch 420/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3173.8970\n",
      "Epoch 420: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.2202 - val_loss: 3068.0166 - lr: 0.0010\n",
      "Epoch 421/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3248.6609\n",
      "Epoch 421: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.7107 - val_loss: 3069.1279 - lr: 0.0010\n",
      "Epoch 422/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3212.0984\n",
      "Epoch 422: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.9011 - val_loss: 3067.9885 - lr: 0.0010\n",
      "Epoch 423/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3062.6772\n",
      "Epoch 423: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.1453 - val_loss: 3067.7542 - lr: 0.0010\n",
      "Epoch 424/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3215.0825\n",
      "Epoch 424: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.3567 - val_loss: 3065.9033 - lr: 0.0010\n",
      "Epoch 425/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3109.2837\n",
      "Epoch 425: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.6707 - val_loss: 3065.6345 - lr: 0.0010\n",
      "Epoch 426/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3248.6296\n",
      "Epoch 426: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.9148 - val_loss: 3075.2847 - lr: 0.0010\n",
      "Epoch 427/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3111.2507\n",
      "Epoch 427: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.2166 - val_loss: 3065.4753 - lr: 0.0010\n",
      "Epoch 428/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3156.8096\n",
      "Epoch 428: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.8042 - val_loss: 3072.8806 - lr: 0.0010\n",
      "Epoch 429/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3157.7947\n",
      "Epoch 429: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3073.7639 - val_loss: 3068.4702 - lr: 0.0010\n",
      "Epoch 430/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3070.9680\n",
      "Epoch 430: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.4170 - val_loss: 3065.8726 - lr: 0.0010\n",
      "Epoch 431/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3361.6370\n",
      "Epoch 431: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.6533 - val_loss: 3071.8862 - lr: 0.0010\n",
      "Epoch 432/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3072.8450\n",
      "Epoch 432: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3068.4331 - val_loss: 3074.4849 - lr: 0.0010\n",
      "Epoch 433/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 3018.7236\n",
      "Epoch 433: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.8167 - val_loss: 3065.5806 - lr: 0.0010\n",
      "Epoch 434/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3267.3450\n",
      "Epoch 434: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3066.6482 - val_loss: 3069.8276 - lr: 0.0010\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3070.9211\n",
      "Epoch 435: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3070.9211 - val_loss: 3065.7949 - lr: 0.0010\n",
      "Epoch 436/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 2734.2573\n",
      "Epoch 436: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3069.1602 - val_loss: 3072.5073 - lr: 0.0010\n",
      "Epoch 437/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3131.9829\n",
      "Epoch 437: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3076.0425 - val_loss: 3069.5151 - lr: 0.0010\n",
      "Epoch 438/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 2986.6094\n",
      "Epoch 438: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3070.6594 - val_loss: 3067.9419 - lr: 0.0010\n",
      "Epoch 439/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3203.6350\n",
      "Epoch 439: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.6362 - val_loss: 3072.3477 - lr: 0.0010\n",
      "Epoch 440/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3358.0854\n",
      "Epoch 440: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.6704 - val_loss: 3075.9194 - lr: 0.0010\n",
      "Epoch 441/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2945.8530\n",
      "Epoch 441: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.5732 - val_loss: 3077.3394 - lr: 0.0010\n",
      "Epoch 442/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3114.2339\n",
      "Epoch 442: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3080.9011 - val_loss: 3066.6375 - lr: 0.0010\n",
      "Epoch 443/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2988.0339\n",
      "Epoch 443: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3084.4302 - val_loss: 3080.7307 - lr: 0.0010\n",
      "Epoch 444/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2990.0056\n",
      "Epoch 444: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3076.9338 - val_loss: 3071.5024 - lr: 0.0010\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3073.8176\n",
      "Epoch 445: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3073.8176 - val_loss: 3075.9023 - lr: 0.0010\n",
      "Epoch 446/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2670.3240\n",
      "Epoch 446: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3069.3540 - val_loss: 3066.5547 - lr: 0.0010\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3067.8401\n",
      "Epoch 447: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3067.8401 - val_loss: 3064.7336 - lr: 0.0010\n",
      "Epoch 448/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3082.9514\n",
      "Epoch 448: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3071.2942 - val_loss: 3089.1140 - lr: 0.0010\n",
      "Epoch 449/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3316.0464\n",
      "Epoch 449: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.8442 - val_loss: 3066.4114 - lr: 0.0010\n",
      "Epoch 450/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3222.5200\n",
      "Epoch 450: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.5244 - val_loss: 3075.2981 - lr: 0.0010\n",
      "Epoch 451/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3184.3826\n",
      "Epoch 451: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3096.3684 - val_loss: 3097.4648 - lr: 0.0010\n",
      "Epoch 452/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3028.6997\n",
      "Epoch 452: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3091.5117 - val_loss: 3091.5115 - lr: 0.0010\n",
      "Epoch 453/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2865.5095\n",
      "Epoch 453: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3087.8086 - val_loss: 3082.7976 - lr: 0.0010\n",
      "Epoch 454/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2951.1938\n",
      "Epoch 454: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.7537 - val_loss: 3071.5029 - lr: 0.0010\n",
      "Epoch 455/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3174.3513\n",
      "Epoch 455: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.4736 - val_loss: 3069.6611 - lr: 0.0010\n",
      "Epoch 456/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2742.4143\n",
      "Epoch 456: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.9148 - val_loss: 3067.1799 - lr: 0.0010\n",
      "Epoch 457/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3227.2715\n",
      "Epoch 457: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.5400 - val_loss: 3075.5500 - lr: 0.0010\n",
      "Epoch 458/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3001.7190\n",
      "Epoch 458: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.2319 - val_loss: 3066.5730 - lr: 0.0010\n",
      "Epoch 459/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3283.0862\n",
      "Epoch 459: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.7239 - val_loss: 3065.8013 - lr: 0.0010\n",
      "Epoch 460/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3110.4033\n",
      "Epoch 460: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.8452 - val_loss: 3075.9094 - lr: 0.0010\n",
      "Epoch 461/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3079.0977\n",
      "Epoch 461: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3067.5510 - val_loss: 3066.4490 - lr: 0.0010\n",
      "Epoch 462/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3267.0642\n",
      "Epoch 462: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.9509 - val_loss: 3067.3022 - lr: 0.0010\n",
      "Epoch 463/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2987.2817\n",
      "Epoch 463: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3066.5415 - val_loss: 3065.2668 - lr: 0.0010\n",
      "Epoch 464/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3448.0168\n",
      "Epoch 464: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3067.4580 - val_loss: 3063.5256 - lr: 0.0010\n",
      "Epoch 465/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3079.7175\n",
      "Epoch 465: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.7341 - val_loss: 3066.8232 - lr: 0.0010\n",
      "Epoch 466/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2729.8132\n",
      "Epoch 466: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.1511 - val_loss: 3075.0623 - lr: 0.0010\n",
      "Epoch 467/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3043.2310\n",
      "Epoch 467: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.1086 - val_loss: 3113.5776 - lr: 0.0010\n",
      "Epoch 468/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3183.8625\n",
      "Epoch 468: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3079.4136 - val_loss: 3073.4241 - lr: 0.0010\n",
      "Epoch 469/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3006.0378\n",
      "Epoch 469: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.1582 - val_loss: 3069.3738 - lr: 0.0010\n",
      "Epoch 470/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3228.7761\n",
      "Epoch 470: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.1333 - val_loss: 3068.1414 - lr: 0.0010\n",
      "Epoch 471/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3212.4727\n",
      "Epoch 471: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3074.6077 - val_loss: 3081.8242 - lr: 0.0010\n",
      "Epoch 472/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3321.6199\n",
      "Epoch 472: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3091.6667 - val_loss: 3076.1987 - lr: 0.0010\n",
      "Epoch 473/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3028.0396\n",
      "Epoch 473: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3084.0417 - val_loss: 3074.6030 - lr: 0.0010\n",
      "Epoch 474/500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 3113.7661\n",
      "Epoch 474: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 3115.5266 - val_loss: 3087.8374 - lr: 0.0010\n",
      "Epoch 475/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3369.2566\n",
      "Epoch 475: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3091.3584 - val_loss: 3086.4104 - lr: 0.0010\n",
      "Epoch 476/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3348.3306\n",
      "Epoch 476: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3125.2695 - val_loss: 3087.6902 - lr: 0.0010\n",
      "Epoch 477/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3313.6375\n",
      "Epoch 477: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3091.6055 - val_loss: 3079.0552 - lr: 0.0010\n",
      "Epoch 478/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3059.0195\n",
      "Epoch 478: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.4751 - val_loss: 3073.0151 - lr: 0.0010\n",
      "Epoch 479/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2928.5051\n",
      "Epoch 479: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.3347 - val_loss: 3072.5789 - lr: 0.0010\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 3074.6843\n",
      "Epoch 480: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 3074.6843 - val_loss: 3087.1648 - lr: 0.0010\n",
      "Epoch 481/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2910.3848\n",
      "Epoch 481: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3091.1672 - val_loss: 3074.1262 - lr: 0.0010\n",
      "Epoch 482/500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 2480.8508\n",
      "Epoch 482: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3070.9202 - val_loss: 3073.9119 - lr: 0.0010\n",
      "Epoch 483/500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 2890.6018\n",
      "Epoch 483: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3070.9404 - val_loss: 3067.6804 - lr: 0.0010\n",
      "Epoch 484/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 2582.0039\n",
      "Epoch 484: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.7266 - val_loss: 3083.4700 - lr: 0.0010\n",
      "Epoch 485/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 2962.3960\n",
      "Epoch 485: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3082.7976 - val_loss: 3085.0017 - lr: 0.0010\n",
      "Epoch 486/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3145.6812\n",
      "Epoch 486: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3078.1514 - val_loss: 3080.1738 - lr: 0.0010\n",
      "Epoch 487/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3126.9934\n",
      "Epoch 487: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.0649 - val_loss: 3067.0396 - lr: 0.0010\n",
      "Epoch 488/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 3098.6357\n",
      "Epoch 488: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.6509 - val_loss: 3069.5945 - lr: 0.0010\n",
      "Epoch 489/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3101.4192\n",
      "Epoch 489: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3072.6472 - val_loss: 3071.1807 - lr: 0.0010\n",
      "Epoch 490/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3084.4827\n",
      "Epoch 490: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.3350 - val_loss: 3065.0789 - lr: 0.0010\n",
      "Epoch 491/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3018.1948\n",
      "Epoch 491: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3064.3020 - val_loss: 3073.0857 - lr: 0.0010\n",
      "Epoch 492/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2988.4180\n",
      "Epoch 492: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3068.7747 - val_loss: 3081.5530 - lr: 0.0010\n",
      "Epoch 493/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3057.7556\n",
      "Epoch 493: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.0291 - val_loss: 3069.5588 - lr: 0.0010\n",
      "Epoch 494/500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 3143.8643\n",
      "Epoch 494: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 3071.8901 - val_loss: 3083.9014 - lr: 0.0010\n",
      "Epoch 495/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 3081.3838\n",
      "Epoch 495: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3073.5935 - val_loss: 3073.0347 - lr: 0.0010\n",
      "Epoch 496/500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 2943.6062\n",
      "Epoch 496: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3069.4885 - val_loss: 3066.5332 - lr: 0.0010\n",
      "Epoch 497/500\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 2854.0911\n",
      "Epoch 497: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3077.3342 - val_loss: 3079.8660 - lr: 0.0010\n",
      "Epoch 498/500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 3172.8499\n",
      "Epoch 498: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3143.2656 - val_loss: 3147.2861 - lr: 0.0010\n",
      "Epoch 499/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3096.5181\n",
      "Epoch 499: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3129.6973 - val_loss: 3122.9170 - lr: 0.0010\n",
      "Epoch 500/500\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 3051.9380\n",
      "Epoch 500: val_loss did not improve from 3060.64795\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 3115.2488 - val_loss: 3099.8091 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.set_value(model.optimizer.learning_rate, 0.0001)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(filepath='data/models/reports_losss.keras', save_best_only=True,\n",
    "                                              monitor='val_loss', mode='min', verbose=1, save_weights_only=False,\n",
    "                                              save_freq='epoch')\n",
    "\n",
    "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                                      patience=10000, verbose=1, mode='auto')\n",
    "history = model.fit(np.array(x_train), np.array(y_train),\n",
    "                    epochs=500, batch_size=20, shuffle=True,\n",
    "                    validation_data=(np.array(x_train), np.array(y_train)), callbacks=[mcp_save, reduce_lr_loss])\n",
    "plot_loss(history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:58:26.616021Z",
     "start_time": "2024-10-12T05:56:18.411899Z"
    }
   },
   "id": "191dc523763f2938"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('data/models/reports_losss.keras', custom_objects={'abs': tf.math.abs, 'custom_loss': custom_loss})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T06:13:13.900148Z",
     "start_time": "2024-10-12T06:13:13.779072Z"
    }
   },
   "id": "c9a1a6a8cf9f8ba1"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000016E8B0BB9A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# prediction = model.evaluate(np.array(x_test), np.array(y_test))\n",
    "prediction = model.predict(np.array(x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T06:13:14.367040Z",
     "start_time": "2024-10-12T06:13:14.251069Z"
    }
   },
   "id": "d7c4e1ecdee1bcf7"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# test on x_train\n",
    "def fetch_prediction(prediction):\n",
    "    return prediction[:128]\n",
    "\n",
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        k = np.random.randint(0, len(x_test))\n",
    "        x_scale_factor = max(pore_widths) / len(x_test[k])\n",
    "        axis[i, j].plot(pore_widths / x_scale_factor, fetch_prediction(prediction[k]), marker=\".\", label=f\"Prediction\")\n",
    "        axis[i, j].plot(pore_widths / x_scale_factor, y_test[k], marker=\".\", label=\"Real distribution\")\n",
    "        axis[i, j].plot(pressures[77:-10]*500, x_test[k], label=\"Isotherm\")\n",
    "        kernal = (data_sorb.T[40:])\n",
    "        axis[i, j].plot(pressures[40:]*500, np.dot(kernal, prediction[k][:128]), label=\"New Isotherm\")\n",
    "        axis[i, j].set_title(f\" {k}\")\n",
    "        axis[i, j].title.set_size(10)\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plt.legend()\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T06:14:20.089871Z",
     "start_time": "2024-10-12T06:13:15.178073Z"
    }
   },
   "id": "bbc48118bf79bc08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test with test Generator\n",
    "from tools import TestApp\n",
    "\n",
    "gen = Generator(path_s=\"data/initial kernels/Kernel_Silica_Adsorption.npy\",\n",
    "                path_d=\"data/initial kernels/Kernel_Silica_Desorption.npy\",\n",
    "                path_p_d=\"data/initial kernels/Pressure_Silica.npy\",\n",
    "                path_p_s=\"data/initial kernels/Pressure_Silica.npy\",\n",
    "                path_a=\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\"\n",
    "                )\n",
    "# gen = Generator(path_s=\"data/initial kernels/Kernel_Carbon_Adsorption.npy\",\n",
    "#                               path_d=\"data/initial kernels/Kernel_Carbon_Desorption.npy\",\n",
    "#                               path_p_d=\"data/initial kernels/Pressure_Carbon.npy\",\n",
    "#                               path_p_s=\"data/initial kernels/Pressure_Carbon.npy\",\n",
    "#                               path_a=\"data/initial kernels/Size_Kernel_Carbon_Adsorption.npy\"\n",
    "#                             )\n",
    "\n",
    "#TestApp.App(model, gen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "b25b4132001702fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_file_list = [\"MCM-41\", \"SBA-15\", \"SBA-16\", \"MIL-101\", \"MIL-101_2\", \"DUT-49\", \"FDM-4\", \"PCN-333\", \"PCN-777\",\n",
    "                 \"MIL-100\"]\n",
    "\n",
    "p_exp_list = []\n",
    "n_s_exp_raw_list = []\n",
    "for exp_file_name in exp_file_list:\n",
    "    data = pd.read_csv(f\"data/real/{exp_file_name}.txt\", header=None)\n",
    "    # p_exp_list.append(data.iloc[:,1].to_numpy())\n",
    "    # n_s_exp_raw_list.append(data.iloc[:,3].to_numpy())\n",
    "    p_exp_list.append(data.iloc[:, 1].to_numpy())\n",
    "    n_s_exp_raw_list.append(data.iloc[:, 3].to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "d0179658dd4e6c12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "j = 2\n",
    "plt.plot(p_exp_list[j], n_s_exp_raw_list[j], marker=\".\", label=exp_file_list[j])\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "316deb7add602c0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#      \n",
    "n_s_exp_list = []\n",
    "for i in range(len(p_exp_list)):\n",
    "    n_s_exp_list.append(np.interp(gen.pressures_s[77:367], p_exp_list[i], n_s_exp_raw_list[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "bfb8c81559e024d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "j = 2\n",
    "plt.plot(gen.pressures_s[77:367], n_s_exp_list[j], marker=\".\", label=exp_file_list[j])\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "61ca6d25ee7153f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_s_exp_for_net_list = [pre_process_isotherm(np.copy(n_s_exp), scale=False) for n_s_exp in n_s_exp_list]\n",
    "fit_exp_list = [model.predict(np.array([n_s_exp_for_net])).T for n_s_exp_for_net in n_s_exp_for_net_list]\n",
    "fit_exp_list = [fetch_prediction(i) for i in fit_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "964f2bfe8f0bb107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_exp_list[k]) / max(n_s_exp_raw_list[k])\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k], marker=\".\", label=f\"Distribution\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k],\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        \n",
    "        kernal = (data_sorb.T[40:])\n",
    "        axis[i, j].plot(gen.pressures_s[40:458]* x_scale_factor, np.dot(kernal, fit_exp_list[k]), label=\"Isotherm by distribution\")\n",
    "        \n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_exp_list[k])], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "dced44b8e1da2a3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Classic\n",
    "import inverse\n",
    "\n",
    "kernel = np.load(\"data/initial kernels/Kernel_Silica_Adsorption.npy\")\n",
    "\n",
    "### normalize on size\n",
    "# a_array = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "# for i in range(len(a_array)):\n",
    "#     kernel[i] /= a_array[i]\n",
    "#     kernel[i] /= a_array[i]\n",
    "###\n",
    "\n",
    "cut_kernel = []\n",
    "for i in range(len(kernel)):\n",
    "    cut_kernel.append(kernel[i][40:458])\n",
    "cut_kernel = np.array(cut_kernel)\n",
    "fit_classic_list = [inverse.fit_SLSQP(adsorption=n_s, kernel=cut_kernel, a_array=pore_widths) for n_s in n_s_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "435b91d710c2ce9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "importlib.reload(inverse)\n",
    "\n",
    "\n",
    "def create_regularization_animation(file):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5, True)\n",
    "    fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=0)\n",
    "    line1, = ax.plot(pore_widths[:-30], fit_classic.x[:-30], marker=\".\", label=f\"a = {0}\")\n",
    "\n",
    "    y_scale_factor = max(fit_classic.x) / max(fit_exp_list[2])\n",
    "    #plt.plot(pore_widths, fit_exp_list[2] * y_scale_factor, marker=\".\", label=f\" \")\n",
    "\n",
    "    ax.set_ylabel(\" , $^3$/  * \")\n",
    "    ax.set_xlabel(\" , \")\n",
    "\n",
    "    L = plt.legend(loc=1)  # Define legend objects\n",
    "\n",
    "    def update(frame):\n",
    "        a = frame / 4 + 2\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        line1.set_ydata(fit_classic.x[:-30])\n",
    "        L.get_texts()[0].set_text(\n",
    "            f\"   ,    = {round(a-2,1)}\")  # Update label each at frame\n",
    "        return line1,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig=fig, func=update, frames=100, interval=100)\n",
    "    writervideo = animation.FFMpegWriter(fps=30)\n",
    "    ani.save(file, writer=writervideo, dpi=200)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "create_regularization_animation(\"SBA-16_regularization.mp4\")\n",
    "def plot_regularization_graphs():\n",
    "    for a in [1, 5, 10, 50]:\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        plt.plot(pore_widths, fit_classic.x, marker=\".\", label=f\" = {a}\")\n",
    "    plt.ylabel(\" , $^3$/  * \")\n",
    "    plt.xlabel(\" , \")\n",
    "    plot()\n",
    "#plot_regularization_graphs()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "1b0c708ab92c81f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_classic_list[k].x) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_net = max(fit_classic_list[k].x) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k] * y_scale_factor,\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_classic_list[k].x)], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "aac060bb7ff7720e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n",
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n"
     ]
    }
   ],
   "source": [
    "### Classic\n",
    "import inverse\n",
    "\n",
    "kernel = np.load(\"data/initial kernels/Kernel_Silica_Adsorption.npy\")\n",
    "\n",
    "### normalize on size\n",
    "# a_array = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "# for i in range(len(a_array)):\n",
    "#     kernel[i] /= a_array[i]\n",
    "#     kernel[i] /= a_array[i]\n",
    "###\n",
    "\n",
    "cut_kernel = []\n",
    "for i in range(len(kernel)):\n",
    "    cut_kernel.append(kernel[i][40:458])\n",
    "cut_kernel = np.array(cut_kernel)\n",
    "fit_classic_list = [inverse.fit_SLSQP(adsorption=n_s, kernel=cut_kernel, a_array=pore_widths) for n_s in n_s_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T13:40:22.307670Z",
     "start_time": "2024-06-18T13:39:59.907685Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "9949a77c1d2e89c0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n",
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "importlib.reload(inverse)\n",
    "\n",
    "\n",
    "def create_regularization_animation(file):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5, True)\n",
    "    fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=0)\n",
    "    line1, = ax.plot(pore_widths[:-30], fit_classic.x[:-30], marker=\".\", label=f\"a = {0}\")\n",
    "\n",
    "    y_scale_factor = max(fit_classic.x) / max(fit_exp_list[2])\n",
    "    #plt.plot(pore_widths, fit_exp_list[2] * y_scale_factor, marker=\".\", label=f\" \")\n",
    "\n",
    "    ax.set_ylabel(\" , $^3$/  * \")\n",
    "    ax.set_xlabel(\" , \")\n",
    "\n",
    "    L = plt.legend(loc=1)  # Define legend objects\n",
    "\n",
    "    def update(frame):\n",
    "        a = frame / 4 + 2\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        line1.set_ydata(fit_classic.x[:-30])\n",
    "        L.get_texts()[0].set_text(\n",
    "            f\"   ,    = {round(a-2,1)}\")  # Update label each at frame\n",
    "        return line1,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig=fig, func=update, frames=100, interval=100)\n",
    "    writervideo = animation.FFMpegWriter(fps=30)\n",
    "    ani.save(file, writer=writervideo, dpi=200)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "create_regularization_animation(\"SBA-16_regularization.mp4\")\n",
    "def plot_regularization_graphs():\n",
    "    for a in [1, 5, 10, 50]:\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        plt.plot(pore_widths, fit_classic.x, marker=\".\", label=f\" = {a}\")\n",
    "    plt.ylabel(\" , $^3$/  * \")\n",
    "    plt.xlabel(\" , \")\n",
    "    plot()\n",
    "#plot_regularization_graphs()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:50:01.331621Z",
     "start_time": "2024-06-18T14:46:23.487577Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "ca3051b964f378a",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_classic_list[k].x) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_net = max(fit_classic_list[k].x) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k] * y_scale_factor,\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_classic_list[k].x)], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "707d030f7db4a23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare net, classic, quantachrome distributions\n",
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "\n",
    "\n",
    "def calculate_isotherm_by_distribution(generator: Generator, a_array, distribution):\n",
    "    generator.a_array = a_array\n",
    "    generator.pore_distribution = distribution\n",
    "    generator.calculate_isotherms()\n",
    "    return generator.n_s\n",
    "\n",
    "\n",
    "#/np.ediff1d(pore_widths, to_begin=pore_widths[0])\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        quantachrome_pore_size = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "                0] / 10 * 2  # /10 -    * 2 -  QH  - Half pore width.\n",
    "        quantachrome_dV = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "        # isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "        # y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "        # quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "        y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "        y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x * y_scale_factor_classic, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(quantachrome_pore_size, quantachrome_dV, marker=\".\", label=f\"quantachrome\")\n",
    "        axis[i, j].set_title(f\"{exp_file_list[k]}\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "8e0b9fbf987405b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "plt = reload(plt)\n",
    "\n",
    "k = 0\n",
    "\n",
    "quantachrome_pore_size = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "        0] / 10 * 2  # /10 -    * 2 -  QH  - Half pore width.\n",
    "quantachrome_dV = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "# isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "# y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "# quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "plt.plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\" \")\n",
    "plt.plot(pore_widths, fit_classic_list[k].x * y_scale_factor_classic, marker=\".\", label=f\" \")\n",
    "#plt.plot(quantachrome_pore_size, quantachrome_dV, marker=\".\", label=f\"  QH\") \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.title(f\"{exp_file_list[k]}\")\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.subplots_adjust(left=0.15,\n",
    "                    bottom=0.133,\n",
    "                    right=0.979,\n",
    "                    top=0.917,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylabel(\" , $^3$/  * \")\n",
    "plt.xlabel(\" , \")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "c5e26ea811ae701c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation 2\n",
    "k = 3\n",
    "plt.plot(p_exp_list[k], n_s_exp_raw_list[k], marker=\".\", color='b', label=f\"{exp_file_list[k]}\")\n",
    "k = 9\n",
    "plt.plot(p_exp_list[k], n_s_exp_raw_list[k], marker=\".\", color='r', label=f\"{exp_file_list[k]}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.title(f\" \")\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.subplots_adjust(left=0.15,\n",
    "                    bottom=0.133,\n",
    "                    right=0.979,\n",
    "                    top=0.917,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylabel(\", $^3$/\")\n",
    "plt.xlabel(\", $P/P_{0}$\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "be93198745d6bc2d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation 3\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6, 7))\n",
    "ax1.set_xlabel(\" , \")\n",
    "ax1.set_ylabel(\" , $^3$/  * \")\n",
    "k = 0\n",
    "plt.title(f\"{exp_file_list[k]}\")\n",
    "quantachrome_pore_size = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "        0] / 10 * 2  # /10 -    * 2 -  QH  - Half pore width.\n",
    "quantachrome_dV = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "# isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "# y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "# quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "ax1.plot(pore_widths[0:100], (fit_exp_list[k] * y_scale_factor_net)[0:100], marker=\".\", label=f\" \")\n",
    "ax1.plot(pore_widths[0:100], (fit_classic_list[k].x * y_scale_factor_classic)[0:100], marker=\".\",\n",
    "         label=f\" \")\n",
    "# ax1.tick_params(axis='y')\n",
    "# plt.legend(loc=\"right\")\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax3 = ax2.twiny()  # instantiate a second axes that shares the same x-axis\n",
    "ax3.set_xlabel(\", $P/P_{0}$\")\n",
    "ax2.set_ylabel(\", $^3$/\")  # we already handled the x-label with ax1\n",
    "ax3.plot(p_exp_list[k], n_s_exp_raw_list[k], color='g', label=f\" \")\n",
    "# plt.legend(loc=\"right\")\n",
    "# \n",
    "# ax2.tick_params(axis='y')\n",
    "# plt.subplots_adjust(left=0.15,\n",
    "#                     bottom=0.133, \n",
    "#                     right=0.979, \n",
    "#                     top=0.917, \n",
    "#                     wspace=0.4, \n",
    "#                     hspace=0.4)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "2d0a60b839334253",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compare isotherms\n",
    "NX, NY = 2, 3\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        net_isotherm = calculate_isotherm_by_distribution(gen, pore_widths, fit_exp_list[k].T[0])\n",
    "        classic_isotherm = calculate_isotherm_by_distribution(gen, pore_widths, fit_classic_list[k].x)\n",
    "        quantachrome_data = np.genfromtxt(f\"data/real/quantachrome/silica/fitting/{exp_file_list[k]}.csv\",\n",
    "                                          delimiter=\",\")[1:].T\n",
    "        quantachrome_pore_size = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "                0] / 10 * 2\n",
    "        quantachrome_dV = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "        quantachrome_data2 = calculate_isotherm_by_distribution(gen, pore_widths,\n",
    "                                                                np.interp(pore_widths, quantachrome_pore_size,\n",
    "                                                                          quantachrome_dV))\n",
    "\n",
    "        y_scale_factor_net = max(net_isotherm) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_classic = max(classic_isotherm) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_quantachrome = max(quantachrome_data[1]) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_quantachrome2 = max(quantachrome_data2) / (\n",
    "            n_s_exp_raw_list[k][-1])  #max(quantachrome_data[1]) / max(n_s_exp_raw_list[k])\n",
    "        print(1 / y_scale_factor_quantachrome2)\n",
    "\n",
    "        axis[i, j].plot(p_exp_list[k], n_s_exp_raw_list[k], label=\"real\")\n",
    "        axis[i, j].plot(gen.pressures_s, classic_isotherm / y_scale_factor_classic, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(gen.pressures_s, net_isotherm / y_scale_factor_net, label=\"net\")\n",
    "        # axis[i, j].plot(quantachrome_data[0], quantachrome_data[1]/y_scale_factor_quantachrome, label=\"quantachrome\")\n",
    "        # axis[i, j].plot(gen.pressures_s, quantachrome_data2/y_scale_factor_quantachrome2, label=\"quantachrome_from_kernal\")\n",
    "        axis[i, j].set_title(f\"{exp_file_list[k]}\")\n",
    "        axis[i, j].legend(loc=\"lower right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "5e20fa504d7c45cc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#QUANTACHROME\n",
    "data = np.genfromtxt(\"data/real/quantachrome/silica/distribution/MIL-101.csv\", delimiter=\",\")[1:].T[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "a2947b6286fe73b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "29ec7e76cb0598ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5d7d844d6e720d14"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40a95cf0ab8cc517"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
