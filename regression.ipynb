{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:47:22.822131Z",
     "start_time": "2024-10-01T05:47:22.806592Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, Sequential, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from generator import Generator\n",
    "from keras import metrics\n",
    "import importlib\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasetLoader import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:38:29.130216Z",
     "start_time": "2024-10-01T05:38:29.116218Z"
    }
   },
   "id": "7eed18058f0ab00e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss', marker=\".\")\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:38:29.587247Z",
     "start_time": "2024-10-01T05:38:29.572249Z"
    }
   },
   "id": "98226eed5b8acbd0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "    data_sorb_tensor = K.constant((data_sorb.T[40:]).T)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    #print(\"data_sorb_tensor.shape = \", data_sorb_tensor.shape, y_pred[:,:128].shape)\n",
    "    isotherm_from_distribution = K.dot(y_pred[:,:128], data_sorb_tensor)\n",
    "    #print(\"isotherm_from_distribution = \", isotherm_from_distribution.shape, y_pred[:,128:].shape)\n",
    "    return K.mean(K.square(y_pred[:,:128] - y_true[:,:128]) + \n",
    "                   0*K.mean(K.square(y_pred[:,128:] - isotherm_from_distribution)))\n",
    "\n",
    "#np.ediff1d(pore_widths, to_begin=pore_widths[0])    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:38:30.957651Z",
     "start_time": "2024-10-01T05:38:30.800093Z"
    }
   },
   "id": "8fa9ad170d8af7ba"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    layer = []\n",
    "    layer.append(Input(shape=290))\n",
    "    layer.append(layers.Dense(290, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(180, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    #layer.append(layers.Concatenate()([layer[-1], layer[0]]))\n",
    "    model = tf.keras.Model(inputs=layer[0], outputs=layer[-1])\n",
    "    #model.compile(loss=custom_loss, optimizer='Adam')\n",
    "    model.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:38:32.410397Z",
     "start_time": "2024-10-01T05:38:32.394395Z"
    }
   },
   "id": "9fc21ec807d452a9"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open(\"data/datasets/reports_no_regularization.npz\", 'rb') as f:\n",
    "    dataset = np.load(f)\n",
    "    isotherm_data = dataset[\"isotherm_data\"]\n",
    "    pore_distribution_data = dataset[\"pore_distribution_data\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:38:34.730964Z",
     "start_time": "2024-10-01T05:38:34.653964Z"
    }
   },
   "id": "9a26370ffb85f924"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "x, y = load_dataset('data/datasets/reports_no_regularization.npz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:38:39.043476Z",
     "start_time": "2024-10-01T05:38:38.995473Z"
    }
   },
   "id": "23e8f27a0688dbc2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:38:45.186710Z",
     "start_time": "2024-10-01T05:38:45.169708Z"
    }
   },
   "id": "a085de6dc72f2ef8"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model = create_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:50:04.711015Z",
     "start_time": "2024-10-01T05:50:04.649015Z"
    }
   },
   "id": "51593764bbe671c3"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 17400.5254\n",
      "Epoch 1: val_loss improved from inf to 13616.78027, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 17237.6113 - val_loss: 13616.7803 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 14230.5576\n",
      "Epoch 2: val_loss did not improve from 13616.78027\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 14028.0996 - val_loss: 14563.5088 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 13570.9004\n",
      "Epoch 3: val_loss improved from 13616.78027 to 12393.62695, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 13570.9004 - val_loss: 12393.6270 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 11962.7656\n",
      "Epoch 4: val_loss improved from 12393.62695 to 11838.58398, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 11765.2549 - val_loss: 11838.5840 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 11326.9834\n",
      "Epoch 5: val_loss improved from 11838.58398 to 11293.05371, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 11176.9248 - val_loss: 11293.0537 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 10478.9688\n",
      "Epoch 6: val_loss improved from 11293.05371 to 10375.84180, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 10796.3174 - val_loss: 10375.8418 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 9721.4697 \n",
      "Epoch 7: val_loss did not improve from 10375.84180\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 10137.5225 - val_loss: 10466.4004 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9748.1162\n",
      "Epoch 8: val_loss did not improve from 10375.84180\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9748.1162 - val_loss: 10381.2646 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 11917.6904\n",
      "Epoch 9: val_loss improved from 10375.84180 to 10024.42285, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 11798.3105 - val_loss: 10024.4229 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 10403.8613\n",
      "Epoch 10: val_loss improved from 10024.42285 to 9848.63574, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 10403.8613 - val_loss: 9848.6357 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 9913.3438\n",
      "Epoch 11: val_loss improved from 9848.63574 to 9683.98828, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 9886.8877 - val_loss: 9683.9883 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 12253.2432\n",
      "Epoch 12: val_loss did not improve from 9683.98828\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 12253.2432 - val_loss: 10526.8643 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 10023.3799\n",
      "Epoch 13: val_loss improved from 9683.98828 to 9354.09570, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 10171.6035 - val_loss: 9354.0957 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 9373.9326\n",
      "Epoch 14: val_loss did not improve from 9354.09570\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9791.3613 - val_loss: 9533.2178 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9735.5791\n",
      "Epoch 15: val_loss did not improve from 9354.09570\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9735.5791 - val_loss: 10771.6475 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 9372.8994\n",
      "Epoch 16: val_loss improved from 9354.09570 to 8895.54395, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 9412.1426 - val_loss: 8895.5439 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 9632.8857 \n",
      "Epoch 17: val_loss did not improve from 8895.54395\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9520.3018 - val_loss: 10435.8467 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9844.5469\n",
      "Epoch 18: val_loss did not improve from 8895.54395\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9844.5469 - val_loss: 9142.5635 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9342.1514 \n",
      "Epoch 19: val_loss did not improve from 8895.54395\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9342.1514 - val_loss: 9054.5029 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9122.7041 \n",
      "Epoch 20: val_loss improved from 8895.54395 to 8586.87598, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 9122.7041 - val_loss: 8586.8760 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 8900.8623\n",
      "Epoch 21: val_loss did not improve from 8586.87598\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 9057.5322 - val_loss: 8719.8896 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 8134.6665\n",
      "Epoch 22: val_loss did not improve from 8586.87598\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8913.3330 - val_loss: 8915.2041 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 9280.0566\n",
      "Epoch 23: val_loss improved from 8586.87598 to 8541.30273, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 9124.1191 - val_loss: 8541.3027 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8852.2930\n",
      "Epoch 24: val_loss did not improve from 8541.30273\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8852.2930 - val_loss: 8590.0996 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 9253.1523\n",
      "Epoch 25: val_loss did not improve from 8541.30273\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9071.3936 - val_loss: 8696.9395 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 8455.5986\n",
      "Epoch 26: val_loss did not improve from 8541.30273\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8738.6055 - val_loss: 8562.7891 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9549.6533\n",
      "Epoch 27: val_loss did not improve from 8541.30273\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9549.6533 - val_loss: 9298.4521 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8568.0000\n",
      "Epoch 28: val_loss did not improve from 8541.30273\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9762.2539 - val_loss: 10706.1064 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 9997.5479\n",
      "Epoch 29: val_loss did not improve from 8541.30273\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9440.3184 - val_loss: 8775.8008 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8554.2930\n",
      "Epoch 30: val_loss improved from 8541.30273 to 8539.83105, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8619.4531 - val_loss: 8539.8311 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9284.6904\n",
      "Epoch 31: val_loss improved from 8539.83105 to 8402.35645, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 9284.6904 - val_loss: 8402.3564 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 8447.5342\n",
      "Epoch 32: val_loss did not improve from 8402.35645\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8790.6279 - val_loss: 9013.0488 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 9805.6816\n",
      "Epoch 33: val_loss did not improve from 8402.35645\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9054.2334 - val_loss: 8428.6787 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 8906.0986\n",
      "Epoch 34: val_loss improved from 8402.35645 to 8249.23926, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8747.3213 - val_loss: 8249.2393 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 8902.1963\n",
      "Epoch 35: val_loss did not improve from 8249.23926\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8577.3711 - val_loss: 8430.4463 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8974.5869 \n",
      "Epoch 36: val_loss did not improve from 8249.23926\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8736.2627 - val_loss: 8398.3418 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8768.5361\n",
      "Epoch 37: val_loss did not improve from 8249.23926\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9976.2910 - val_loss: 10135.5361 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 9097.2432\n",
      "Epoch 38: val_loss did not improve from 8249.23926\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9097.2432 - val_loss: 8487.9150 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 8515.2119 \n",
      "Epoch 39: val_loss improved from 8249.23926 to 8218.99121, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8371.7012 - val_loss: 8218.9912 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 8685.3027\n",
      "Epoch 40: val_loss improved from 8218.99121 to 8032.95898, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8567.4336 - val_loss: 8032.9590 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7964.8008\n",
      "Epoch 41: val_loss did not improve from 8032.95898\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8239.9219 - val_loss: 8207.2227 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 9032.3457\n",
      "Epoch 42: val_loss did not improve from 8032.95898\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8339.9551 - val_loss: 8857.4443 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 9904.2461\n",
      "Epoch 43: val_loss did not improve from 8032.95898\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9179.0557 - val_loss: 9029.4521 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8371.4902\n",
      "Epoch 44: val_loss improved from 8032.95898 to 7957.32471, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8319.4766 - val_loss: 7957.3247 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 8321.3516\n",
      "Epoch 45: val_loss improved from 7957.32471 to 7903.39941, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8243.1201 - val_loss: 7903.3994 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8513.3105\n",
      "Epoch 46: val_loss improved from 7903.39941 to 7902.63867, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8513.3105 - val_loss: 7902.6387 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 8286.0088\n",
      "Epoch 47: val_loss did not improve from 7902.63867\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 7999.4561 - val_loss: 7906.3145 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 8440.8818\n",
      "Epoch 48: val_loss did not improve from 7902.63867\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7921.2139 - val_loss: 8760.3594 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 8772.5029\n",
      "Epoch 49: val_loss did not improve from 7902.63867\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 8341.2822 - val_loss: 7905.1348 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8071.6304\n",
      "Epoch 50: val_loss improved from 7902.63867 to 7846.74951, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 8071.6304 - val_loss: 7846.7495 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 7963.8271\n",
      "Epoch 51: val_loss did not improve from 7846.74951\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 7921.0518 - val_loss: 7862.4082 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7878.5767\n",
      "Epoch 52: val_loss did not improve from 7846.74951\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7826.5625 - val_loss: 7870.2109 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 7862.8970\n",
      "Epoch 53: val_loss improved from 7846.74951 to 7696.61914, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 8031.4932 - val_loss: 7696.6191 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8214.0459\n",
      "Epoch 54: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8120.6416 - val_loss: 11358.3252 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 9748.7588\n",
      "Epoch 55: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 9279.5342 - val_loss: 8249.3555 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 8400.7070 \n",
      "Epoch 56: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8497.7607 - val_loss: 8087.5801 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8344.0830\n",
      "Epoch 57: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8344.0830 - val_loss: 8309.8711 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7450.1875\n",
      "Epoch 58: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 8136.8247 - val_loss: 7889.1123 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 8311.5605\n",
      "Epoch 59: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8219.1592 - val_loss: 7778.8169 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 8432.0166\n",
      "Epoch 60: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8164.6870 - val_loss: 8014.1685 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 8345.7607\n",
      "Epoch 61: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8395.0547 - val_loss: 12253.2930 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 9792.2490 \n",
      "Epoch 62: val_loss did not improve from 7696.61914\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9647.4766 - val_loss: 8080.0962 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7456.6602\n",
      "Epoch 63: val_loss improved from 7696.61914 to 7603.47852, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7824.7993 - val_loss: 7603.4785 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 7535.9146\n",
      "Epoch 64: val_loss did not improve from 7603.47852\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8197.6553 - val_loss: 8412.5439 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 7424.1445\n",
      "Epoch 65: val_loss did not improve from 7603.47852\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 7877.6772 - val_loss: 7966.8989 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 7987.0840\n",
      "Epoch 66: val_loss improved from 7603.47852 to 7590.22266, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 7958.1440 - val_loss: 7590.2227 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 8310.1758\n",
      "Epoch 67: val_loss did not improve from 7590.22266\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8572.6582 - val_loss: 8337.9121 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 8284.6191\n",
      "Epoch 68: val_loss did not improve from 7590.22266\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 8193.3838 - val_loss: 7660.4858 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 7995.9790\n",
      "Epoch 69: val_loss did not improve from 7590.22266\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7777.8037 - val_loss: 7598.2754 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7908.3218\n",
      "Epoch 70: val_loss did not improve from 7590.22266\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7908.3218 - val_loss: 8838.3271 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7612.9990\n",
      "Epoch 71: val_loss did not improve from 7590.22266\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7920.6450 - val_loss: 7817.6250 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8096.9429\n",
      "Epoch 72: val_loss did not improve from 7590.22266\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8096.9429 - val_loss: 9773.0830 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 8646.3301\n",
      "Epoch 73: val_loss did not improve from 7590.22266\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8295.9482 - val_loss: 7971.5933 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7945.4634\n",
      "Epoch 74: val_loss improved from 7590.22266 to 7516.08740, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7730.1133 - val_loss: 7516.0874 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8108.4487\n",
      "Epoch 75: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8108.4487 - val_loss: 8312.5859 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7443.9648\n",
      "Epoch 76: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7995.2241 - val_loss: 7877.0249 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7470.4688\n",
      "Epoch 77: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7702.3711 - val_loss: 7945.6284 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8235.6885\n",
      "Epoch 78: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7710.0410 - val_loss: 7727.1499 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7653.5791\n",
      "Epoch 79: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7705.5454 - val_loss: 7632.2891 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 6724.3311\n",
      "Epoch 80: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7620.6143 - val_loss: 7591.4580 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7670.4771\n",
      "Epoch 81: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7535.2271 - val_loss: 7516.8203 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7898.2007\n",
      "Epoch 82: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7898.2007 - val_loss: 8104.0576 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8428.6016\n",
      "Epoch 83: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7721.5908 - val_loss: 9370.3438 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 9504.1533\n",
      "Epoch 84: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 9268.2207 - val_loss: 8592.8271 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7999.1245\n",
      "Epoch 85: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7999.1245 - val_loss: 7640.0625 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 8389.4717\n",
      "Epoch 86: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8772.1465 - val_loss: 8301.7646 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7402.0845\n",
      "Epoch 87: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8009.2510 - val_loss: 7661.9810 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7905.5381\n",
      "Epoch 88: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7905.5381 - val_loss: 7721.5293 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 9124.6953\n",
      "Epoch 89: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7892.9199 - val_loss: 7816.0166 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 8238.3896 \n",
      "Epoch 90: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7862.0532 - val_loss: 7797.0850 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7787.7891\n",
      "Epoch 91: val_loss did not improve from 7516.08740\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7787.7891 - val_loss: 7757.0347 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7781.4302\n",
      "Epoch 92: val_loss improved from 7516.08740 to 7513.11719, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7586.7534 - val_loss: 7513.1172 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 6976.6328\n",
      "Epoch 93: val_loss did not improve from 7513.11719\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7748.6099 - val_loss: 8686.0410 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7409.1182\n",
      "Epoch 94: val_loss did not improve from 7513.11719\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8099.3975 - val_loss: 8064.0386 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7718.9468\n",
      "Epoch 95: val_loss did not improve from 7513.11719\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7668.9521 - val_loss: 7966.4111 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7461.0137\n",
      "Epoch 96: val_loss did not improve from 7513.11719\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7906.6294 - val_loss: 7565.7622 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 6842.8706\n",
      "Epoch 97: val_loss improved from 7513.11719 to 7439.61816, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7607.4204 - val_loss: 7439.6182 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 7928.3730\n",
      "Epoch 98: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7700.1216 - val_loss: 8031.7686 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8145.1729\n",
      "Epoch 99: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8189.6987 - val_loss: 7893.6489 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7797.3999\n",
      "Epoch 100: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8278.8135 - val_loss: 8021.6162 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 8824.3955\n",
      "Epoch 101: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8636.9473 - val_loss: 8610.4111 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8308.2246\n",
      "Epoch 102: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8308.2246 - val_loss: 7707.4355 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 8077.7603\n",
      "Epoch 103: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8046.3696 - val_loss: 7697.0850 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7438.5430\n",
      "Epoch 104: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7712.9224 - val_loss: 7566.0962 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7882.6890\n",
      "Epoch 105: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7750.8970 - val_loss: 8733.2676 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 7844.8716\n",
      "Epoch 106: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8108.2549 - val_loss: 7602.4902 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 9143.5371\n",
      "Epoch 107: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8296.0723 - val_loss: 8914.9854 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8300.5381\n",
      "Epoch 108: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7995.9072 - val_loss: 7602.8975 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7414.7529\n",
      "Epoch 109: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7647.3921 - val_loss: 7947.6567 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7959.2017\n",
      "Epoch 110: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7570.5479 - val_loss: 8328.2334 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8040.3726\n",
      "Epoch 111: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7664.9863 - val_loss: 7478.3975 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7648.0186\n",
      "Epoch 112: val_loss did not improve from 7439.61816\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7648.0186 - val_loss: 7748.5513 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7781.7075\n",
      "Epoch 113: val_loss improved from 7439.61816 to 7369.28320, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7572.3330 - val_loss: 7369.2832 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7641.2153\n",
      "Epoch 114: val_loss did not improve from 7369.28320\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7553.4819 - val_loss: 7505.1611 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 7576.0601\n",
      "Epoch 115: val_loss did not improve from 7369.28320\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7702.9873 - val_loss: 7598.9863 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7656.2866\n",
      "Epoch 116: val_loss did not improve from 7369.28320\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7909.8208 - val_loss: 7593.7598 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 8542.3965\n",
      "Epoch 117: val_loss did not improve from 7369.28320\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8432.2793 - val_loss: 7982.1592 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 8438.0391\n",
      "Epoch 118: val_loss did not improve from 7369.28320\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7699.6245 - val_loss: 7482.5063 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 7266.3345\n",
      "Epoch 119: val_loss improved from 7369.28320 to 7341.48291, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7422.1357 - val_loss: 7341.4829 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7425.8970\n",
      "Epoch 120: val_loss did not improve from 7341.48291\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7508.1821 - val_loss: 7629.6113 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7288.8564\n",
      "Epoch 121: val_loss did not improve from 7341.48291\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7595.0239 - val_loss: 7662.0840 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7767.6904\n",
      "Epoch 122: val_loss did not improve from 7341.48291\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7715.9414 - val_loss: 7354.0044 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 8699.4707\n",
      "Epoch 123: val_loss did not improve from 7341.48291\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8300.7686 - val_loss: 8425.3516 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8472.6768\n",
      "Epoch 124: val_loss did not improve from 7341.48291\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8472.6768 - val_loss: 10413.9668 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7719.8369\n",
      "Epoch 125: val_loss did not improve from 7341.48291\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8237.9512 - val_loss: 8113.5718 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 7348.3525\n",
      "Epoch 126: val_loss did not improve from 7341.48291\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7768.5879 - val_loss: 7432.4316 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7126.4707\n",
      "Epoch 127: val_loss improved from 7341.48291 to 7336.19873, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7447.0063 - val_loss: 7336.1987 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7915.2256\n",
      "Epoch 128: val_loss did not improve from 7336.19873\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7426.0674 - val_loss: 7399.5522 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7499.6641\n",
      "Epoch 129: val_loss did not improve from 7336.19873\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7439.7910 - val_loss: 8789.9043 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8077.2568\n",
      "Epoch 130: val_loss did not improve from 7336.19873\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7782.8369 - val_loss: 7428.2441 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 7019.0542\n",
      "Epoch 131: val_loss improved from 7336.19873 to 7137.91406, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7324.7720 - val_loss: 7137.9141 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7110.1353\n",
      "Epoch 132: val_loss did not improve from 7137.91406\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7230.4580 - val_loss: 7252.0986 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 6709.8726\n",
      "Epoch 133: val_loss did not improve from 7137.91406\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7330.6606 - val_loss: 8134.9692 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7294.2642\n",
      "Epoch 134: val_loss did not improve from 7137.91406\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7600.1646 - val_loss: 7247.3472 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7984.3682\n",
      "Epoch 135: val_loss did not improve from 7137.91406\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7984.3682 - val_loss: 8274.3662 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 6836.0171\n",
      "Epoch 136: val_loss did not improve from 7137.91406\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7453.6304 - val_loss: 7241.2017 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7265.5156\n",
      "Epoch 137: val_loss improved from 7137.91406 to 7046.35596, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7265.5156 - val_loss: 7046.3560 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 6922.3232\n",
      "Epoch 138: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7112.7241 - val_loss: 7074.4072 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 6632.5737\n",
      "Epoch 139: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7262.1670 - val_loss: 7094.2339 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7165.9409\n",
      "Epoch 140: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7165.9409 - val_loss: 7160.1069 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7282.9243\n",
      "Epoch 141: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7219.4072 - val_loss: 7177.4526 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8163.7988\n",
      "Epoch 142: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7569.8525 - val_loss: 8288.8818 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7901.1719\n",
      "Epoch 143: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7901.1719 - val_loss: 7514.8662 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7614.9668\n",
      "Epoch 144: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7408.0933 - val_loss: 7811.6851 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 6831.8076\n",
      "Epoch 145: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7311.8657 - val_loss: 7081.2471 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 7573.3486\n",
      "Epoch 146: val_loss did not improve from 7046.35596\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7779.6714 - val_loss: 7323.6094 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 7267.0928\n",
      "Epoch 147: val_loss improved from 7046.35596 to 7024.82959, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7198.0107 - val_loss: 7024.8296 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7099.5737\n",
      "Epoch 148: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7099.5737 - val_loss: 7086.6475 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 6330.3262\n",
      "Epoch 149: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7494.9219 - val_loss: 7157.3716 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8268.7910\n",
      "Epoch 150: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8268.7910 - val_loss: 7409.1104 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7185.6914\n",
      "Epoch 151: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7682.1494 - val_loss: 7214.8843 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 6691.2290\n",
      "Epoch 152: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7165.3672 - val_loss: 7395.3916 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7204.5996\n",
      "Epoch 153: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7445.4854 - val_loss: 9212.7314 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8835.7021\n",
      "Epoch 154: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7884.7168 - val_loss: 7325.9126 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7169.9966\n",
      "Epoch 155: val_loss did not improve from 7024.82959\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7268.6704 - val_loss: 7143.6494 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7031.7324\n",
      "Epoch 156: val_loss improved from 7024.82959 to 6981.39062, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7099.6475 - val_loss: 6981.3906 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8239.3770\n",
      "Epoch 157: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7568.8682 - val_loss: 7112.0596 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 7372.3223\n",
      "Epoch 158: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7232.9126 - val_loss: 7166.0684 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7266.7432\n",
      "Epoch 159: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7146.8540 - val_loss: 7028.7007 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 6535.6143\n",
      "Epoch 160: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 7146.0034 - val_loss: 7762.2056 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 7603.2227\n",
      "Epoch 161: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7897.5347 - val_loss: 7605.3281 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7383.6143\n",
      "Epoch 162: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7379.2310 - val_loss: 7379.4478 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 6296.2539\n",
      "Epoch 163: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7253.3682 - val_loss: 7522.2002 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7301.4331\n",
      "Epoch 164: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7301.4331 - val_loss: 7163.4336 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7574.5469\n",
      "Epoch 165: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7327.5854 - val_loss: 7541.7847 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 8963.9512\n",
      "Epoch 166: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8543.8516 - val_loss: 7820.2305 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7988.4541\n",
      "Epoch 167: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7988.4541 - val_loss: 7348.2363 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 7008.8965\n",
      "Epoch 168: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7468.7339 - val_loss: 7194.7271 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 7441.7959\n",
      "Epoch 169: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7380.5591 - val_loss: 7310.8652 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 7251.2871\n",
      "Epoch 170: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 7447.3623 - val_loss: 7406.6084 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 7082.5000\n",
      "Epoch 171: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7154.5186 - val_loss: 6998.2021 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 7182.8057\n",
      "Epoch 172: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7041.0537 - val_loss: 6989.2134 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 6814.9780\n",
      "Epoch 173: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 7073.9678 - val_loss: 7005.8613 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7060.3340\n",
      "Epoch 174: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 6998.7524 - val_loss: 7135.7715 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "27/38 [====================>.........] - ETA: 0s - loss: 7769.6509\n",
      "Epoch 175: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8117.3638 - val_loss: 7488.4834 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 8659.8115\n",
      "Epoch 176: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 8575.9004 - val_loss: 7599.8638 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 7829.3809\n",
      "Epoch 177: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7506.7607 - val_loss: 7261.3672 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8137.3452\n",
      "Epoch 178: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7151.2344 - val_loss: 7399.3726 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 7137.0967\n",
      "Epoch 179: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7499.4336 - val_loss: 7132.4556 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 7214.5757\n",
      "Epoch 180: val_loss did not improve from 6981.39062\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7214.5757 - val_loss: 7175.8672 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7130.5518\n",
      "Epoch 181: val_loss improved from 6981.39062 to 6947.89160, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 7059.5933 - val_loss: 6947.8916 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 7168.6650\n",
      "Epoch 182: val_loss improved from 6947.89160 to 6910.00244, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 6958.4355 - val_loss: 6910.0024 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8380.4854\n",
      "Epoch 183: val_loss did not improve from 6910.00244\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8380.4854 - val_loss: 9837.0693 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 8055.8145\n",
      "Epoch 184: val_loss did not improve from 6910.00244\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8055.8145 - val_loss: 7254.7290 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 7867.9751\n",
      "Epoch 185: val_loss did not improve from 6910.00244\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7563.6172 - val_loss: 7825.1558 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 7452.3062\n",
      "Epoch 186: val_loss did not improve from 6910.00244\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7689.1812 - val_loss: 7314.8052 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7191.4326\n",
      "Epoch 187: val_loss did not improve from 6910.00244\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7282.2549 - val_loss: 6948.4297 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 7093.4043\n",
      "Epoch 188: val_loss improved from 6910.00244 to 6872.20166, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 7002.5420 - val_loss: 6872.2017 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 7703.6284\n",
      "Epoch 189: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7528.7134 - val_loss: 7361.3262 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 7508.2100\n",
      "Epoch 190: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7544.9673 - val_loss: 6930.1704 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7432.2202\n",
      "Epoch 191: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7399.4956 - val_loss: 7691.1646 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 8110.7124\n",
      "Epoch 192: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7298.4819 - val_loss: 6924.5044 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 7115.9365\n",
      "Epoch 193: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7241.4312 - val_loss: 7152.0200 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 8089.1230\n",
      "Epoch 194: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 7610.4580 - val_loss: 7014.5327 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 6978.3149\n",
      "Epoch 195: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 6978.3149 - val_loss: 6962.1079 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7609.4976\n",
      "Epoch 196: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7372.4917 - val_loss: 7106.1509 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - ETA: 0s - loss: 6964.3789\n",
      "Epoch 197: val_loss did not improve from 6872.20166\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 6964.3789 - val_loss: 6939.0498 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "37/38 [============================>.] - ETA: 0s - loss: 6767.0718\n",
      "Epoch 198: val_loss improved from 6872.20166 to 6794.33936, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 6853.2979 - val_loss: 6794.3394 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 6941.5752\n",
      "Epoch 199: val_loss improved from 6794.33936 to 6758.31152, saving model to data/models\\reports.keras\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 6852.7393 - val_loss: 6758.3115 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 7307.9478\n",
      "Epoch 200: val_loss did not improve from 6758.31152\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 7050.1196 - val_loss: 6916.6099 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.set_value(model.optimizer.learning_rate, 0.0001)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(filepath='data/models/reports.keras', save_best_only=True,\n",
    "                                              monitor='val_loss', mode='min', verbose=1, save_weights_only=False,\n",
    "                                              save_freq='epoch')\n",
    "\n",
    "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                                      patience=10000, verbose=1, mode='auto')\n",
    "history = model.fit(np.array(x_train), np.array(y_train),\n",
    "                    epochs=200, batch_size=20, shuffle=True,\n",
    "                    validation_data=(np.array(x_train), np.array(y_train)), callbacks=[mcp_save, reduce_lr_loss])\n",
    "plot_loss(history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:51:54.108965Z",
     "start_time": "2024-10-01T05:50:05.185176Z"
    }
   },
   "id": "646d0a18ee5d9e94"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('data/models/reports.keras', custom_objects={'abs': tf.math.abs, 'custom_loss': custom_loss})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-01T05:51:57.348863Z",
     "start_time": "2024-10-01T05:51:57.226865Z"
    }
   },
   "id": "ec1419b3bc971e4c"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('data/models/reports.keras', custom_objects={'abs': tf.math.abs, 'custom_loss': custom_loss})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:51:58.047395Z",
     "start_time": "2024-10-01T05:51:57.919397Z"
    }
   },
   "id": "ac8cae34578158f6"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step - loss: 6638.0894\n"
     ]
    }
   ],
   "source": [
    "prediction = model.evaluate(np.array(x_test), np.array(y_test))\n",
    "#prediction = model.predict(np.array(x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:56:12.332796Z",
     "start_time": "2024-10-01T05:56:12.147795Z"
    }
   },
   "id": "3936d7f526408c2e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "6638.08935546875"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:56:16.502432Z",
     "start_time": "2024-10-01T05:56:16.487428Z"
    }
   },
   "id": "cb8dac3c04e46035",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# test on x_train\n",
    "def fetch_prediction(prediction):\n",
    "    return prediction[:128]\n",
    "\n",
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        k = np.random.randint(0, len(x_test))\n",
    "        x_scale_factor = max(pore_widths) / len(x_test[k])\n",
    "        axis[i, j].plot(pore_widths / x_scale_factor, fetch_prediction(prediction[k]), marker=\".\", label=f\"Prediction\")\n",
    "        axis[i, j].plot(pore_widths / x_scale_factor, y_test[k], marker=\".\", label=\"Real distribution\")\n",
    "        axis[i, j].plot(pressures[77:367]*500, x_test[k], label=\"Isotherm\")\n",
    "        kernal = (data_sorb.T[40:])\n",
    "        axis[i, j].plot(pressures[40:]*500, np.dot(kernal, prediction[k][:128]), label=\"New Isotherm\")\n",
    "        axis[i, j].set_title(f\" {k}\")\n",
    "        axis[i, j].title.set_size(10)\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plt.legend()\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:54:09.483895Z",
     "start_time": "2024-10-01T05:52:57.443162Z"
    }
   },
   "id": "c0f7f9f4bae77a4c"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# test with test Generator\n",
    "from tools import TestApp\n",
    "\n",
    "gen = Generator(path_s=\"data/initial kernels/Kernel_Silica_Adsorption.npy\",\n",
    "                path_d=\"data/initial kernels/Kernel_Silica_Desorption.npy\",\n",
    "                path_p_d=\"data/initial kernels/Pressure_Silica.npy\",\n",
    "                path_p_s=\"data/initial kernels/Pressure_Silica.npy\",\n",
    "                path_a=\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\"\n",
    "                )\n",
    "# gen = Generator(path_s=\"data/initial kernels/Kernel_Carbon_Adsorption.npy\",\n",
    "#                               path_d=\"data/initial kernels/Kernel_Carbon_Desorption.npy\",\n",
    "#                               path_p_d=\"data/initial kernels/Pressure_Carbon.npy\",\n",
    "#                               path_p_s=\"data/initial kernels/Pressure_Carbon.npy\",\n",
    "#                               path_a=\"data/initial kernels/Size_Kernel_Carbon_Adsorption.npy\"\n",
    "#                             )\n",
    "\n",
    "#TestApp.App(model, gen)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T07:15:19.654199Z",
     "start_time": "2024-09-24T07:15:19.639200Z"
    }
   },
   "id": "efcd0aa0fe36bac8"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "exp_file_list = [\"MCM-41\", \"SBA-15\", \"SBA-16\", \"MIL-101\", \"MIL-101_2\", \"DUT-49\", \"FDM-4\", \"PCN-333\", \"PCN-777\",\n",
    "                 \"MIL-100\"]\n",
    "\n",
    "p_exp_list = []\n",
    "n_s_exp_raw_list = []\n",
    "for exp_file_name in exp_file_list:\n",
    "    data = pd.read_csv(f\"data/real/{exp_file_name}.txt\", header=None)\n",
    "    # p_exp_list.append(data.iloc[:,1].to_numpy())\n",
    "    # n_s_exp_raw_list.append(data.iloc[:,3].to_numpy())\n",
    "    p_exp_list.append(data.iloc[:, 1].to_numpy())\n",
    "    n_s_exp_raw_list.append(data.iloc[:, 3].to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T07:15:20.251196Z",
     "start_time": "2024-09-24T07:15:20.221217Z"
    }
   },
   "id": "1b77e8c380dc9e48"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "j = 2\n",
    "plt.plot(p_exp_list[j], n_s_exp_raw_list[j], marker=\".\", label=exp_file_list[j])\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T07:15:22.021884Z",
     "start_time": "2024-09-24T07:15:20.850541Z"
    }
   },
   "id": "a7600267dd8ee248"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "#      \n",
    "n_s_exp_list = []\n",
    "for i in range(len(p_exp_list)):\n",
    "    n_s_exp_list.append(np.interp(gen.pressures_s[77:367], p_exp_list[i], n_s_exp_raw_list[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T07:15:22.614301Z",
     "start_time": "2024-09-24T07:15:22.598276Z"
    }
   },
   "id": "17fa210fb149ba49"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "j = 2\n",
    "plt.plot(gen.pressures_s[77:367], n_s_exp_list[j], marker=\".\", label=exp_file_list[j])\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T07:15:24.279559Z",
     "start_time": "2024-09-24T07:15:23.077276Z"
    }
   },
   "id": "7648e2fb6c309ea0"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "n_s_exp_for_net_list = [pre_process_isotherm(np.copy(n_s_exp), scale=False) for n_s_exp in n_s_exp_list]\n",
    "fit_exp_list = [model.predict(np.array([n_s_exp_for_net])).T for n_s_exp_for_net in n_s_exp_for_net_list]\n",
    "fit_exp_list = [fetch_prediction(i) for i in fit_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T07:15:25.467351Z",
     "start_time": "2024-09-24T07:15:24.887042Z"
    }
   },
   "id": "e5eb0f653a71de51"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_exp_list[k]) / max(n_s_exp_raw_list[k])\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k], marker=\".\", label=f\"Distribution\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k],\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        \n",
    "        kernal = (data_sorb.T[40:])\n",
    "        axis[i, j].plot(gen.pressures_s[40:458]* x_scale_factor, np.dot(kernal, fit_exp_list[k]), label=\"Isotherm by distribution\")\n",
    "        \n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_exp_list[k])], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T07:16:21.252075Z",
     "start_time": "2024-09-24T07:15:26.185352Z"
    }
   },
   "id": "cae6c851c04bbfaf"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n",
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n"
     ]
    }
   ],
   "source": [
    "### Classic\n",
    "import inverse\n",
    "\n",
    "kernel = np.load(\"data/initial kernels/Kernel_Silica_Adsorption.npy\")\n",
    "\n",
    "### normalize on size\n",
    "# a_array = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "# for i in range(len(a_array)):\n",
    "#     kernel[i] /= a_array[i]\n",
    "#     kernel[i] /= a_array[i]\n",
    "###\n",
    "\n",
    "cut_kernel = []\n",
    "for i in range(len(kernel)):\n",
    "    cut_kernel.append(kernel[i][40:458])\n",
    "cut_kernel = np.array(cut_kernel)\n",
    "fit_classic_list = [inverse.fit_SLSQP(adsorption=n_s, kernel=cut_kernel, a_array=pore_widths) for n_s in n_s_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T13:40:22.307670Z",
     "start_time": "2024-06-18T13:39:59.907685Z"
    }
   },
   "id": "9949a77c1d2e89c0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n",
      "C:\\Users\\ivano\\PycharmProjects\\isotherm\\inverse.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  adsorption)).sum(axis=0) + alpha * np.sum(pore_dist * np.log(pore_dist)) #/ len(pore_dist) #+ beta * np.sum(\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "importlib.reload(inverse)\n",
    "\n",
    "\n",
    "def create_regularization_animation(file):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5, True)\n",
    "    fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=0)\n",
    "    line1, = ax.plot(pore_widths[:-30], fit_classic.x[:-30], marker=\".\", label=f\"a = {0}\")\n",
    "\n",
    "    y_scale_factor = max(fit_classic.x) / max(fit_exp_list[2])\n",
    "    #plt.plot(pore_widths, fit_exp_list[2] * y_scale_factor, marker=\".\", label=f\" \")\n",
    "\n",
    "    ax.set_ylabel(\" , $^3$/  * \")\n",
    "    ax.set_xlabel(\" , \")\n",
    "\n",
    "    L = plt.legend(loc=1)  # Define legend objects\n",
    "\n",
    "    def update(frame):\n",
    "        a = frame / 4 + 2\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        line1.set_ydata(fit_classic.x[:-30])\n",
    "        L.get_texts()[0].set_text(\n",
    "            f\"   ,    = {round(a-2,1)}\")  # Update label each at frame\n",
    "        return line1,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig=fig, func=update, frames=100, interval=100)\n",
    "    writervideo = animation.FFMpegWriter(fps=30)\n",
    "    ani.save(file, writer=writervideo, dpi=200)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "create_regularization_animation(\"SBA-16_regularization.mp4\")\n",
    "def plot_regularization_graphs():\n",
    "    for a in [1, 5, 10, 50]:\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        plt.plot(pore_widths, fit_classic.x, marker=\".\", label=f\" = {a}\")\n",
    "    plt.ylabel(\" , $^3$/  * \")\n",
    "    plt.xlabel(\" , \")\n",
    "    plot()\n",
    "#plot_regularization_graphs()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:50:01.331621Z",
     "start_time": "2024-06-18T14:46:23.487577Z"
    }
   },
   "id": "ca3051b964f378a",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_classic_list[k].x) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_net = max(fit_classic_list[k].x) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k] * y_scale_factor,\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_classic_list[k].x)], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "707d030f7db4a23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare net, classic, quantachrome distributions\n",
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "\n",
    "\n",
    "def calculate_isotherm_by_distribution(generator: Generator, a_array, distribution):\n",
    "    generator.a_array = a_array\n",
    "    generator.pore_distribution = distribution\n",
    "    generator.calculate_isotherms()\n",
    "    return generator.n_s\n",
    "\n",
    "\n",
    "#/np.ediff1d(pore_widths, to_begin=pore_widths[0])\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        quantachrome_pore_size = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "                0] / 10 * 2  # /10 -    * 2 -  QH  - Half pore width.\n",
    "        quantachrome_dV = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "        # isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "        # y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "        # quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "        y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "        y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x * y_scale_factor_classic, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(quantachrome_pore_size, quantachrome_dV, marker=\".\", label=f\"quantachrome\")\n",
    "        axis[i, j].set_title(f\"{exp_file_list[k]}\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e0b9fbf987405b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "plt = reload(plt)\n",
    "\n",
    "k = 0\n",
    "\n",
    "quantachrome_pore_size = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "        0] / 10 * 2  # /10 -    * 2 -  QH  - Half pore width.\n",
    "quantachrome_dV = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "# isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "# y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "# quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "plt.plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\" \")\n",
    "plt.plot(pore_widths, fit_classic_list[k].x * y_scale_factor_classic, marker=\".\", label=f\" \")\n",
    "#plt.plot(quantachrome_pore_size, quantachrome_dV, marker=\".\", label=f\"  QH\") \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.title(f\"{exp_file_list[k]}\")\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.subplots_adjust(left=0.15,\n",
    "                    bottom=0.133,\n",
    "                    right=0.979,\n",
    "                    top=0.917,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylabel(\" , $^3$/  * \")\n",
    "plt.xlabel(\" , \")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5e26ea811ae701c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation 2\n",
    "k = 3\n",
    "plt.plot(p_exp_list[k], n_s_exp_raw_list[k], marker=\".\", color='b', label=f\"{exp_file_list[k]}\")\n",
    "k = 9\n",
    "plt.plot(p_exp_list[k], n_s_exp_raw_list[k], marker=\".\", color='r', label=f\"{exp_file_list[k]}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.title(f\" \")\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.subplots_adjust(left=0.15,\n",
    "                    bottom=0.133,\n",
    "                    right=0.979,\n",
    "                    top=0.917,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylabel(\", $^3$/\")\n",
    "plt.xlabel(\", $P/P_{0}$\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be93198745d6bc2d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation 3\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6, 7))\n",
    "ax1.set_xlabel(\" , \")\n",
    "ax1.set_ylabel(\" , $^3$/  * \")\n",
    "k = 0\n",
    "plt.title(f\"{exp_file_list[k]}\")\n",
    "quantachrome_pore_size = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "        0] / 10 * 2  # /10 -    * 2 -  QH  - Half pore width.\n",
    "quantachrome_dV = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "# isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "# y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "# quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "ax1.plot(pore_widths[0:100], (fit_exp_list[k] * y_scale_factor_net)[0:100], marker=\".\", label=f\" \")\n",
    "ax1.plot(pore_widths[0:100], (fit_classic_list[k].x * y_scale_factor_classic)[0:100], marker=\".\",\n",
    "         label=f\" \")\n",
    "# ax1.tick_params(axis='y')\n",
    "# plt.legend(loc=\"right\")\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax3 = ax2.twiny()  # instantiate a second axes that shares the same x-axis\n",
    "ax3.set_xlabel(\", $P/P_{0}$\")\n",
    "ax2.set_ylabel(\", $^3$/\")  # we already handled the x-label with ax1\n",
    "ax3.plot(p_exp_list[k], n_s_exp_raw_list[k], color='g', label=f\" \")\n",
    "# plt.legend(loc=\"right\")\n",
    "# \n",
    "# ax2.tick_params(axis='y')\n",
    "# plt.subplots_adjust(left=0.15,\n",
    "#                     bottom=0.133, \n",
    "#                     right=0.979, \n",
    "#                     top=0.917, \n",
    "#                     wspace=0.4, \n",
    "#                     hspace=0.4)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d0a60b839334253",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compare isotherms\n",
    "NX, NY = 2, 3\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        net_isotherm = calculate_isotherm_by_distribution(gen, pore_widths, fit_exp_list[k].T[0])\n",
    "        classic_isotherm = calculate_isotherm_by_distribution(gen, pore_widths, fit_classic_list[k].x)\n",
    "        quantachrome_data = np.genfromtxt(f\"data/real/quantachrome/silica/fitting/{exp_file_list[k]}.csv\",\n",
    "                                          delimiter=\",\")[1:].T\n",
    "        quantachrome_pore_size = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "                0] / 10 * 2\n",
    "        quantachrome_dV = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "        quantachrome_data2 = calculate_isotherm_by_distribution(gen, pore_widths,\n",
    "                                                                np.interp(pore_widths, quantachrome_pore_size,\n",
    "                                                                          quantachrome_dV))\n",
    "\n",
    "        y_scale_factor_net = max(net_isotherm) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_classic = max(classic_isotherm) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_quantachrome = max(quantachrome_data[1]) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_quantachrome2 = max(quantachrome_data2) / (\n",
    "            n_s_exp_raw_list[k][-1])  #max(quantachrome_data[1]) / max(n_s_exp_raw_list[k])\n",
    "        print(1 / y_scale_factor_quantachrome2)\n",
    "\n",
    "        axis[i, j].plot(p_exp_list[k], n_s_exp_raw_list[k], label=\"real\")\n",
    "        axis[i, j].plot(gen.pressures_s, classic_isotherm / y_scale_factor_classic, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(gen.pressures_s, net_isotherm / y_scale_factor_net, label=\"net\")\n",
    "        # axis[i, j].plot(quantachrome_data[0], quantachrome_data[1]/y_scale_factor_quantachrome, label=\"quantachrome\")\n",
    "        # axis[i, j].plot(gen.pressures_s, quantachrome_data2/y_scale_factor_quantachrome2, label=\"quantachrome_from_kernal\")\n",
    "        axis[i, j].set_title(f\"{exp_file_list[k]}\")\n",
    "        axis[i, j].legend(loc=\"lower right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e20fa504d7c45cc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#QUANTACHROME\n",
    "data = np.genfromtxt(\"data/real/quantachrome/silica/distribution/MIL-101.csv\", delimiter=\",\")[1:].T[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2947b6286fe73b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29ec7e76cb0598ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
