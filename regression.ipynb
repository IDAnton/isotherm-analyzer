{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, Sequential, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from generator import Generator\n",
    "from keras import metrics\n",
    "import importlib\n",
    "import keras.backend as K"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T04:18:23.550248Z",
     "start_time": "2024-10-12T04:18:16.004499Z"
    }
   },
   "id": "f70dabf5c8e7e463"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from datasetLoader import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T04:18:23.566249Z",
     "start_time": "2024-10-12T04:18:23.554251Z"
    }
   },
   "id": "8d981596359c20a5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss', marker=\".\")\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T04:18:23.581250Z",
     "start_time": "2024-10-12T04:18:23.568250Z"
    }
   },
   "id": "d8275c36c94a1d86"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open(\"data/initial kernels/Kernel_Silica_Adsorption.npy\", 'rb') as f:\n",
    "    data_sorb = np.load(f)\n",
    "    #data_sorb_tensor = K.constant((data_sorb.T[40:]).T)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    pass\n",
    "    # #print(\"data_sorb_tensor.shape = \", data_sorb_tensor.shape, y_pred[:,:128].shape)\n",
    "    # isotherm_from_distribution = K.dot(y_pred[:,:128], data_sorb_tensor)\n",
    "    # #print(\"isotherm_from_distribution = \", isotherm_from_distribution.shape, y_pred[:,128:].shape)\n",
    "    # return K.mean(K.square(y_pred[:,:128] - y_true[:,:128]) +\n",
    "    #                0.2*K.mean(K.square(y_pred[:,128:] - isotherm_from_distribution)))\n",
    "\n",
    "# np.ediff1d(pore_widths, to_begin=pore_widths[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:52.488451Z",
     "start_time": "2024-10-12T05:43:52.472456Z"
    }
   },
   "id": "619b039f2d0a573a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_model(x):\n",
    "    layer = []\n",
    "    layer.append(Input(shape=x[0].shape))\n",
    "    layer.append(layers.Dense(458, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    layer.append(layers.Dense(128, activation='relu')(layer[-1]))\n",
    "    # layer.append(layers.Concatenate()([layer[-1], layer[0]]))\n",
    "    model = tf.keras.Model(inputs=layer[0], outputs=layer[-1])\n",
    "    # model.compile(loss=custom_loss, optimizer='Adam')\n",
    "    model.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:52.976451Z",
     "start_time": "2024-10-12T05:43:52.957449Z"
    }
   },
   "id": "8016a917c590c98a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# with open(\"data/datasets/reports_best_tyhanov.npz\", 'rb') as f:\n",
    "#     dataset = np.load(f)\n",
    "#     isotherm_data = dataset[\"isotherm_data\"]\n",
    "#     pore_distribution_data = dataset[\"pore_distribution_data\"]\n",
    "#     pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:53.652853Z",
     "start_time": "2024-10-12T05:43:53.622851Z"
    }
   },
   "id": "58dd8a7b2ff1a173"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "x, y = load_dataset('data/datasets/reports_best_tyhanov.npz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:54.134059Z",
     "start_time": "2024-10-12T05:43:54.098083Z"
    }
   },
   "id": "a0d9a25ca1cb5c4b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:54.463060Z",
     "start_time": "2024-10-12T05:43:54.456062Z"
    }
   },
   "id": "fffa14b27b9fcdc7"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "with open(f'data/datasets/tyhanov_test.npz', \"wb\") as f:\n",
    "    np.savez_compressed(f, x_test=x_test, y_test=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model = create_model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:43:54.841060Z",
     "start_time": "2024-10-12T05:43:54.772059Z"
    }
   },
   "id": "a098bec7cbb731de"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2117.6211 \n",
      "Epoch 1: val_loss improved from inf to 2073.72119, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 2113.1204 - val_loss: 2073.7212 - learning_rate: 0.0010\n",
      "Epoch 2/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2351.3486 \n",
      "Epoch 2: val_loss did not improve from 2073.72119\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2314.4082 - val_loss: 2133.4207 - learning_rate: 0.0010\n",
      "Epoch 3/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2499.7998 \n",
      "Epoch 3: val_loss improved from 2073.72119 to 2066.60205, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2456.4324 - val_loss: 2066.6021 - learning_rate: 0.0010\n",
      "Epoch 4/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1568.2432\n",
      "Epoch 4: val_loss did not improve from 2066.60205\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1640.0850 - val_loss: 2081.9033 - learning_rate: 0.0010\n",
      "Epoch 5/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2235.6702 \n",
      "Epoch 5: val_loss did not improve from 2066.60205\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2228.2910 - val_loss: 2140.7864 - learning_rate: 0.0010\n",
      "Epoch 6/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1889.3496 \n",
      "Epoch 6: val_loss did not improve from 2066.60205\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1923.4530 - val_loss: 2086.3301 - learning_rate: 0.0010\n",
      "Epoch 7/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2778.7146 \n",
      "Epoch 7: val_loss did not improve from 2066.60205\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2682.1560 - val_loss: 2074.6511 - learning_rate: 0.0010\n",
      "Epoch 8/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2000.1290\n",
      "Epoch 8: val_loss improved from 2066.60205 to 2066.42603, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2007.0651 - val_loss: 2066.4260 - learning_rate: 0.0010\n",
      "Epoch 9/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2118.0200 \n",
      "Epoch 9: val_loss did not improve from 2066.42603\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2115.3240 - val_loss: 2120.7141 - learning_rate: 0.0010\n",
      "Epoch 10/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2072.9688 \n",
      "Epoch 10: val_loss did not improve from 2066.42603\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2083.0178 - val_loss: 2115.7053 - learning_rate: 0.0010\n",
      "Epoch 11/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1950.5225 \n",
      "Epoch 11: val_loss did not improve from 2066.42603\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1970.7848 - val_loss: 2109.1162 - learning_rate: 0.0010\n",
      "Epoch 12/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1945.1714 \n",
      "Epoch 12: val_loss did not improve from 2066.42603\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1961.0704 - val_loss: 2112.5571 - learning_rate: 0.0010\n",
      "Epoch 13/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2583.7805 \n",
      "Epoch 13: val_loss did not improve from 2066.42603\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2539.5312 - val_loss: 2109.8193 - learning_rate: 0.0010\n",
      "Epoch 14/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2338.1621 \n",
      "Epoch 14: val_loss did not improve from 2066.42603\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2309.6885 - val_loss: 2079.5837 - learning_rate: 0.0010\n",
      "Epoch 15/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2717.9468 \n",
      "Epoch 15: val_loss improved from 2066.42603 to 2064.98682, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2650.2581 - val_loss: 2064.9868 - learning_rate: 0.0010\n",
      "Epoch 16/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1993.8354 \n",
      "Epoch 16: val_loss improved from 2064.98682 to 2051.37183, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2000.8903 - val_loss: 2051.3718 - learning_rate: 0.0010\n",
      "Epoch 17/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1835.0848\n",
      "Epoch 17: val_loss improved from 2051.37183 to 2046.30725, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1862.2542 - val_loss: 2046.3073 - learning_rate: 0.0010\n",
      "Epoch 18/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1551.7242 \n",
      "Epoch 18: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1625.7833 - val_loss: 2047.5343 - learning_rate: 0.0010\n",
      "Epoch 19/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2252.0237 \n",
      "Epoch 19: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2227.3469 - val_loss: 2051.5515 - learning_rate: 0.0010\n",
      "Epoch 20/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1814.3870 \n",
      "Epoch 20: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1840.6492 - val_loss: 2046.6425 - learning_rate: 0.0010\n",
      "Epoch 21/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1707.4449\n",
      "Epoch 21: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1745.7933 - val_loss: 2054.7791 - learning_rate: 0.0010\n",
      "Epoch 22/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1956.7196 \n",
      "Epoch 22: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1972.7705 - val_loss: 2068.4998 - learning_rate: 0.0010\n",
      "Epoch 23/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1896.9851 \n",
      "Epoch 23: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1912.8429 - val_loss: 2056.8445 - learning_rate: 0.0010\n",
      "Epoch 24/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1834.1387\n",
      "Epoch 24: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1863.5344 - val_loss: 2071.1553 - learning_rate: 0.0010\n",
      "Epoch 25/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2270.6499 \n",
      "Epoch 25: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2255.3457 - val_loss: 2081.4419 - learning_rate: 0.0010\n",
      "Epoch 26/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2099.4011 \n",
      "Epoch 26: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2096.0999 - val_loss: 2062.9875 - learning_rate: 0.0010\n",
      "Epoch 27/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2357.1165 \n",
      "Epoch 27: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2327.1362 - val_loss: 2087.3486 - learning_rate: 0.0010\n",
      "Epoch 28/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2001.7704 \n",
      "Epoch 28: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2010.4735 - val_loss: 2063.0703 - learning_rate: 0.0010\n",
      "Epoch 29/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1813.5637 \n",
      "Epoch 29: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1856.0421 - val_loss: 2688.5806 - learning_rate: 0.0010\n",
      "Epoch 30/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2059.8157 \n",
      "Epoch 30: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2087.5112 - val_loss: 2310.6184 - learning_rate: 0.0010\n",
      "Epoch 31/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2212.8381 \n",
      "Epoch 31: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2208.8740 - val_loss: 2123.6414 - learning_rate: 0.0010\n",
      "Epoch 32/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2152.7878 \n",
      "Epoch 32: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2147.5967 - val_loss: 2098.4243 - learning_rate: 0.0010\n",
      "Epoch 33/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2278.5996 \n",
      "Epoch 33: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2259.6750 - val_loss: 2092.7012 - learning_rate: 0.0010\n",
      "Epoch 34/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2300.2690 \n",
      "Epoch 34: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2296.7207 - val_loss: 2348.8936 - learning_rate: 0.0010\n",
      "Epoch 35/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2041.8455 \n",
      "Epoch 35: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2060.0515 - val_loss: 2104.8862 - learning_rate: 0.0010\n",
      "Epoch 36/500\n",
      "\u001B[1m22/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2228.7156 \n",
      "Epoch 36: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2219.3193 - val_loss: 2103.1687 - learning_rate: 0.0010\n",
      "Epoch 37/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2237.7878\n",
      "Epoch 37: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2223.8362 - val_loss: 2072.2332 - learning_rate: 0.0010\n",
      "Epoch 38/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1988.5302 \n",
      "Epoch 38: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2004.0248 - val_loss: 2067.1873 - learning_rate: 0.0010\n",
      "Epoch 39/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2101.5261 \n",
      "Epoch 39: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2097.4746 - val_loss: 2056.9111 - learning_rate: 0.0010\n",
      "Epoch 40/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2001.7681 \n",
      "Epoch 40: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2009.0575 - val_loss: 2049.2788 - learning_rate: 0.0010\n",
      "Epoch 41/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1862.8689 \n",
      "Epoch 41: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1887.0698 - val_loss: 2077.6448 - learning_rate: 0.0010\n",
      "Epoch 42/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2288.7253 \n",
      "Epoch 42: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2273.4790 - val_loss: 2122.2986 - learning_rate: 0.0010\n",
      "Epoch 43/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1978.6240 \n",
      "Epoch 43: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1993.1165 - val_loss: 2156.3391 - learning_rate: 0.0010\n",
      "Epoch 44/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2471.2183 \n",
      "Epoch 44: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2438.3508 - val_loss: 2128.1626 - learning_rate: 0.0010\n",
      "Epoch 45/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2165.4297 \n",
      "Epoch 45: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2167.0632 - val_loss: 2183.8662 - learning_rate: 0.0010\n",
      "Epoch 46/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1792.4484 \n",
      "Epoch 46: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1819.1017 - val_loss: 2086.7458 - learning_rate: 0.0010\n",
      "Epoch 47/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2004.8030 \n",
      "Epoch 47: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2015.8639 - val_loss: 2076.2000 - learning_rate: 0.0010\n",
      "Epoch 48/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2192.1692 \n",
      "Epoch 48: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2179.3987 - val_loss: 2053.8184 - learning_rate: 0.0010\n",
      "Epoch 49/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1772.9419\n",
      "Epoch 49: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1804.3375 - val_loss: 2056.9150 - learning_rate: 0.0010\n",
      "Epoch 50/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1490.2574\n",
      "Epoch 50: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1554.0443 - val_loss: 2049.0259 - learning_rate: 0.0010\n",
      "Epoch 51/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2232.1292 \n",
      "Epoch 51: val_loss did not improve from 2046.30725\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2212.1274 - val_loss: 2046.7500 - learning_rate: 0.0010\n",
      "Epoch 52/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2179.5740 \n",
      "Epoch 52: val_loss improved from 2046.30725 to 2042.33167, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2159.3059 - val_loss: 2042.3317 - learning_rate: 0.0010\n",
      "Epoch 53/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2027.1597 \n",
      "Epoch 53: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2028.8456 - val_loss: 2067.6338 - learning_rate: 0.0010\n",
      "Epoch 54/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1965.5107 \n",
      "Epoch 54: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1976.9912 - val_loss: 2045.6443 - learning_rate: 0.0010\n",
      "Epoch 55/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2388.2798 \n",
      "Epoch 55: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2350.6858 - val_loss: 2043.8138 - learning_rate: 0.0010\n",
      "Epoch 56/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2154.5791 \n",
      "Epoch 56: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2144.5178 - val_loss: 2043.3888 - learning_rate: 0.0010\n",
      "Epoch 57/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2075.8237 \n",
      "Epoch 57: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2073.5190 - val_loss: 2065.4590 - learning_rate: 0.0010\n",
      "Epoch 58/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1721.5452 \n",
      "Epoch 58: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1757.6188 - val_loss: 2062.0127 - learning_rate: 0.0010\n",
      "Epoch 59/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2238.9124 \n",
      "Epoch 59: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2215.2739 - val_loss: 2045.2307 - learning_rate: 0.0010\n",
      "Epoch 60/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2199.0430 \n",
      "Epoch 60: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2188.1099 - val_loss: 2140.4485 - learning_rate: 0.0010\n",
      "Epoch 61/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1810.1399 \n",
      "Epoch 61: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1839.9233 - val_loss: 2081.3389 - learning_rate: 0.0010\n",
      "Epoch 62/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1895.5330\n",
      "Epoch 62: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1913.8019 - val_loss: 2046.2488 - learning_rate: 0.0010\n",
      "Epoch 63/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2254.3335 \n",
      "Epoch 63: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2239.1697 - val_loss: 2057.5864 - learning_rate: 0.0010\n",
      "Epoch 64/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1941.8972 \n",
      "Epoch 64: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1958.9414 - val_loss: 2073.9114 - learning_rate: 0.0010\n",
      "Epoch 65/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2170.5620\n",
      "Epoch 65: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2165.2817 - val_loss: 2099.8269 - learning_rate: 0.0010\n",
      "Epoch 66/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2706.9897 \n",
      "Epoch 66: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2617.6714 - val_loss: 2057.2107 - learning_rate: 0.0010\n",
      "Epoch 67/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1943.5999 \n",
      "Epoch 67: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1969.7494 - val_loss: 2198.2195 - learning_rate: 0.0010\n",
      "Epoch 68/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2145.0098 \n",
      "Epoch 68: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2140.0815 - val_loss: 2061.4211 - learning_rate: 0.0010\n",
      "Epoch 69/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2114.7690\n",
      "Epoch 69: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2106.7517 - val_loss: 2068.2903 - learning_rate: 0.0010\n",
      "Epoch 70/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2283.0483 \n",
      "Epoch 70: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2266.6309 - val_loss: 2050.3787 - learning_rate: 0.0010\n",
      "Epoch 71/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2121.7607 \n",
      "Epoch 71: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2115.5242 - val_loss: 2061.2063 - learning_rate: 0.0010\n",
      "Epoch 72/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1915.6674\n",
      "Epoch 72: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1933.0027 - val_loss: 2094.4617 - learning_rate: 0.0010\n",
      "Epoch 73/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1964.3835 \n",
      "Epoch 73: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1992.3633 - val_loss: 2131.0544 - learning_rate: 0.0010\n",
      "Epoch 74/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1884.1635 \n",
      "Epoch 74: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1922.5842 - val_loss: 2104.1223 - learning_rate: 0.0010\n",
      "Epoch 75/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2050.9463 \n",
      "Epoch 75: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2061.1833 - val_loss: 2119.7500 - learning_rate: 0.0010\n",
      "Epoch 76/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2250.7720\n",
      "Epoch 76: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2234.4258 - val_loss: 2063.6780 - learning_rate: 0.0010\n",
      "Epoch 77/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2250.0583 \n",
      "Epoch 77: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2221.7734 - val_loss: 2051.5330 - learning_rate: 0.0010\n",
      "Epoch 78/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1848.1371\n",
      "Epoch 78: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1869.7045 - val_loss: 2065.4341 - learning_rate: 0.0010\n",
      "Epoch 79/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2004.4657 \n",
      "Epoch 79: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2010.0587 - val_loss: 2062.7366 - learning_rate: 0.0010\n",
      "Epoch 80/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1744.3379 \n",
      "Epoch 80: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1790.0476 - val_loss: 2052.8308 - learning_rate: 0.0010\n",
      "Epoch 81/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2204.5127 \n",
      "Epoch 81: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2186.5005 - val_loss: 2043.8580 - learning_rate: 0.0010\n",
      "Epoch 82/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2090.2332 \n",
      "Epoch 82: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2082.5601 - val_loss: 2043.7665 - learning_rate: 0.0010\n",
      "Epoch 83/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2194.6167 \n",
      "Epoch 83: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2182.2712 - val_loss: 2084.4309 - learning_rate: 0.0010\n",
      "Epoch 84/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2328.6943 \n",
      "Epoch 84: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2301.6006 - val_loss: 2061.4011 - learning_rate: 0.0010\n",
      "Epoch 85/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2384.9253 \n",
      "Epoch 85: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2349.8162 - val_loss: 2093.9609 - learning_rate: 0.0010\n",
      "Epoch 86/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2038.8722 \n",
      "Epoch 86: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2041.2281 - val_loss: 2055.6770 - learning_rate: 0.0010\n",
      "Epoch 87/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1696.4912 \n",
      "Epoch 87: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1735.2787 - val_loss: 2042.9558 - learning_rate: 0.0010\n",
      "Epoch 88/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1946.6208\n",
      "Epoch 88: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1958.8799 - val_loss: 2135.3259 - learning_rate: 0.0010\n",
      "Epoch 89/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2211.1289 \n",
      "Epoch 89: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2217.5952 - val_loss: 2306.2021 - learning_rate: 0.0010\n",
      "Epoch 90/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2156.4155 \n",
      "Epoch 90: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2180.0291 - val_loss: 2223.7158 - learning_rate: 0.0010\n",
      "Epoch 91/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2357.2502 \n",
      "Epoch 91: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2350.1108 - val_loss: 2283.1067 - learning_rate: 0.0010\n",
      "Epoch 92/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1840.7609\n",
      "Epoch 92: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1879.1925 - val_loss: 2133.5830 - learning_rate: 0.0010\n",
      "Epoch 93/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2241.5117 \n",
      "Epoch 93: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2243.5100 - val_loss: 2279.6372 - learning_rate: 0.0010\n",
      "Epoch 94/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2096.8943 \n",
      "Epoch 94: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2103.7725 - val_loss: 2096.1543 - learning_rate: 0.0010\n",
      "Epoch 95/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1971.3383 \n",
      "Epoch 95: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1980.9279 - val_loss: 2056.6848 - learning_rate: 0.0010\n",
      "Epoch 96/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2026.7076 \n",
      "Epoch 96: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2030.8387 - val_loss: 2057.2253 - learning_rate: 0.0010\n",
      "Epoch 97/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2160.2700 \n",
      "Epoch 97: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2156.4927 - val_loss: 2079.9873 - learning_rate: 0.0010\n",
      "Epoch 98/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2177.6812 \n",
      "Epoch 98: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2172.4375 - val_loss: 2170.1672 - learning_rate: 0.0010\n",
      "Epoch 99/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2247.3203 \n",
      "Epoch 99: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2236.3735 - val_loss: 2270.1187 - learning_rate: 0.0010\n",
      "Epoch 100/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2291.6741 \n",
      "Epoch 100: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2283.3179 - val_loss: 2164.0505 - learning_rate: 0.0010\n",
      "Epoch 101/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1986.9905\n",
      "Epoch 101: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2007.6285 - val_loss: 2112.4829 - learning_rate: 0.0010\n",
      "Epoch 102/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2358.0171 \n",
      "Epoch 102: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2328.1990 - val_loss: 2059.2085 - learning_rate: 0.0010\n",
      "Epoch 103/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2210.8816 \n",
      "Epoch 103: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2193.8794 - val_loss: 2057.4358 - learning_rate: 0.0010\n",
      "Epoch 104/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2009.1434 \n",
      "Epoch 104: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2025.1208 - val_loss: 2069.9812 - learning_rate: 0.0010\n",
      "Epoch 105/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2021.0515 \n",
      "Epoch 105: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2031.6860 - val_loss: 2134.0254 - learning_rate: 0.0010\n",
      "Epoch 106/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2254.9736 \n",
      "Epoch 106: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2242.6750 - val_loss: 2085.3823 - learning_rate: 0.0010\n",
      "Epoch 107/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2361.5847 \n",
      "Epoch 107: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2335.8689 - val_loss: 2153.0366 - learning_rate: 0.0010\n",
      "Epoch 108/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2331.5244 \n",
      "Epoch 108: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2305.8086 - val_loss: 2076.9260 - learning_rate: 0.0010\n",
      "Epoch 109/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2425.9600 \n",
      "Epoch 109: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2389.2861 - val_loss: 2048.8909 - learning_rate: 0.0010\n",
      "Epoch 110/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2726.5830 \n",
      "Epoch 110: val_loss did not improve from 2042.33167\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2654.7605 - val_loss: 2071.3757 - learning_rate: 0.0010\n",
      "Epoch 111/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2083.2581 \n",
      "Epoch 111: val_loss improved from 2042.33167 to 2041.37085, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2081.1543 - val_loss: 2041.3708 - learning_rate: 0.0010\n",
      "Epoch 112/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1959.7898\n",
      "Epoch 112: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1976.7050 - val_loss: 2093.3682 - learning_rate: 0.0010\n",
      "Epoch 113/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2234.7334 \n",
      "Epoch 113: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2217.6375 - val_loss: 2115.1831 - learning_rate: 0.0010\n",
      "Epoch 114/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1761.9252\n",
      "Epoch 114: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1797.6121 - val_loss: 2066.8179 - learning_rate: 0.0010\n",
      "Epoch 115/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2190.3909 \n",
      "Epoch 115: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2172.2600 - val_loss: 2044.1758 - learning_rate: 0.0010\n",
      "Epoch 116/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2082.1069 \n",
      "Epoch 116: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2077.4080 - val_loss: 2051.3445 - learning_rate: 0.0010\n",
      "Epoch 117/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1796.5177\n",
      "Epoch 117: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1823.7173 - val_loss: 2058.4365 - learning_rate: 0.0010\n",
      "Epoch 118/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2262.5498 \n",
      "Epoch 118: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2245.7991 - val_loss: 2134.0752 - learning_rate: 0.0010\n",
      "Epoch 119/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2271.3530 \n",
      "Epoch 119: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2249.1125 - val_loss: 2088.4670 - learning_rate: 0.0010\n",
      "Epoch 120/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2108.4092 \n",
      "Epoch 120: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2105.2798 - val_loss: 2050.4553 - learning_rate: 0.0010\n",
      "Epoch 121/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2010.2267 \n",
      "Epoch 121: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2017.6276 - val_loss: 2088.6355 - learning_rate: 0.0010\n",
      "Epoch 122/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1879.1530 \n",
      "Epoch 122: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1905.0995 - val_loss: 2092.9761 - learning_rate: 0.0010\n",
      "Epoch 123/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1818.6918 \n",
      "Epoch 123: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1845.8484 - val_loss: 2043.9553 - learning_rate: 0.0010\n",
      "Epoch 124/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1934.9774 \n",
      "Epoch 124: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1947.8231 - val_loss: 2047.9531 - learning_rate: 0.0010\n",
      "Epoch 125/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2053.4099 \n",
      "Epoch 125: val_loss did not improve from 2041.37085\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2052.7927 - val_loss: 2041.9613 - learning_rate: 0.0010\n",
      "Epoch 126/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2258.1060 \n",
      "Epoch 126: val_loss improved from 2041.37085 to 2037.35901, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2233.2029 - val_loss: 2037.3590 - learning_rate: 0.0010\n",
      "Epoch 127/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1737.3672 \n",
      "Epoch 127: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1773.6959 - val_loss: 2088.6230 - learning_rate: 0.0010\n",
      "Epoch 128/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1979.8837 \n",
      "Epoch 128: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1985.8871 - val_loss: 2047.5022 - learning_rate: 0.0010\n",
      "Epoch 129/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1961.0592 \n",
      "Epoch 129: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1971.7900 - val_loss: 2050.0059 - learning_rate: 0.0010\n",
      "Epoch 130/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1840.3613\n",
      "Epoch 130: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1867.1858 - val_loss: 2061.9133 - learning_rate: 0.0010\n",
      "Epoch 131/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2086.7114 \n",
      "Epoch 131: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2083.6382 - val_loss: 2057.3684 - learning_rate: 0.0010\n",
      "Epoch 132/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2329.3921 \n",
      "Epoch 132: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2308.6460 - val_loss: 2042.9561 - learning_rate: 0.0010\n",
      "Epoch 133/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1914.4008\n",
      "Epoch 133: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1934.4541 - val_loss: 2044.1381 - learning_rate: 0.0010\n",
      "Epoch 134/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1644.8312 \n",
      "Epoch 134: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1689.6106 - val_loss: 2039.9187 - learning_rate: 0.0010\n",
      "Epoch 135/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2387.7898 \n",
      "Epoch 135: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2352.2993 - val_loss: 2049.2236 - learning_rate: 0.0010\n",
      "Epoch 136/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2318.9346 \n",
      "Epoch 136: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2288.2354 - val_loss: 2041.1095 - learning_rate: 0.0010\n",
      "Epoch 137/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2798.2764 \n",
      "Epoch 137: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2714.0249 - val_loss: 2050.0344 - learning_rate: 0.0010\n",
      "Epoch 138/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1747.1071 \n",
      "Epoch 138: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1780.5192 - val_loss: 2054.8354 - learning_rate: 0.0010\n",
      "Epoch 139/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2352.0327 \n",
      "Epoch 139: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2320.6814 - val_loss: 2053.7144 - learning_rate: 0.0010\n",
      "Epoch 140/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2043.2177 \n",
      "Epoch 140: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2046.8730 - val_loss: 2057.0242 - learning_rate: 0.0010\n",
      "Epoch 141/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2749.9663 \n",
      "Epoch 141: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2672.9702 - val_loss: 2054.6458 - learning_rate: 0.0010\n",
      "Epoch 142/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2068.7502 \n",
      "Epoch 142: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2067.4187 - val_loss: 2040.1045 - learning_rate: 0.0010\n",
      "Epoch 143/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2053.9351 \n",
      "Epoch 143: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2056.8259 - val_loss: 2092.1150 - learning_rate: 0.0010\n",
      "Epoch 144/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2175.2686 \n",
      "Epoch 144: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2168.2288 - val_loss: 2116.4668 - learning_rate: 0.0010\n",
      "Epoch 145/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2207.8389 \n",
      "Epoch 145: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2186.2542 - val_loss: 2068.7310 - learning_rate: 0.0010\n",
      "Epoch 146/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1807.1353\n",
      "Epoch 146: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1837.7977 - val_loss: 2047.9800 - learning_rate: 0.0010\n",
      "Epoch 147/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2119.3010 \n",
      "Epoch 147: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2112.3950 - val_loss: 2060.7625 - learning_rate: 0.0010\n",
      "Epoch 148/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1552.1893 \n",
      "Epoch 148: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1589.0048 - val_loss: 2046.3267 - learning_rate: 0.0010\n",
      "Epoch 149/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1572.8158 \n",
      "Epoch 149: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1625.4163 - val_loss: 2040.9635 - learning_rate: 0.0010\n",
      "Epoch 150/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2465.6172 \n",
      "Epoch 150: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2418.9072 - val_loss: 2040.6134 - learning_rate: 0.0010\n",
      "Epoch 151/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2301.1526\n",
      "Epoch 151: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2268.9607 - val_loss: 2051.8206 - learning_rate: 0.0010\n",
      "Epoch 152/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2302.4666 \n",
      "Epoch 152: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2282.4639 - val_loss: 2101.9651 - learning_rate: 0.0010\n",
      "Epoch 153/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2275.8655 \n",
      "Epoch 153: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2262.8762 - val_loss: 2079.3423 - learning_rate: 0.0010\n",
      "Epoch 154/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2066.6738 \n",
      "Epoch 154: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2067.5540 - val_loss: 2077.9919 - learning_rate: 0.0010\n",
      "Epoch 155/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2277.4626 \n",
      "Epoch 155: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2253.9438 - val_loss: 2054.2256 - learning_rate: 0.0010\n",
      "Epoch 156/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2127.6399 \n",
      "Epoch 156: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2119.9504 - val_loss: 2043.6930 - learning_rate: 0.0010\n",
      "Epoch 157/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2019.0184 \n",
      "Epoch 157: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2022.5586 - val_loss: 2052.3923 - learning_rate: 0.0010\n",
      "Epoch 158/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2373.2288 \n",
      "Epoch 158: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2334.9507 - val_loss: 2038.6501 - learning_rate: 0.0010\n",
      "Epoch 159/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1815.3419\n",
      "Epoch 159: val_loss did not improve from 2037.35901\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1841.9868 - val_loss: 2045.3068 - learning_rate: 0.0010\n",
      "Epoch 160/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1656.0746 \n",
      "Epoch 160: val_loss improved from 2037.35901 to 2036.76379, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1693.7742 - val_loss: 2036.7638 - learning_rate: 0.0010\n",
      "Epoch 161/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2226.5637 \n",
      "Epoch 161: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2207.6072 - val_loss: 2047.9290 - learning_rate: 0.0010\n",
      "Epoch 162/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2025.5592 \n",
      "Epoch 162: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2032.8958 - val_loss: 2087.6545 - learning_rate: 0.0010\n",
      "Epoch 163/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1987.4476 \n",
      "Epoch 163: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2002.0267 - val_loss: 2081.9209 - learning_rate: 0.0010\n",
      "Epoch 164/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2049.5896 \n",
      "Epoch 164: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2060.9131 - val_loss: 2087.2551 - learning_rate: 0.0010\n",
      "Epoch 165/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1719.7111\n",
      "Epoch 165: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1754.1227 - val_loss: 2071.9976 - learning_rate: 0.0010\n",
      "Epoch 166/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2008.4326 \n",
      "Epoch 166: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2016.9053 - val_loss: 2153.8054 - learning_rate: 0.0010\n",
      "Epoch 167/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1689.1213 \n",
      "Epoch 167: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1741.9890 - val_loss: 2096.9482 - learning_rate: 0.0010\n",
      "Epoch 168/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2007.2892 \n",
      "Epoch 168: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2014.3682 - val_loss: 2086.6675 - learning_rate: 0.0010\n",
      "Epoch 169/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1943.1708 \n",
      "Epoch 169: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1958.9889 - val_loss: 2090.0286 - learning_rate: 0.0010\n",
      "Epoch 170/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2037.2279 \n",
      "Epoch 170: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2046.2122 - val_loss: 2229.2930 - learning_rate: 0.0010\n",
      "Epoch 171/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2075.7720\n",
      "Epoch 171: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2087.7485 - val_loss: 2122.1594 - learning_rate: 0.0010\n",
      "Epoch 172/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2163.7524 \n",
      "Epoch 172: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2159.5908 - val_loss: 2154.1182 - learning_rate: 0.0010\n",
      "Epoch 173/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2063.7717\n",
      "Epoch 173: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2068.4954 - val_loss: 2079.4036 - learning_rate: 0.0010\n",
      "Epoch 174/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2479.4990 \n",
      "Epoch 174: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2432.5461 - val_loss: 2050.0632 - learning_rate: 0.0010\n",
      "Epoch 175/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1900.2559\n",
      "Epoch 175: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1917.5651 - val_loss: 2040.0212 - learning_rate: 0.0010\n",
      "Epoch 176/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2181.1604 \n",
      "Epoch 176: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2171.7781 - val_loss: 2255.0417 - learning_rate: 0.0010\n",
      "Epoch 177/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1899.0337 \n",
      "Epoch 177: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1923.8348 - val_loss: 2092.3721 - learning_rate: 0.0010\n",
      "Epoch 178/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1905.4297 \n",
      "Epoch 178: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1926.5923 - val_loss: 2053.0969 - learning_rate: 0.0010\n",
      "Epoch 179/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2480.5493 \n",
      "Epoch 179: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2436.5598 - val_loss: 2101.9177 - learning_rate: 0.0010\n",
      "Epoch 180/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2357.3374 \n",
      "Epoch 180: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2327.3660 - val_loss: 2141.2424 - learning_rate: 0.0010\n",
      "Epoch 181/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2221.6135 \n",
      "Epoch 181: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2206.2439 - val_loss: 2065.7686 - learning_rate: 0.0010\n",
      "Epoch 182/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1835.5342 \n",
      "Epoch 182: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1858.8301 - val_loss: 2042.1008 - learning_rate: 0.0010\n",
      "Epoch 183/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2440.4612 \n",
      "Epoch 183: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2395.6072 - val_loss: 2068.0020 - learning_rate: 0.0010\n",
      "Epoch 184/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1894.9585 \n",
      "Epoch 184: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1906.7361 - val_loss: 2044.0065 - learning_rate: 0.0010\n",
      "Epoch 185/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2130.5759 \n",
      "Epoch 185: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2131.6936 - val_loss: 2122.4587 - learning_rate: 0.0010\n",
      "Epoch 186/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2463.7075 \n",
      "Epoch 186: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2443.4912 - val_loss: 2194.5571 - learning_rate: 0.0010\n",
      "Epoch 187/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2385.9656 \n",
      "Epoch 187: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2361.5327 - val_loss: 2119.0374 - learning_rate: 0.0010\n",
      "Epoch 188/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2170.8987 \n",
      "Epoch 188: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2168.7878 - val_loss: 2177.2690 - learning_rate: 0.0010\n",
      "Epoch 189/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2215.7710 \n",
      "Epoch 189: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2206.0146 - val_loss: 2072.7456 - learning_rate: 0.0010\n",
      "Epoch 190/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1731.3562\n",
      "Epoch 190: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1768.7091 - val_loss: 2070.3875 - learning_rate: 0.0010\n",
      "Epoch 191/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1774.1980 \n",
      "Epoch 191: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1814.4188 - val_loss: 2053.4436 - learning_rate: 0.0010\n",
      "Epoch 192/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2168.1682 \n",
      "Epoch 192: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2156.7524 - val_loss: 2101.9902 - learning_rate: 0.0010\n",
      "Epoch 193/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2568.6560 \n",
      "Epoch 193: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2514.7532 - val_loss: 2053.2590 - learning_rate: 0.0010\n",
      "Epoch 194/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1725.4674\n",
      "Epoch 194: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1762.1024 - val_loss: 2079.1104 - learning_rate: 0.0010\n",
      "Epoch 195/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2153.0725 \n",
      "Epoch 195: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2142.1409 - val_loss: 2044.2181 - learning_rate: 0.0010\n",
      "Epoch 196/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2014.9808 \n",
      "Epoch 196: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2018.4751 - val_loss: 2047.4823 - learning_rate: 0.0010\n",
      "Epoch 197/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1957.0492 \n",
      "Epoch 197: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1967.8286 - val_loss: 2091.0149 - learning_rate: 0.0010\n",
      "Epoch 198/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1975.4561 \n",
      "Epoch 198: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1983.5306 - val_loss: 2052.6912 - learning_rate: 0.0010\n",
      "Epoch 199/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2143.6360 \n",
      "Epoch 199: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2139.1929 - val_loss: 2118.9333 - learning_rate: 0.0010\n",
      "Epoch 200/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1812.2413 \n",
      "Epoch 200: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1840.5535 - val_loss: 2052.7974 - learning_rate: 0.0010\n",
      "Epoch 201/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2112.9922 \n",
      "Epoch 201: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2105.2249 - val_loss: 2043.6354 - learning_rate: 0.0010\n",
      "Epoch 202/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1732.6277 \n",
      "Epoch 202: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1768.1110 - val_loss: 2065.3784 - learning_rate: 0.0010\n",
      "Epoch 203/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2538.3052 \n",
      "Epoch 203: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2498.2703 - val_loss: 2104.6619 - learning_rate: 0.0010\n",
      "Epoch 204/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2569.7368 \n",
      "Epoch 204: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2537.4744 - val_loss: 2177.1008 - learning_rate: 0.0010\n",
      "Epoch 205/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2232.7861\n",
      "Epoch 205: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2231.6729 - val_loss: 2263.4607 - learning_rate: 0.0010\n",
      "Epoch 206/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1949.4250 \n",
      "Epoch 206: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1969.7399 - val_loss: 2104.8718 - learning_rate: 0.0010\n",
      "Epoch 207/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2131.2393 \n",
      "Epoch 207: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2124.9312 - val_loss: 2066.8496 - learning_rate: 0.0010\n",
      "Epoch 208/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2061.9517 \n",
      "Epoch 208: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2062.9946 - val_loss: 2049.4072 - learning_rate: 0.0010\n",
      "Epoch 209/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2627.7883 \n",
      "Epoch 209: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2565.7988 - val_loss: 2045.8600 - learning_rate: 0.0010\n",
      "Epoch 210/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2077.4631 \n",
      "Epoch 210: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2075.1086 - val_loss: 2080.7114 - learning_rate: 0.0010\n",
      "Epoch 211/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1936.6836 \n",
      "Epoch 211: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1954.2224 - val_loss: 2053.6211 - learning_rate: 0.0010\n",
      "Epoch 212/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2056.2139 \n",
      "Epoch 212: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2057.6201 - val_loss: 2048.0400 - learning_rate: 0.0010\n",
      "Epoch 213/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1621.3163 \n",
      "Epoch 213: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1667.6879 - val_loss: 2046.3218 - learning_rate: 0.0010\n",
      "Epoch 214/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2137.7278 \n",
      "Epoch 214: val_loss did not improve from 2036.76379\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2128.1316 - val_loss: 2039.7650 - learning_rate: 0.0010\n",
      "Epoch 215/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2465.2344 \n",
      "Epoch 215: val_loss improved from 2036.76379 to 2036.40430, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2419.3276 - val_loss: 2036.4043 - learning_rate: 0.0010\n",
      "Epoch 216/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2208.0586 \n",
      "Epoch 216: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2194.0371 - val_loss: 2207.8904 - learning_rate: 0.0010\n",
      "Epoch 217/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1932.0637\n",
      "Epoch 217: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1959.4745 - val_loss: 2103.0652 - learning_rate: 0.0010\n",
      "Epoch 218/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1988.8586 \n",
      "Epoch 218: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2008.2767 - val_loss: 2256.2080 - learning_rate: 0.0010\n",
      "Epoch 219/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2424.2156 \n",
      "Epoch 219: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2402.1914 - val_loss: 2144.9861 - learning_rate: 0.0010\n",
      "Epoch 220/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2473.4336 \n",
      "Epoch 220: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2441.2629 - val_loss: 2292.6252 - learning_rate: 0.0010\n",
      "Epoch 221/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1932.5392 \n",
      "Epoch 221: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1954.4509 - val_loss: 2088.4915 - learning_rate: 0.0010\n",
      "Epoch 222/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1918.4700 \n",
      "Epoch 222: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1937.0728 - val_loss: 2058.2119 - learning_rate: 0.0010\n",
      "Epoch 223/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2456.9211 \n",
      "Epoch 223: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2415.4238 - val_loss: 2065.6938 - learning_rate: 0.0010\n",
      "Epoch 224/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2456.2705 \n",
      "Epoch 224: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2412.0659 - val_loss: 2052.5303 - learning_rate: 0.0010\n",
      "Epoch 225/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2035.8196 \n",
      "Epoch 225: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2044.6306 - val_loss: 2041.2565 - learning_rate: 0.0010\n",
      "Epoch 226/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1747.6995\n",
      "Epoch 226: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1794.7638 - val_loss: 2056.5583 - learning_rate: 0.0010\n",
      "Epoch 227/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2407.7314 \n",
      "Epoch 227: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2368.1279 - val_loss: 2041.3563 - learning_rate: 0.0010\n",
      "Epoch 228/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2358.3799 \n",
      "Epoch 228: val_loss did not improve from 2036.40430\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2325.8687 - val_loss: 2065.9548 - learning_rate: 0.0010\n",
      "Epoch 229/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1881.0221\n",
      "Epoch 229: val_loss improved from 2036.40430 to 2036.23230, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1901.0492 - val_loss: 2036.2323 - learning_rate: 0.0010\n",
      "Epoch 230/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2355.3889 \n",
      "Epoch 230: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2320.3047 - val_loss: 2041.2463 - learning_rate: 0.0010\n",
      "Epoch 231/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2378.4177 \n",
      "Epoch 231: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2330.9248 - val_loss: 2037.2212 - learning_rate: 0.0010\n",
      "Epoch 232/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2341.2812 \n",
      "Epoch 232: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2308.4229 - val_loss: 2036.6243 - learning_rate: 0.0010\n",
      "Epoch 233/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1899.6188 \n",
      "Epoch 233: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1928.8632 - val_loss: 2067.2749 - learning_rate: 0.0010\n",
      "Epoch 234/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2051.1792 \n",
      "Epoch 234: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2053.1323 - val_loss: 2074.3040 - learning_rate: 0.0010\n",
      "Epoch 235/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1575.2097 \n",
      "Epoch 235: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1627.0217 - val_loss: 2050.9238 - learning_rate: 0.0010\n",
      "Epoch 236/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1895.6514 \n",
      "Epoch 236: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1914.0999 - val_loss: 2045.4115 - learning_rate: 0.0010\n",
      "Epoch 237/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2161.9783 \n",
      "Epoch 237: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2154.3542 - val_loss: 2056.0667 - learning_rate: 0.0010\n",
      "Epoch 238/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2260.5525 \n",
      "Epoch 238: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2244.9692 - val_loss: 2065.9182 - learning_rate: 0.0010\n",
      "Epoch 239/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1989.2109\n",
      "Epoch 239: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1997.1031 - val_loss: 2037.2094 - learning_rate: 0.0010\n",
      "Epoch 240/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2182.5298 \n",
      "Epoch 240: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2165.7939 - val_loss: 2038.4241 - learning_rate: 0.0010\n",
      "Epoch 241/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1959.0571 \n",
      "Epoch 241: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1971.1718 - val_loss: 2077.6843 - learning_rate: 0.0010\n",
      "Epoch 242/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1987.4011 \n",
      "Epoch 242: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1995.5848 - val_loss: 2062.7505 - learning_rate: 0.0010\n",
      "Epoch 243/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1650.7729 \n",
      "Epoch 243: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1691.9142 - val_loss: 2037.0070 - learning_rate: 0.0010\n",
      "Epoch 244/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1955.7252 \n",
      "Epoch 244: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1965.3026 - val_loss: 2056.6301 - learning_rate: 0.0010\n",
      "Epoch 245/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2243.5840 \n",
      "Epoch 245: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2226.2825 - val_loss: 2130.8557 - learning_rate: 0.0010\n",
      "Epoch 246/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2209.8657 \n",
      "Epoch 246: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2196.1348 - val_loss: 2079.5515 - learning_rate: 0.0010\n",
      "Epoch 247/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2038.7542 \n",
      "Epoch 247: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2037.6089 - val_loss: 2087.5403 - learning_rate: 0.0010\n",
      "Epoch 248/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1813.0565\n",
      "Epoch 248: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1845.1221 - val_loss: 2114.9512 - learning_rate: 0.0010\n",
      "Epoch 249/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2373.5869 \n",
      "Epoch 249: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2345.2754 - val_loss: 2103.1716 - learning_rate: 0.0010\n",
      "Epoch 250/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2451.0859 \n",
      "Epoch 250: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2410.0808 - val_loss: 2046.0508 - learning_rate: 0.0010\n",
      "Epoch 251/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2305.2031 \n",
      "Epoch 251: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2279.6550 - val_loss: 2064.4534 - learning_rate: 0.0010\n",
      "Epoch 252/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2332.1265 \n",
      "Epoch 252: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2306.6592 - val_loss: 2079.5427 - learning_rate: 0.0010\n",
      "Epoch 253/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1938.0167 \n",
      "Epoch 253: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1949.7228 - val_loss: 2081.6685 - learning_rate: 0.0010\n",
      "Epoch 254/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2137.8535 \n",
      "Epoch 254: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2128.3701 - val_loss: 2051.9033 - learning_rate: 0.0010\n",
      "Epoch 255/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2166.8457 \n",
      "Epoch 255: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2155.0535 - val_loss: 2039.4502 - learning_rate: 0.0010\n",
      "Epoch 256/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2021.8894 \n",
      "Epoch 256: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2024.9286 - val_loss: 2045.0710 - learning_rate: 0.0010\n",
      "Epoch 257/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2232.8821 \n",
      "Epoch 257: val_loss did not improve from 2036.23230\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2212.9258 - val_loss: 2055.2517 - learning_rate: 0.0010\n",
      "Epoch 258/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2028.5680 \n",
      "Epoch 258: val_loss improved from 2036.23230 to 2033.86145, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2033.9270 - val_loss: 2033.8615 - learning_rate: 0.0010\n",
      "Epoch 259/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2178.4724 \n",
      "Epoch 259: val_loss improved from 2033.86145 to 2033.81653, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2162.8083 - val_loss: 2033.8165 - learning_rate: 0.0010\n",
      "Epoch 260/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2034.8536 \n",
      "Epoch 260: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2036.0306 - val_loss: 2042.1664 - learning_rate: 0.0010\n",
      "Epoch 261/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2140.2727 \n",
      "Epoch 261: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2130.8096 - val_loss: 2076.4666 - learning_rate: 0.0010\n",
      "Epoch 262/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1960.8904 \n",
      "Epoch 262: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1972.3929 - val_loss: 2074.8291 - learning_rate: 0.0010\n",
      "Epoch 263/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1765.6777 \n",
      "Epoch 263: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1802.1566 - val_loss: 2057.0876 - learning_rate: 0.0010\n",
      "Epoch 264/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1853.4946 \n",
      "Epoch 264: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1875.5769 - val_loss: 2149.2664 - learning_rate: 0.0010\n",
      "Epoch 265/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2366.5271 \n",
      "Epoch 265: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2343.3738 - val_loss: 2174.0334 - learning_rate: 0.0010\n",
      "Epoch 266/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2223.9531 \n",
      "Epoch 266: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2216.2764 - val_loss: 2128.5654 - learning_rate: 0.0010\n",
      "Epoch 267/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2186.3196 \n",
      "Epoch 267: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2170.1467 - val_loss: 2149.9373 - learning_rate: 0.0010\n",
      "Epoch 268/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2439.1799 \n",
      "Epoch 268: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2405.2905 - val_loss: 2177.4624 - learning_rate: 0.0010\n",
      "Epoch 269/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2007.6367 \n",
      "Epoch 269: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2031.1489 - val_loss: 2186.5967 - learning_rate: 0.0010\n",
      "Epoch 270/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2125.8018 \n",
      "Epoch 270: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2125.5000 - val_loss: 2152.2942 - learning_rate: 0.0010\n",
      "Epoch 271/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1629.6798 \n",
      "Epoch 271: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1676.2592 - val_loss: 2074.9885 - learning_rate: 0.0010\n",
      "Epoch 272/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1995.8888 \n",
      "Epoch 272: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2001.1121 - val_loss: 2043.4879 - learning_rate: 0.0010\n",
      "Epoch 273/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1868.7681\n",
      "Epoch 273: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1881.8905 - val_loss: 2039.6381 - learning_rate: 0.0010\n",
      "Epoch 274/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2231.0649 \n",
      "Epoch 274: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2210.5037 - val_loss: 2047.9763 - learning_rate: 0.0010\n",
      "Epoch 275/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2076.8625 \n",
      "Epoch 275: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2070.5620 - val_loss: 2045.0389 - learning_rate: 0.0010\n",
      "Epoch 276/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2513.8984 \n",
      "Epoch 276: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2463.0820 - val_loss: 2045.1122 - learning_rate: 0.0010\n",
      "Epoch 277/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2382.1526 \n",
      "Epoch 277: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2345.8589 - val_loss: 2041.5239 - learning_rate: 0.0010\n",
      "Epoch 278/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1933.4196 \n",
      "Epoch 278: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1947.4442 - val_loss: 2038.2041 - learning_rate: 0.0010\n",
      "Epoch 279/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2124.4565 \n",
      "Epoch 279: val_loss did not improve from 2033.81653\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2117.0774 - val_loss: 2035.9325 - learning_rate: 0.0010\n",
      "Epoch 280/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2017.8529 \n",
      "Epoch 280: val_loss improved from 2033.81653 to 2032.51477, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2018.1552 - val_loss: 2032.5148 - learning_rate: 0.0010\n",
      "Epoch 281/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2190.0913 \n",
      "Epoch 281: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2175.6846 - val_loss: 2093.6616 - learning_rate: 0.0010\n",
      "Epoch 282/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2315.2539 \n",
      "Epoch 282: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2290.0259 - val_loss: 2056.7578 - learning_rate: 0.0010\n",
      "Epoch 283/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2299.8975 \n",
      "Epoch 283: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2270.6992 - val_loss: 2061.9551 - learning_rate: 0.0010\n",
      "Epoch 284/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2111.3325 \n",
      "Epoch 284: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2108.0210 - val_loss: 2071.0625 - learning_rate: 0.0010\n",
      "Epoch 285/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2258.6125 \n",
      "Epoch 285: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2233.9834 - val_loss: 2046.0675 - learning_rate: 0.0010\n",
      "Epoch 286/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2170.0354\n",
      "Epoch 286: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2156.4631 - val_loss: 2051.6782 - learning_rate: 0.0010\n",
      "Epoch 287/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1882.2440\n",
      "Epoch 287: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1903.0964 - val_loss: 2060.3215 - learning_rate: 0.0010\n",
      "Epoch 288/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2157.5647 \n",
      "Epoch 288: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2141.2581 - val_loss: 2046.2855 - learning_rate: 0.0010\n",
      "Epoch 289/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2273.5984 \n",
      "Epoch 289: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2249.8525 - val_loss: 2060.0898 - learning_rate: 0.0010\n",
      "Epoch 290/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2622.8638 \n",
      "Epoch 290: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2562.2849 - val_loss: 2070.9602 - learning_rate: 0.0010\n",
      "Epoch 291/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2017.3174 \n",
      "Epoch 291: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2023.2250 - val_loss: 2309.4734 - learning_rate: 0.0010\n",
      "Epoch 292/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2071.6184 \n",
      "Epoch 292: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2092.0869 - val_loss: 2202.2241 - learning_rate: 0.0010\n",
      "Epoch 293/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2633.0186 \n",
      "Epoch 293: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2581.6848 - val_loss: 2184.4185 - learning_rate: 0.0010\n",
      "Epoch 294/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1867.1299 \n",
      "Epoch 294: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1897.5789 - val_loss: 2213.7754 - learning_rate: 0.0010\n",
      "Epoch 295/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2225.8149 \n",
      "Epoch 295: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2212.0444 - val_loss: 2110.6089 - learning_rate: 0.0010\n",
      "Epoch 296/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2200.2000 \n",
      "Epoch 296: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2190.6089 - val_loss: 2090.1121 - learning_rate: 0.0010\n",
      "Epoch 297/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1798.1123 \n",
      "Epoch 297: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1846.4581 - val_loss: 2464.9236 - learning_rate: 0.0010\n",
      "Epoch 298/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2626.6497 \n",
      "Epoch 298: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2562.9360 - val_loss: 2068.4536 - learning_rate: 0.0010\n",
      "Epoch 299/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2272.3872 \n",
      "Epoch 299: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2250.8611 - val_loss: 2054.7888 - learning_rate: 0.0010\n",
      "Epoch 300/500\n",
      "\u001B[1m22/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1710.6691\n",
      "Epoch 300: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1778.0553 - val_loss: 2056.5278 - learning_rate: 0.0010\n",
      "Epoch 301/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2124.4893 \n",
      "Epoch 301: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2114.9910 - val_loss: 2075.5581 - learning_rate: 0.0010\n",
      "Epoch 302/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1952.7080 \n",
      "Epoch 302: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1964.9852 - val_loss: 2050.5227 - learning_rate: 0.0010\n",
      "Epoch 303/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1838.3020 \n",
      "Epoch 303: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1873.3967 - val_loss: 2039.4606 - learning_rate: 0.0010\n",
      "Epoch 304/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1842.6566\n",
      "Epoch 304: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1866.6377 - val_loss: 2044.2021 - learning_rate: 0.0010\n",
      "Epoch 305/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1785.1188 \n",
      "Epoch 305: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1804.9226 - val_loss: 2037.6625 - learning_rate: 0.0010\n",
      "Epoch 306/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2415.2139 \n",
      "Epoch 306: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2374.0708 - val_loss: 2042.7190 - learning_rate: 0.0010\n",
      "Epoch 307/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1897.8151 \n",
      "Epoch 307: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1916.2283 - val_loss: 2040.8910 - learning_rate: 0.0010\n",
      "Epoch 308/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2309.3118 \n",
      "Epoch 308: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2275.4795 - val_loss: 2050.7119 - learning_rate: 0.0010\n",
      "Epoch 309/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1775.5385 \n",
      "Epoch 309: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1811.2850 - val_loss: 2060.9651 - learning_rate: 0.0010\n",
      "Epoch 310/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2063.4988\n",
      "Epoch 310: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2073.2717 - val_loss: 2052.6638 - learning_rate: 0.0010\n",
      "Epoch 311/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1809.4497 \n",
      "Epoch 311: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1837.2108 - val_loss: 2048.3555 - learning_rate: 0.0010\n",
      "Epoch 312/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2488.1138 \n",
      "Epoch 312: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2427.9407 - val_loss: 2041.8348 - learning_rate: 0.0010\n",
      "Epoch 313/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1972.4595\n",
      "Epoch 313: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1977.3870 - val_loss: 2045.7056 - learning_rate: 0.0010\n",
      "Epoch 314/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1910.3264\n",
      "Epoch 314: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1919.9954 - val_loss: 2035.8378 - learning_rate: 0.0010\n",
      "Epoch 315/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1994.5367 \n",
      "Epoch 315: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1998.2515 - val_loss: 2036.0580 - learning_rate: 0.0010\n",
      "Epoch 316/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1981.5741 \n",
      "Epoch 316: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1987.0620 - val_loss: 2043.1453 - learning_rate: 0.0010\n",
      "Epoch 317/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2129.0898 \n",
      "Epoch 317: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2126.4233 - val_loss: 2155.9829 - learning_rate: 0.0010\n",
      "Epoch 318/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1749.7144 \n",
      "Epoch 318: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1786.5315 - val_loss: 2098.5769 - learning_rate: 0.0010\n",
      "Epoch 319/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1934.8745 \n",
      "Epoch 319: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1951.4526 - val_loss: 2097.9290 - learning_rate: 0.0010\n",
      "Epoch 320/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2139.6812 \n",
      "Epoch 320: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2132.2734 - val_loss: 2055.8323 - learning_rate: 0.0010\n",
      "Epoch 321/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2273.3259 \n",
      "Epoch 321: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2249.0793 - val_loss: 2039.4598 - learning_rate: 0.0010\n",
      "Epoch 322/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2587.2842 \n",
      "Epoch 322: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2511.7747 - val_loss: 2042.0682 - learning_rate: 0.0010\n",
      "Epoch 323/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2685.3892 \n",
      "Epoch 323: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2592.5771 - val_loss: 2037.8629 - learning_rate: 0.0010\n",
      "Epoch 324/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2079.5098 \n",
      "Epoch 324: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2076.4907 - val_loss: 2040.0222 - learning_rate: 0.0010\n",
      "Epoch 325/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1867.9391 \n",
      "Epoch 325: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1888.5359 - val_loss: 2074.4978 - learning_rate: 0.0010\n",
      "Epoch 326/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2207.7000 \n",
      "Epoch 326: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2193.3440 - val_loss: 2061.4019 - learning_rate: 0.0010\n",
      "Epoch 327/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1892.0288 \n",
      "Epoch 327: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1912.6951 - val_loss: 2124.1609 - learning_rate: 0.0010\n",
      "Epoch 328/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1716.2438 \n",
      "Epoch 328: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1759.3936 - val_loss: 2068.6958 - learning_rate: 0.0010\n",
      "Epoch 329/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2622.9983 \n",
      "Epoch 329: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2582.5239 - val_loss: 2066.2708 - learning_rate: 0.0010\n",
      "Epoch 330/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2124.2166 \n",
      "Epoch 330: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2118.3130 - val_loss: 2049.5337 - learning_rate: 0.0010\n",
      "Epoch 331/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2139.7949 \n",
      "Epoch 331: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2136.2903 - val_loss: 2086.3069 - learning_rate: 0.0010\n",
      "Epoch 332/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1886.9249 \n",
      "Epoch 332: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1913.6985 - val_loss: 2098.4924 - learning_rate: 0.0010\n",
      "Epoch 333/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2013.2720 \n",
      "Epoch 333: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2021.7997 - val_loss: 2097.6487 - learning_rate: 0.0010\n",
      "Epoch 334/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1997.4498\n",
      "Epoch 334: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2008.6382 - val_loss: 2056.8279 - learning_rate: 0.0010\n",
      "Epoch 335/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2327.9651 \n",
      "Epoch 335: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2284.4214 - val_loss: 2049.1729 - learning_rate: 0.0010\n",
      "Epoch 336/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2085.7004\n",
      "Epoch 336: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2082.8459 - val_loss: 2037.6630 - learning_rate: 0.0010\n",
      "Epoch 337/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2015.9084 \n",
      "Epoch 337: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2020.5437 - val_loss: 2074.9246 - learning_rate: 0.0010\n",
      "Epoch 338/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2336.0947 \n",
      "Epoch 338: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2306.9160 - val_loss: 2058.3813 - learning_rate: 0.0010\n",
      "Epoch 339/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2131.1489 \n",
      "Epoch 339: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2119.2876 - val_loss: 2038.4330 - learning_rate: 0.0010\n",
      "Epoch 340/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2057.7852 \n",
      "Epoch 340: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2055.6841 - val_loss: 2036.8533 - learning_rate: 0.0010\n",
      "Epoch 341/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1730.1942 \n",
      "Epoch 341: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1764.2662 - val_loss: 2041.7887 - learning_rate: 0.0010\n",
      "Epoch 342/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1676.3713\n",
      "Epoch 342: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1715.3741 - val_loss: 2038.3020 - learning_rate: 0.0010\n",
      "Epoch 343/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1821.4042\n",
      "Epoch 343: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1854.2096 - val_loss: 2032.8547 - learning_rate: 0.0010\n",
      "Epoch 344/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2007.6814 \n",
      "Epoch 344: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2019.5657 - val_loss: 2066.3435 - learning_rate: 0.0010\n",
      "Epoch 345/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1987.8690 \n",
      "Epoch 345: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1998.2693 - val_loss: 2044.4060 - learning_rate: 0.0010\n",
      "Epoch 346/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2308.8623 \n",
      "Epoch 346: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2280.4419 - val_loss: 2093.1729 - learning_rate: 0.0010\n",
      "Epoch 347/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2080.4829 \n",
      "Epoch 347: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2088.0283 - val_loss: 2121.3206 - learning_rate: 0.0010\n",
      "Epoch 348/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2495.0476 \n",
      "Epoch 348: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2450.8953 - val_loss: 2062.9126 - learning_rate: 0.0010\n",
      "Epoch 349/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2248.1736 \n",
      "Epoch 349: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2228.3506 - val_loss: 2068.0762 - learning_rate: 0.0010\n",
      "Epoch 350/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1764.7695\n",
      "Epoch 350: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1793.5205 - val_loss: 2043.7520 - learning_rate: 0.0010\n",
      "Epoch 351/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2126.4177 \n",
      "Epoch 351: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2115.8511 - val_loss: 2046.0784 - learning_rate: 0.0010\n",
      "Epoch 352/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2324.3657 \n",
      "Epoch 352: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2292.3989 - val_loss: 2051.8328 - learning_rate: 0.0010\n",
      "Epoch 353/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2064.6057\n",
      "Epoch 353: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2064.3630 - val_loss: 2046.1545 - learning_rate: 0.0010\n",
      "Epoch 354/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2232.6565 \n",
      "Epoch 354: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2212.9319 - val_loss: 2062.0374 - learning_rate: 0.0010\n",
      "Epoch 355/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2086.5706\n",
      "Epoch 355: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2089.3042 - val_loss: 2078.9944 - learning_rate: 0.0010\n",
      "Epoch 356/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1872.0560 \n",
      "Epoch 356: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1893.8348 - val_loss: 2052.0437 - learning_rate: 0.0010\n",
      "Epoch 357/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2227.9065 \n",
      "Epoch 357: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2209.7117 - val_loss: 2084.1316 - learning_rate: 0.0010\n",
      "Epoch 358/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2106.6111 \n",
      "Epoch 358: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2107.8689 - val_loss: 2277.7825 - learning_rate: 0.0010\n",
      "Epoch 359/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1952.8381\n",
      "Epoch 359: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1977.0375 - val_loss: 2044.0549 - learning_rate: 0.0010\n",
      "Epoch 360/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1998.4767 \n",
      "Epoch 360: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2002.0586 - val_loss: 2038.0461 - learning_rate: 0.0010\n",
      "Epoch 361/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2195.0840 \n",
      "Epoch 361: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2176.8442 - val_loss: 2067.1206 - learning_rate: 0.0010\n",
      "Epoch 362/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1939.8142 \n",
      "Epoch 362: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1951.7783 - val_loss: 2059.4429 - learning_rate: 0.0010\n",
      "Epoch 363/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1622.5308\n",
      "Epoch 363: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1685.4006 - val_loss: 2080.8037 - learning_rate: 0.0010\n",
      "Epoch 364/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1923.9131 \n",
      "Epoch 364: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1941.2278 - val_loss: 2071.0955 - learning_rate: 0.0010\n",
      "Epoch 365/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2328.8428 \n",
      "Epoch 365: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2285.7256 - val_loss: 2067.8252 - learning_rate: 0.0010\n",
      "Epoch 366/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1804.6217 \n",
      "Epoch 366: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1833.2742 - val_loss: 2050.7322 - learning_rate: 0.0010\n",
      "Epoch 367/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2198.6809 \n",
      "Epoch 367: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2180.8232 - val_loss: 2049.1023 - learning_rate: 0.0010\n",
      "Epoch 368/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2247.6758 \n",
      "Epoch 368: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2226.2532 - val_loss: 2076.3140 - learning_rate: 0.0010\n",
      "Epoch 369/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1850.8190 \n",
      "Epoch 369: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1871.8220 - val_loss: 2039.9978 - learning_rate: 0.0010\n",
      "Epoch 370/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2301.7483 \n",
      "Epoch 370: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2275.0747 - val_loss: 2090.9673 - learning_rate: 0.0010\n",
      "Epoch 371/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2601.5798 \n",
      "Epoch 371: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2543.7305 - val_loss: 2090.9482 - learning_rate: 0.0010\n",
      "Epoch 372/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2066.3167 \n",
      "Epoch 372: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2072.5852 - val_loss: 2119.1416 - learning_rate: 0.0010\n",
      "Epoch 373/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1815.9229 \n",
      "Epoch 373: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1853.1975 - val_loss: 2062.5654 - learning_rate: 0.0010\n",
      "Epoch 374/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2029.8761 \n",
      "Epoch 374: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2032.7682 - val_loss: 2076.1753 - learning_rate: 0.0010\n",
      "Epoch 375/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2224.4910 \n",
      "Epoch 375: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2206.9534 - val_loss: 2070.9321 - learning_rate: 0.0010\n",
      "Epoch 376/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2331.6624 \n",
      "Epoch 376: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2291.3574 - val_loss: 2052.0586 - learning_rate: 0.0010\n",
      "Epoch 377/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2003.0139 \n",
      "Epoch 377: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2014.3923 - val_loss: 2049.4814 - learning_rate: 0.0010\n",
      "Epoch 378/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1898.5985 \n",
      "Epoch 378: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1922.4255 - val_loss: 2044.3240 - learning_rate: 0.0010\n",
      "Epoch 379/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2335.0349 \n",
      "Epoch 379: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2292.2947 - val_loss: 2041.7439 - learning_rate: 0.0010\n",
      "Epoch 380/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2232.3022 \n",
      "Epoch 380: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2211.1931 - val_loss: 2068.7793 - learning_rate: 0.0010\n",
      "Epoch 381/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1942.7661 \n",
      "Epoch 381: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1952.8970 - val_loss: 2035.8306 - learning_rate: 0.0010\n",
      "Epoch 382/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2021.8032 \n",
      "Epoch 382: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2027.2358 - val_loss: 2040.3193 - learning_rate: 0.0010\n",
      "Epoch 383/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2409.9724\n",
      "Epoch 383: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2371.4839 - val_loss: 2049.9915 - learning_rate: 0.0010\n",
      "Epoch 384/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2157.8247 \n",
      "Epoch 384: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2142.0486 - val_loss: 2069.9102 - learning_rate: 0.0010\n",
      "Epoch 385/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1974.0269 \n",
      "Epoch 385: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1982.3229 - val_loss: 2040.0273 - learning_rate: 0.0010\n",
      "Epoch 386/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2084.5232 \n",
      "Epoch 386: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2077.9019 - val_loss: 2040.3286 - learning_rate: 0.0010\n",
      "Epoch 387/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1750.5411 \n",
      "Epoch 387: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1795.4933 - val_loss: 2038.7372 - learning_rate: 0.0010\n",
      "Epoch 388/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2262.5100 \n",
      "Epoch 388: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2240.6213 - val_loss: 2037.5118 - learning_rate: 0.0010\n",
      "Epoch 389/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1724.7676 \n",
      "Epoch 389: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1748.3146 - val_loss: 2059.8281 - learning_rate: 0.0010\n",
      "Epoch 390/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2163.6831 \n",
      "Epoch 390: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2152.1924 - val_loss: 2033.3240 - learning_rate: 0.0010\n",
      "Epoch 391/500\n",
      "\u001B[1m22/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2126.1384 \n",
      "Epoch 391: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2106.5017 - val_loss: 2039.5358 - learning_rate: 0.0010\n",
      "Epoch 392/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1900.0787 \n",
      "Epoch 392: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1917.2087 - val_loss: 2060.7051 - learning_rate: 0.0010\n",
      "Epoch 393/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1964.3236 \n",
      "Epoch 393: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1975.5819 - val_loss: 2059.5623 - learning_rate: 0.0010\n",
      "Epoch 394/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2290.9817 \n",
      "Epoch 394: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2257.7271 - val_loss: 2038.4199 - learning_rate: 0.0010\n",
      "Epoch 395/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2281.1902 \n",
      "Epoch 395: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2254.3945 - val_loss: 2041.4386 - learning_rate: 0.0010\n",
      "Epoch 396/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1951.5560 \n",
      "Epoch 396: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1962.6050 - val_loss: 2035.7340 - learning_rate: 0.0010\n",
      "Epoch 397/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1864.6772\n",
      "Epoch 397: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1890.6643 - val_loss: 2041.4216 - learning_rate: 0.0010\n",
      "Epoch 398/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1899.4650 \n",
      "Epoch 398: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1915.5844 - val_loss: 2060.6472 - learning_rate: 0.0010\n",
      "Epoch 399/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2006.7196 \n",
      "Epoch 399: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2012.4791 - val_loss: 2051.1060 - learning_rate: 0.0010\n",
      "Epoch 400/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2370.4229 \n",
      "Epoch 400: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2338.5139 - val_loss: 2066.5967 - learning_rate: 0.0010\n",
      "Epoch 401/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2330.9705 \n",
      "Epoch 401: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2306.9363 - val_loss: 2102.8555 - learning_rate: 0.0010\n",
      "Epoch 402/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2467.8643 \n",
      "Epoch 402: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2417.4937 - val_loss: 2082.6541 - learning_rate: 0.0010\n",
      "Epoch 403/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2192.7271 \n",
      "Epoch 403: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2180.8738 - val_loss: 2067.5801 - learning_rate: 0.0010\n",
      "Epoch 404/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1587.9004\n",
      "Epoch 404: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1657.2755 - val_loss: 2043.6849 - learning_rate: 0.0010\n",
      "Epoch 405/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2065.4751 \n",
      "Epoch 405: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2065.5745 - val_loss: 2036.6199 - learning_rate: 0.0010\n",
      "Epoch 406/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2033.4548\n",
      "Epoch 406: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2034.4098 - val_loss: 2046.8438 - learning_rate: 0.0010\n",
      "Epoch 407/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1995.4435 \n",
      "Epoch 407: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2001.4773 - val_loss: 2048.3269 - learning_rate: 0.0010\n",
      "Epoch 408/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2053.2192 \n",
      "Epoch 408: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2059.0759 - val_loss: 2048.0107 - learning_rate: 0.0010\n",
      "Epoch 409/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2289.8345 \n",
      "Epoch 409: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2260.9380 - val_loss: 2047.2421 - learning_rate: 0.0010\n",
      "Epoch 410/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1889.0709 \n",
      "Epoch 410: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1906.6067 - val_loss: 2047.1896 - learning_rate: 0.0010\n",
      "Epoch 411/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2382.5835 \n",
      "Epoch 411: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2344.5732 - val_loss: 2040.5251 - learning_rate: 0.0010\n",
      "Epoch 412/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2379.1956 \n",
      "Epoch 412: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2341.9490 - val_loss: 2038.5878 - learning_rate: 0.0010\n",
      "Epoch 413/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1917.8412 \n",
      "Epoch 413: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1927.9167 - val_loss: 2041.5481 - learning_rate: 0.0010\n",
      "Epoch 414/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1984.6456 \n",
      "Epoch 414: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1993.0684 - val_loss: 2037.1248 - learning_rate: 0.0010\n",
      "Epoch 415/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2175.1558\n",
      "Epoch 415: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2159.5994 - val_loss: 2043.4603 - learning_rate: 0.0010\n",
      "Epoch 416/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2085.1094 \n",
      "Epoch 416: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2081.4365 - val_loss: 2048.1611 - learning_rate: 0.0010\n",
      "Epoch 417/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1899.1781 \n",
      "Epoch 417: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1913.4065 - val_loss: 2039.7212 - learning_rate: 0.0010\n",
      "Epoch 418/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1765.9036 \n",
      "Epoch 418: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1804.3781 - val_loss: 2036.7438 - learning_rate: 0.0010\n",
      "Epoch 419/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2142.0647 \n",
      "Epoch 419: val_loss did not improve from 2032.51477\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2132.3794 - val_loss: 2037.5740 - learning_rate: 0.0010\n",
      "Epoch 420/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1835.9790 \n",
      "Epoch 420: val_loss improved from 2032.51477 to 2031.05872, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1856.2207 - val_loss: 2031.0587 - learning_rate: 0.0010\n",
      "Epoch 421/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2162.3855 \n",
      "Epoch 421: val_loss did not improve from 2031.05872\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2146.5071 - val_loss: 2039.3639 - learning_rate: 0.0010\n",
      "Epoch 422/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1743.1050 \n",
      "Epoch 422: val_loss did not improve from 2031.05872\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1777.6688 - val_loss: 2041.1344 - learning_rate: 0.0010\n",
      "Epoch 423/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2035.7947 \n",
      "Epoch 423: val_loss did not improve from 2031.05872\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2040.0852 - val_loss: 2065.1106 - learning_rate: 0.0010\n",
      "Epoch 424/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2339.2588 \n",
      "Epoch 424: val_loss did not improve from 2031.05872\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2308.2937 - val_loss: 2096.4839 - learning_rate: 0.0010\n",
      "Epoch 425/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2439.3989 \n",
      "Epoch 425: val_loss did not improve from 2031.05872\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2384.4128 - val_loss: 2059.7927 - learning_rate: 0.0010\n",
      "Epoch 426/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2206.4214 \n",
      "Epoch 426: val_loss improved from 2031.05872 to 2016.10693, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2188.0032 - val_loss: 2016.1069 - learning_rate: 0.0010\n",
      "Epoch 427/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2093.4785\n",
      "Epoch 427: val_loss did not improve from 2016.10693\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2087.6392 - val_loss: 2023.5184 - learning_rate: 0.0010\n",
      "Epoch 428/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2860.0486 \n",
      "Epoch 428: val_loss did not improve from 2016.10693\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2768.4707 - val_loss: 2020.1716 - learning_rate: 0.0010\n",
      "Epoch 429/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1864.7532 \n",
      "Epoch 429: val_loss did not improve from 2016.10693\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1885.6344 - val_loss: 2096.0269 - learning_rate: 0.0010\n",
      "Epoch 430/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1752.2261 \n",
      "Epoch 430: val_loss did not improve from 2016.10693\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1792.6193 - val_loss: 2031.9500 - learning_rate: 0.0010\n",
      "Epoch 431/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1865.0470\n",
      "Epoch 431: val_loss improved from 2016.10693 to 2011.06824, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1888.9863 - val_loss: 2011.0682 - learning_rate: 0.0010\n",
      "Epoch 432/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1961.1990 \n",
      "Epoch 432: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1972.2267 - val_loss: 2021.9398 - learning_rate: 0.0010\n",
      "Epoch 433/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1811.2592 \n",
      "Epoch 433: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1835.2634 - val_loss: 2027.5339 - learning_rate: 0.0010\n",
      "Epoch 434/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1783.8848 \n",
      "Epoch 434: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1812.3029 - val_loss: 2022.6084 - learning_rate: 0.0010\n",
      "Epoch 435/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2258.5098 \n",
      "Epoch 435: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2231.7722 - val_loss: 2017.1627 - learning_rate: 0.0010\n",
      "Epoch 436/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1936.3257\n",
      "Epoch 436: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1941.9904 - val_loss: 2015.0873 - learning_rate: 0.0010\n",
      "Epoch 437/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1944.7957 \n",
      "Epoch 437: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1953.4891 - val_loss: 2019.0560 - learning_rate: 0.0010\n",
      "Epoch 438/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1849.3691 \n",
      "Epoch 438: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1870.5477 - val_loss: 2067.1665 - learning_rate: 0.0010\n",
      "Epoch 439/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2091.9092 \n",
      "Epoch 439: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2084.6853 - val_loss: 2037.9456 - learning_rate: 0.0010\n",
      "Epoch 440/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1799.0859\n",
      "Epoch 440: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1828.4109 - val_loss: 2087.3799 - learning_rate: 0.0010\n",
      "Epoch 441/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2324.1389 \n",
      "Epoch 441: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2295.7107 - val_loss: 2058.6279 - learning_rate: 0.0010\n",
      "Epoch 442/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1528.7687\n",
      "Epoch 442: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1584.9957 - val_loss: 2018.8864 - learning_rate: 0.0010\n",
      "Epoch 443/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2095.4573 \n",
      "Epoch 443: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2084.7588 - val_loss: 2015.1033 - learning_rate: 0.0010\n",
      "Epoch 444/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2106.9753 \n",
      "Epoch 444: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2096.7251 - val_loss: 2019.2296 - learning_rate: 0.0010\n",
      "Epoch 445/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2095.3628 \n",
      "Epoch 445: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2088.3862 - val_loss: 2019.0553 - learning_rate: 0.0010\n",
      "Epoch 446/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1982.3704 \n",
      "Epoch 446: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1988.7344 - val_loss: 2104.5164 - learning_rate: 0.0010\n",
      "Epoch 447/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2032.5345 \n",
      "Epoch 447: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2036.6328 - val_loss: 2094.6077 - learning_rate: 0.0010\n",
      "Epoch 448/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2632.7598 \n",
      "Epoch 448: val_loss did not improve from 2011.06824\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 2580.1030 - val_loss: 2031.5500 - learning_rate: 0.0010\n",
      "Epoch 449/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1723.4014 \n",
      "Epoch 449: val_loss improved from 2011.06824 to 1397.38330, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1709.9741 - val_loss: 1397.3833 - learning_rate: 0.0010\n",
      "Epoch 450/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1441.5891\n",
      "Epoch 450: val_loss improved from 1397.38330 to 1318.25647, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1433.2189 - val_loss: 1318.2565 - learning_rate: 0.0010\n",
      "Epoch 451/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1207.7489\n",
      "Epoch 451: val_loss improved from 1318.25647 to 1292.29285, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1217.1913 - val_loss: 1292.2928 - learning_rate: 0.0010\n",
      "Epoch 452/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1329.7705 \n",
      "Epoch 452: val_loss improved from 1292.29285 to 1276.71045, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1321.5164 - val_loss: 1276.7104 - learning_rate: 0.0010\n",
      "Epoch 453/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1316.7662 \n",
      "Epoch 453: val_loss did not improve from 1276.71045\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1315.6774 - val_loss: 1293.7561 - learning_rate: 0.0010\n",
      "Epoch 454/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1302.1780 \n",
      "Epoch 454: val_loss improved from 1276.71045 to 1259.15320, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1300.9604 - val_loss: 1259.1532 - learning_rate: 0.0010\n",
      "Epoch 455/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1095.6038\n",
      "Epoch 455: val_loss improved from 1259.15320 to 1252.37573, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1117.4808 - val_loss: 1252.3757 - learning_rate: 0.0010\n",
      "Epoch 456/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1130.4764 \n",
      "Epoch 456: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1143.0355 - val_loss: 1302.2766 - learning_rate: 0.0010\n",
      "Epoch 457/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1526.3115 \n",
      "Epoch 457: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1498.3019 - val_loss: 1252.4069 - learning_rate: 0.0010\n",
      "Epoch 458/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1152.9520\n",
      "Epoch 458: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1165.3882 - val_loss: 1281.2721 - learning_rate: 0.0010\n",
      "Epoch 459/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1211.4111\n",
      "Epoch 459: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1218.0697 - val_loss: 1267.1809 - learning_rate: 0.0010\n",
      "Epoch 460/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1374.1373 \n",
      "Epoch 460: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1361.3859 - val_loss: 1262.8685 - learning_rate: 0.0010\n",
      "Epoch 461/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 966.6985 \n",
      "Epoch 461: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 996.9108 - val_loss: 1257.3121 - learning_rate: 0.0010\n",
      "Epoch 462/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1363.7281 \n",
      "Epoch 462: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1355.7811 - val_loss: 1275.7927 - learning_rate: 0.0010\n",
      "Epoch 463/500\n",
      "\u001B[1m22/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1196.4966\n",
      "Epoch 463: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1215.1312 - val_loss: 1278.6576 - learning_rate: 0.0010\n",
      "Epoch 464/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1158.8591\n",
      "Epoch 464: val_loss did not improve from 1252.37573\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1174.3700 - val_loss: 1261.2400 - learning_rate: 0.0010\n",
      "Epoch 465/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1096.7194\n",
      "Epoch 465: val_loss improved from 1252.37573 to 1247.31018, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1113.9231 - val_loss: 1247.3102 - learning_rate: 0.0010\n",
      "Epoch 466/500\n",
      "\u001B[1m22/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1257.4731\n",
      "Epoch 466: val_loss did not improve from 1247.31018\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1263.1550 - val_loss: 1263.4435 - learning_rate: 0.0010\n",
      "Epoch 467/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1241.9547 \n",
      "Epoch 467: val_loss did not improve from 1247.31018\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1244.6569 - val_loss: 1283.7450 - learning_rate: 0.0010\n",
      "Epoch 468/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1233.8134 \n",
      "Epoch 468: val_loss did not improve from 1247.31018\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1241.4846 - val_loss: 1254.4362 - learning_rate: 0.0010\n",
      "Epoch 469/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1117.9974 \n",
      "Epoch 469: val_loss did not improve from 1247.31018\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1132.8605 - val_loss: 1258.8367 - learning_rate: 0.0010\n",
      "Epoch 470/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1304.9509 \n",
      "Epoch 470: val_loss improved from 1247.31018 to 1243.72888, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1298.7612 - val_loss: 1243.7289 - learning_rate: 0.0010\n",
      "Epoch 471/500\n",
      "\u001B[1m22/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1224.8477\n",
      "Epoch 471: val_loss improved from 1243.72888 to 1240.83521, saving model to data/models/silica_best_tyhanov.keras\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1232.8073 - val_loss: 1240.8352 - learning_rate: 0.0010\n",
      "Epoch 472/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1277.8746 \n",
      "Epoch 472: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1272.2446 - val_loss: 1250.0234 - learning_rate: 0.0010\n",
      "Epoch 473/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1100.1210\n",
      "Epoch 473: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1115.0548 - val_loss: 1260.8512 - learning_rate: 0.0010\n",
      "Epoch 474/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1055.5693\n",
      "Epoch 474: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1089.4301 - val_loss: 1333.0104 - learning_rate: 0.0010\n",
      "Epoch 475/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1463.0605 \n",
      "Epoch 475: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1445.8636 - val_loss: 1295.3330 - learning_rate: 0.0010\n",
      "Epoch 476/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1462.6923 \n",
      "Epoch 476: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1442.3397 - val_loss: 1292.4257 - learning_rate: 0.0010\n",
      "Epoch 477/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1434.1228 \n",
      "Epoch 477: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1423.2339 - val_loss: 1476.5054 - learning_rate: 0.0010\n",
      "Epoch 478/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1230.0865\n",
      "Epoch 478: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1243.7516 - val_loss: 1277.9541 - learning_rate: 0.0010\n",
      "Epoch 479/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1384.5170 \n",
      "Epoch 479: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1370.5360 - val_loss: 1300.9657 - learning_rate: 0.0010\n",
      "Epoch 480/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1433.9644 \n",
      "Epoch 480: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1424.9465 - val_loss: 1283.9222 - learning_rate: 0.0010\n",
      "Epoch 481/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1426.0005 \n",
      "Epoch 481: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1409.2949 - val_loss: 1281.3915 - learning_rate: 0.0010\n",
      "Epoch 482/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1065.7842 \n",
      "Epoch 482: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1086.2617 - val_loss: 1248.5582 - learning_rate: 0.0010\n",
      "Epoch 483/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1056.4930\n",
      "Epoch 483: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1083.3395 - val_loss: 1357.2800 - learning_rate: 0.0010\n",
      "Epoch 484/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1344.0800 \n",
      "Epoch 484: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1349.9316 - val_loss: 1467.7490 - learning_rate: 0.0010\n",
      "Epoch 485/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1327.4286 \n",
      "Epoch 485: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1332.4077 - val_loss: 1303.6877 - learning_rate: 0.0010\n",
      "Epoch 486/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1330.9128 \n",
      "Epoch 486: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1327.4536 - val_loss: 1317.4041 - learning_rate: 0.0010\n",
      "Epoch 487/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1331.7374 \n",
      "Epoch 487: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1325.3710 - val_loss: 1292.2040 - learning_rate: 0.0010\n",
      "Epoch 488/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1274.7080\n",
      "Epoch 488: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1274.3972 - val_loss: 1257.3114 - learning_rate: 0.0010\n",
      "Epoch 489/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1541.2153 \n",
      "Epoch 489: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1510.6288 - val_loss: 1252.5259 - learning_rate: 0.0010\n",
      "Epoch 490/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1596.6982 \n",
      "Epoch 490: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1558.9652 - val_loss: 1253.0183 - learning_rate: 0.0010\n",
      "Epoch 491/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1286.7404 \n",
      "Epoch 491: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1280.6373 - val_loss: 1241.3552 - learning_rate: 0.0010\n",
      "Epoch 492/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1317.2899 \n",
      "Epoch 492: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1310.2378 - val_loss: 1275.3650 - learning_rate: 0.0010\n",
      "Epoch 493/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1204.0480\n",
      "Epoch 493: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1210.2787 - val_loss: 1242.8492 - learning_rate: 0.0010\n",
      "Epoch 494/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1088.9672 \n",
      "Epoch 494: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1105.7688 - val_loss: 1242.2642 - learning_rate: 0.0010\n",
      "Epoch 495/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1204.7124 \n",
      "Epoch 495: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1210.4938 - val_loss: 1244.9617 - learning_rate: 0.0010\n",
      "Epoch 496/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1146.5594 \n",
      "Epoch 496: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1159.6249 - val_loss: 1266.5161 - learning_rate: 0.0010\n",
      "Epoch 497/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 989.8841 \n",
      "Epoch 497: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1024.4753 - val_loss: 1281.7937 - learning_rate: 0.0010\n",
      "Epoch 498/500\n",
      "\u001B[1m25/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1093.4783 \n",
      "Epoch 498: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1105.7889 - val_loss: 1248.8322 - learning_rate: 0.0010\n",
      "Epoch 499/500\n",
      "\u001B[1m24/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1384.2230 \n",
      "Epoch 499: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1369.3519 - val_loss: 1252.6808 - learning_rate: 0.0010\n",
      "Epoch 500/500\n",
      "\u001B[1m23/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1281.6534 \n",
      "Epoch 500: val_loss did not improve from 1240.83521\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1283.0118 - val_loss: 1275.6667 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.set_value(model.optimizer.learning_rate, 0.0001)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(filepath='data/models/silica_best_tyhanov.keras', save_best_only=True,\n",
    "                                              monitor='val_loss', mode='min', verbose=1, save_weights_only=False,\n",
    "                                              save_freq='epoch')\n",
    "\n",
    "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                                      patience=10000, verbose=1, mode='auto')\n",
    "history = model.fit(np.array(x_train), np.array(y_train),\n",
    "                    epochs=500, batch_size=20, shuffle=True,\n",
    "                    validation_data=(np.array(x_train), np.array(y_train)), callbacks=[mcp_save, reduce_lr_loss])\n",
    "plot_loss(history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T05:58:26.616021Z",
     "start_time": "2024-10-12T05:56:18.411899Z"
    }
   },
   "id": "191dc523763f2938"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('data/models/silica_random30K.keras', custom_objects={'abs': tf.math.abs, 'custom_loss': custom_loss})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T06:13:13.900148Z",
     "start_time": "2024-10-12T06:13:13.779072Z"
    }
   },
   "id": "c9a1a6a8cf9f8ba1"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3/3\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n"
     ]
    }
   ],
   "source": [
    "# prediction = model.evaluate(np.array(x_test), np.array(y_test))\n",
    "prediction = model.predict(np.array(x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T06:13:14.367040Z",
     "start_time": "2024-10-12T06:13:14.251069Z"
    }
   },
   "id": "d7c4e1ecdee1bcf7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# test on x_train\n",
    "def fetch_prediction(prediction):\n",
    "    return prediction[:128]\n",
    "\n",
    "pore_widths = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "pressures = np.load(\"data/initial kernels/Pressure_Silica.npy\")\n",
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        k = np.random.randint(0, len(x_test))\n",
    "        x_scale_factor = max(pore_widths) / len(x_test[k])\n",
    "        axis[i, j].plot(pore_widths / x_scale_factor, fetch_prediction(prediction[k]), marker=\".\", label=f\"Prediction\")\n",
    "        axis[i, j].plot(pore_widths / x_scale_factor, y_test[k], marker=\".\", label=\"Real distribution\")\n",
    "        axis[i, j].plot(pressures[77:-10]*500, x_test[k], label=\"Isotherm\")\n",
    "        kernal = (data_sorb.T[40:])\n",
    "        axis[i, j].plot(pressures[40:]*500, np.dot(kernal, prediction[k][:128]), label=\"New Isotherm\")\n",
    "        axis[i, j].set_title(f\"№ {k}\")\n",
    "        axis[i, j].title.set_size(10)\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plt.legend()\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T06:14:20.089871Z",
     "start_time": "2024-10-12T06:13:15.178073Z"
    }
   },
   "id": "bbc48118bf79bc08"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# test with test Generator\n",
    "from tools import TestApp\n",
    "\n",
    "gen = Generator(path_s=\"data/initial kernels/Kernel_Silica_Adsorption.npy\",\n",
    "                path_d=\"data/initial kernels/Kernel_Silica_Desorption.npy\",\n",
    "                path_p_d=\"data/initial kernels/Pressure_Silica.npy\",\n",
    "                path_p_s=\"data/initial kernels/Pressure_Silica.npy\",\n",
    "                path_a=\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\"\n",
    "                )\n",
    "# gen = Generator(path_s=\"data/initial kernels/Kernel_Carbon_Adsorption.npy\",\n",
    "#                               path_d=\"data/initial kernels/Kernel_Carbon_Desorption.npy\",\n",
    "#                               path_p_d=\"data/initial kernels/Pressure_Carbon.npy\",\n",
    "#                               path_p_s=\"data/initial kernels/Pressure_Carbon.npy\",\n",
    "#                               path_a=\"data/initial kernels/Size_Kernel_Carbon_Adsorption.npy\"\n",
    "#                             )\n",
    "\n",
    "#TestApp.App(model, gen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "b25b4132001702fe"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "exp_file_list = [\"MCM-41\", \"SBA-15\", \"SBA-16\", \"MIL-101\", \"MIL-101_2\", \"DUT-49\", \"FDM-4\", \"PCN-333\", \"PCN-777\",\n",
    "                 \"MIL-100\"]\n",
    "\n",
    "p_exp_list = []\n",
    "n_s_exp_raw_list = []\n",
    "for exp_file_name in exp_file_list:\n",
    "    data = pd.read_csv(f\"data/real/{exp_file_name}.txt\", header=None)\n",
    "    # p_exp_list.append(data.iloc[:,1].to_numpy())\n",
    "    # n_s_exp_raw_list.append(data.iloc[:,3].to_numpy())\n",
    "    p_exp_list.append(data.iloc[:, 1].to_numpy())\n",
    "    n_s_exp_raw_list.append(data.iloc[:, 3].to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "d0179658dd4e6c12"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "j = 2\n",
    "plt.plot(p_exp_list[j], n_s_exp_raw_list[j], marker=\".\", label=exp_file_list[j])\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "316deb7add602c0a"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# интерполируем экспериментальную изотерму под давления кернала\n",
    "n_s_exp_list = []\n",
    "for i in range(len(p_exp_list)):\n",
    "    n_s_exp_list.append(np.interp(gen.pressures_s[77:367], p_exp_list[i], n_s_exp_raw_list[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "bfb8c81559e024d"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "j = 2\n",
    "plt.plot(gen.pressures_s[77:367], n_s_exp_list[j], marker=\".\", label=exp_file_list[j])\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "61ca6d25ee7153f0"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node functional_3_1/dense_15_1/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 487, in advance_eventloop\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\eventloops.py\", line 309, in loop_tk\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\eventloops.py\", line 306, in start\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1485, in mainloop\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 861, in callit\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\eventloops.py\", line 299, in on_timer\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 267, in __step\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 520, in do_one_iteration\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20048\\2050301347.py\", line 2, in <module>\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20048\\2050301347.py\", line 2, in <listcomp>\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 508, in predict\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 208, in one_step_on_data_distributed\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 198, in one_step_on_data\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 96, in predict_step\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 175, in call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 556, in call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 148, in call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\activations\\activations.py\", line 47, in relu\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\activations\\activations.py\", line 99, in static_call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 15, in relu\n\nMatrix size-incompatible: In[0]: [1,290], In[1]: [371,458]\n\t [[{{node functional_3_1/dense_15_1/Relu}}]] [Op:__inference_one_step_on_data_distributed_2114652]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[127], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m n_s_exp_for_net_list \u001B[38;5;241m=\u001B[39m [pre_process_isotherm(np\u001B[38;5;241m.\u001B[39mcopy(n_s_exp), scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m n_s_exp \u001B[38;5;129;01min\u001B[39;00m n_s_exp_list]\n\u001B[1;32m----> 2\u001B[0m fit_exp_list \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mn_s_exp_for_net\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mn_s_exp_for_net\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mn_s_exp_for_net_list\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      3\u001B[0m fit_exp_list \u001B[38;5;241m=\u001B[39m [fetch_prediction(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m fit_exp_list]\n",
      "Cell \u001B[1;32mIn[127], line 2\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m n_s_exp_for_net_list \u001B[38;5;241m=\u001B[39m [pre_process_isotherm(np\u001B[38;5;241m.\u001B[39mcopy(n_s_exp), scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m n_s_exp \u001B[38;5;129;01min\u001B[39;00m n_s_exp_list]\n\u001B[1;32m----> 2\u001B[0m fit_exp_list \u001B[38;5;241m=\u001B[39m [\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mn_s_exp_for_net\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mT \u001B[38;5;28;01mfor\u001B[39;00m n_s_exp_for_net \u001B[38;5;129;01min\u001B[39;00m n_s_exp_for_net_list]\n\u001B[0;32m      3\u001B[0m fit_exp_list \u001B[38;5;241m=\u001B[39m [fetch_prediction(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m fit_exp_list]\n",
      "File \u001B[1;32m~\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     54\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Graph execution error:\n\nDetected at node functional_3_1/dense_15_1/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 487, in advance_eventloop\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\eventloops.py\", line 309, in loop_tk\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\eventloops.py\", line 306, in start\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1485, in mainloop\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 861, in callit\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\eventloops.py\", line 299, in on_timer\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 267, in __step\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 520, in do_one_iteration\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20048\\2050301347.py\", line 2, in <module>\n\n  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20048\\2050301347.py\", line 2, in <listcomp>\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 508, in predict\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 208, in one_step_on_data_distributed\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 198, in one_step_on_data\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 96, in predict_step\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 175, in call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 556, in call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 148, in call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\activations\\activations.py\", line 47, in relu\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\activations\\activations.py\", line 99, in static_call\n\n  File \"C:\\Users\\Admin\\Desktop\\iso\\isotherm-analyzer\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 15, in relu\n\nMatrix size-incompatible: In[0]: [1,290], In[1]: [371,458]\n\t [[{{node functional_3_1/dense_15_1/Relu}}]] [Op:__inference_one_step_on_data_distributed_2114652]"
     ]
    }
   ],
   "source": [
    "n_s_exp_for_net_list = [pre_process_isotherm(np.copy(n_s_exp), scale=False) for n_s_exp in n_s_exp_list]\n",
    "fit_exp_list = [model.predict(np.array([n_s_exp_for_net])).T for n_s_exp_for_net in n_s_exp_for_net_list]\n",
    "fit_exp_list = [fetch_prediction(i) for i in fit_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "964f2bfe8f0bb107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_exp_list[k]) / max(n_s_exp_raw_list[k])\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k], marker=\".\", label=f\"Distribution\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k],\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        \n",
    "        kernal = (data_sorb.T[40:])\n",
    "        axis[i, j].plot(gen.pressures_s[40:458]* x_scale_factor, np.dot(kernal, fit_exp_list[k]), label=\"Isotherm by distribution\")\n",
    "        \n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_exp_list[k])], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "dced44b8e1da2a3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Classic\n",
    "import inverse\n",
    "\n",
    "kernel = np.load(\"data/initial kernels/Kernel_Silica_Adsorption.npy\")\n",
    "\n",
    "### normalize on size\n",
    "# a_array = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "# for i in range(len(a_array)):\n",
    "#     kernel[i] /= a_array[i]\n",
    "#     kernel[i] /= a_array[i]\n",
    "###\n",
    "\n",
    "cut_kernel = []\n",
    "for i in range(len(kernel)):\n",
    "    cut_kernel.append(kernel[i][40:458])\n",
    "cut_kernel = np.array(cut_kernel)\n",
    "fit_classic_list = [inverse.fit_SLSQP(adsorption=n_s, kernel=cut_kernel, a_array=pore_widths) for n_s in n_s_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "435b91d710c2ce9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "importlib.reload(inverse)\n",
    "\n",
    "\n",
    "def create_regularization_animation(file):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5, True)\n",
    "    fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=0)\n",
    "    line1, = ax.plot(pore_widths[:-30], fit_classic.x[:-30], marker=\".\", label=f\"a = {0}\")\n",
    "\n",
    "    y_scale_factor = max(fit_classic.x) / max(fit_exp_list[2])\n",
    "    #plt.plot(pore_widths, fit_exp_list[2] * y_scale_factor, marker=\".\", label=f\"Суррогатная модель\")\n",
    "\n",
    "    ax.set_ylabel(\"Объем пор, $см^3$/ нм * г\")\n",
    "    ax.set_xlabel(\"Размер пор, нм\")\n",
    "\n",
    "    L = plt.legend(loc=1)  # Define legend objects\n",
    "\n",
    "    def update(frame):\n",
    "        a = frame / 4 + 2\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        line1.set_ydata(fit_classic.x[:-30])\n",
    "        L.get_texts()[0].set_text(\n",
    "            f\"Распределение пор по размерам, параметр регуляризации α = {round(a-2,1)}\")  # Update label each at frame\n",
    "        return line1,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig=fig, func=update, frames=100, interval=100)\n",
    "    writervideo = animation.FFMpegWriter(fps=30)\n",
    "    ani.save(file, writer=writervideo, dpi=200)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "create_regularization_animation(\"SBA-16_regularization.mp4\")\n",
    "def plot_regularization_graphs():\n",
    "    for a in [1, 5, 10, 50]:\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        plt.plot(pore_widths, fit_classic.x, marker=\".\", label=f\"α = {a}\")\n",
    "    plt.ylabel(\"Объем пор, $см^3$/ нм * гр\")\n",
    "    plt.xlabel(\"Размер пор, нм\")\n",
    "    plot()\n",
    "#plot_regularization_graphs()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "1b0c708ab92c81f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_classic_list[k].x) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_net = max(fit_classic_list[k].x) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k] * y_scale_factor,\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_classic_list[k].x)], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "aac060bb7ff7720e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Classic\n",
    "import inverse\n",
    "\n",
    "kernel = np.load(\"data/initial kernels/Kernel_Silica_Adsorption.npy\")\n",
    "\n",
    "### normalize on size\n",
    "# a_array = np.load(\"data/initial kernels/Size_Kernel_Silica_Adsorption.npy\")\n",
    "# for i in range(len(a_array)):\n",
    "#     kernel[i] /= a_array[i]\n",
    "#     kernel[i] /= a_array[i]\n",
    "###\n",
    "\n",
    "cut_kernel = []\n",
    "for i in range(len(kernel)):\n",
    "    cut_kernel.append(kernel[i][40:458])\n",
    "cut_kernel = np.array(cut_kernel)\n",
    "fit_classic_list = [inverse.fit_SLSQP(adsorption=n_s, kernel=cut_kernel, a_array=pore_widths) for n_s in n_s_exp_list]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T13:40:22.307670Z",
     "start_time": "2024-06-18T13:39:59.907685Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "9949a77c1d2e89c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "importlib.reload(inverse)\n",
    "\n",
    "\n",
    "def create_regularization_animation(file):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5, True)\n",
    "    fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=0)\n",
    "    line1, = ax.plot(pore_widths[:-30], fit_classic.x[:-30], marker=\".\", label=f\"a = {0}\")\n",
    "\n",
    "    y_scale_factor = max(fit_classic.x) / max(fit_exp_list[2])\n",
    "    #plt.plot(pore_widths, fit_exp_list[2] * y_scale_factor, marker=\".\", label=f\"Суррогатная модель\")\n",
    "\n",
    "    ax.set_ylabel(\"Объем пор, $см^3$/ нм * г\")\n",
    "    ax.set_xlabel(\"Размер пор, нм\")\n",
    "\n",
    "    L = plt.legend(loc=1)  # Define legend objects\n",
    "\n",
    "    def update(frame):\n",
    "        a = frame / 4 + 2\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        line1.set_ydata(fit_classic.x[:-30])\n",
    "        L.get_texts()[0].set_text(\n",
    "            f\"Распределение пор по размерам, параметр регуляризации α = {round(a-2,1)}\")  # Update label each at frame\n",
    "        return line1,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig=fig, func=update, frames=100, interval=100)\n",
    "    writervideo = animation.FFMpegWriter(fps=30)\n",
    "    ani.save(file, writer=writervideo, dpi=200)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "create_regularization_animation(\"SBA-16_regularization.mp4\")\n",
    "def plot_regularization_graphs():\n",
    "    for a in [1, 5, 10, 50]:\n",
    "        fit_classic = inverse.fit_SLSQP(adsorption=n_s_exp_list[2], kernel=cut_kernel, a_array=pore_widths, alpha=a)\n",
    "        plt.plot(pore_widths, fit_classic.x, marker=\".\", label=f\"α = {a}\")\n",
    "    plt.ylabel(\"Объем пор, $см^3$/ нм * гр\")\n",
    "    plt.xlabel(\"Размер пор, нм\")\n",
    "    plot()\n",
    "#plot_regularization_graphs()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:50:01.331621Z",
     "start_time": "2024-06-18T14:46:23.487577Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "ca3051b964f378a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        x_scale_factor = max(gen.a_array) / max(p_exp_list[k])\n",
    "        y_scale_factor = max(fit_classic_list[k].x) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_net = max(fit_classic_list[k].x) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(p_exp_list[k] * x_scale_factor, n_s_exp_raw_list[k] * y_scale_factor,\n",
    "                        label=f\"{exp_file_list[k]}\", marker=\".\")\n",
    "        axis[i, j].set_title(f\"max at {round(gen.a_array[np.argmax(fit_classic_list[k].x)], 2)} nm\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "707d030f7db4a23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare net, classic, quantachrome distributions\n",
    "NX, NY = 3, 4\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "\n",
    "\n",
    "def calculate_isotherm_by_distribution(generator: Generator, a_array, distribution):\n",
    "    generator.a_array = a_array\n",
    "    generator.pore_distribution = distribution\n",
    "    generator.calculate_isotherms()\n",
    "    return generator.n_s\n",
    "\n",
    "\n",
    "#/np.ediff1d(pore_widths, to_begin=pore_widths[0])\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        quantachrome_pore_size = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "                0] / 10 * 2  # /10 - перевод в НМ * 2 - в QH размер - Half pore width.\n",
    "        quantachrome_dV = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "        # isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "        # y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "        # quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "        y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "        y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "        axis[i, j].plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"net\")\n",
    "        axis[i, j].plot(pore_widths, fit_classic_list[k].x * y_scale_factor_classic, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(quantachrome_pore_size, quantachrome_dV, marker=\".\", label=f\"quantachrome\")\n",
    "        axis[i, j].set_title(f\"{exp_file_list[k]}\")\n",
    "        axis[i, j].title.set_size(12)\n",
    "        axis[i, j].legend(loc=\"upper right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "8e0b9fbf987405b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "plt = reload(plt)\n",
    "\n",
    "k = 0\n",
    "\n",
    "quantachrome_pore_size = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "        0] / 10 * 2  # /10 - перевод в НМ * 2 - в QH размер - Half pore width.\n",
    "quantachrome_dV = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "# isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "# y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "# quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "plt.plot(pore_widths, fit_exp_list[k] * y_scale_factor_net, marker=\".\", label=f\"Суррогатная модель\")\n",
    "plt.plot(pore_widths, fit_classic_list[k].x * y_scale_factor_classic, marker=\".\", label=f\"Математическое решение\")\n",
    "#plt.plot(quantachrome_pore_size, quantachrome_dV, marker=\".\", label=f\"Математическое решение QH\") \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.title(f\"{exp_file_list[k]}\")\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.subplots_adjust(left=0.15,\n",
    "                    bottom=0.133,\n",
    "                    right=0.979,\n",
    "                    top=0.917,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Объем пор, $см^3$/ нм * гр\")\n",
    "plt.xlabel(\"Размер пор, нм\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "c5e26ea811ae701c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation 2\n",
    "k = 3\n",
    "plt.plot(p_exp_list[k], n_s_exp_raw_list[k], marker=\".\", color='b', label=f\"{exp_file_list[k]}\")\n",
    "k = 9\n",
    "plt.plot(p_exp_list[k], n_s_exp_raw_list[k], marker=\".\", color='r', label=f\"{exp_file_list[k]}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.title(f\"Изотермы адсорбции\")\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.subplots_adjust(left=0.15,\n",
    "                    bottom=0.133,\n",
    "                    right=0.979,\n",
    "                    top=0.917,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Адсорбция, $см^3$/г\")\n",
    "plt.xlabel(\"Давление, $P/P_{0}$\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "be93198745d6bc2d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plots for presentation 3\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6, 7))\n",
    "ax1.set_xlabel(\"Размер пор, нм\")\n",
    "ax1.set_ylabel(\"Объем пор, $см^3$/ нм * гр\")\n",
    "k = 0\n",
    "plt.title(f\"{exp_file_list[k]}\")\n",
    "quantachrome_pore_size = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "        0] / 10 * 2  # /10 - перевод в НМ * 2 - в QH размер - Half pore width.\n",
    "quantachrome_dV = \\\n",
    "    np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "# isotherm_formQC = calculate_isotherm_by_distribution(gen, pore_widths, np.interp(pore_widths, quantachrome_pore_size, quantachrome_dV))\n",
    "# y_scale_factor_QH = max(n_s_exp_raw_list[k]) / max(isotherm_formQC)\n",
    "# quantachrome_dV *= y_scale_factor_QH\n",
    "\n",
    "y_scale_factor_classic = max(quantachrome_dV) / max(fit_classic_list[k].x)\n",
    "y_scale_factor_net = max(quantachrome_dV) / max(fit_exp_list[k])\n",
    "\n",
    "ax1.plot(pore_widths[0:100], (fit_exp_list[k] * y_scale_factor_net)[0:100], marker=\".\", label=f\"Суррогатная модель\")\n",
    "ax1.plot(pore_widths[0:100], (fit_classic_list[k].x * y_scale_factor_classic)[0:100], marker=\".\",\n",
    "         label=f\"Математическое решение\")\n",
    "# ax1.tick_params(axis='y')\n",
    "# plt.legend(loc=\"right\")\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax3 = ax2.twiny()  # instantiate a second axes that shares the same x-axis\n",
    "ax3.set_xlabel(\"Давление, $P/P_{0}$\")\n",
    "ax2.set_ylabel(\"Адсорбция, $см^3$/г\")  # we already handled the x-label with ax1\n",
    "ax3.plot(p_exp_list[k], n_s_exp_raw_list[k], color='g', label=f\"Изотерма адсорбции\")\n",
    "# plt.legend(loc=\"right\")\n",
    "# \n",
    "# ax2.tick_params(axis='y')\n",
    "# plt.subplots_adjust(left=0.15,\n",
    "#                     bottom=0.133, \n",
    "#                     right=0.979, \n",
    "#                     top=0.917, \n",
    "#                     wspace=0.4, \n",
    "#                     hspace=0.4)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "2d0a60b839334253",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compare isotherms\n",
    "NX, NY = 2, 3\n",
    "figure, axis = plt.subplots(NX, NY)\n",
    "k = 0\n",
    "\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        net_isotherm = calculate_isotherm_by_distribution(gen, pore_widths, fit_exp_list[k].T[0])\n",
    "        classic_isotherm = calculate_isotherm_by_distribution(gen, pore_widths, fit_classic_list[k].x)\n",
    "        quantachrome_data = np.genfromtxt(f\"data/real/quantachrome/silica/fitting/{exp_file_list[k]}.csv\",\n",
    "                                          delimiter=\",\")[1:].T\n",
    "        quantachrome_pore_size = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[\n",
    "                0] / 10 * 2\n",
    "        quantachrome_dV = \\\n",
    "            np.genfromtxt(f\"data/real/quantachrome/silica/distribution/{exp_file_list[k]}.csv\", delimiter=\",\")[1:].T[3]\n",
    "        quantachrome_data2 = calculate_isotherm_by_distribution(gen, pore_widths,\n",
    "                                                                np.interp(pore_widths, quantachrome_pore_size,\n",
    "                                                                          quantachrome_dV))\n",
    "\n",
    "        y_scale_factor_net = max(net_isotherm) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_classic = max(classic_isotherm) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_quantachrome = max(quantachrome_data[1]) / max(n_s_exp_raw_list[k])\n",
    "        y_scale_factor_quantachrome2 = max(quantachrome_data2) / (\n",
    "            n_s_exp_raw_list[k][-1])  #max(quantachrome_data[1]) / max(n_s_exp_raw_list[k])\n",
    "        print(1 / y_scale_factor_quantachrome2)\n",
    "\n",
    "        axis[i, j].plot(p_exp_list[k], n_s_exp_raw_list[k], label=\"real\")\n",
    "        axis[i, j].plot(gen.pressures_s, classic_isotherm / y_scale_factor_classic, marker=\".\", label=f\"classic\")\n",
    "        axis[i, j].plot(gen.pressures_s, net_isotherm / y_scale_factor_net, label=\"net\")\n",
    "        # axis[i, j].plot(quantachrome_data[0], quantachrome_data[1]/y_scale_factor_quantachrome, label=\"quantachrome\")\n",
    "        # axis[i, j].plot(gen.pressures_s, quantachrome_data2/y_scale_factor_quantachrome2, label=\"quantachrome_from_kernal\")\n",
    "        axis[i, j].set_title(f\"{exp_file_list[k]}\")\n",
    "        axis[i, j].legend(loc=\"lower right\")\n",
    "        axis[i, j].grid()\n",
    "        k += 1\n",
    "        if k >= len(fit_exp_list):\n",
    "            break\n",
    "plt.subplots_adjust(hspace=0.6, right=0.95, left=0.05, bottom=0.05, top=0.95)\n",
    "plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "5e20fa504d7c45cc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#QUANTACHROME\n",
    "data = np.genfromtxt(\"data/real/quantachrome/silica/distribution/MIL-101.csv\", delimiter=\",\")[1:].T[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "a2947b6286fe73b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "29ec7e76cb0598ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "5d7d844d6e720d14"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "40a95cf0ab8cc517"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}